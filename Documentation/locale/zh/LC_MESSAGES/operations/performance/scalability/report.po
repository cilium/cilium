# SOME DESCRIPTIVE TITLE.
# Copyright (C) Cilium Authors
# This file is distributed under the same license as the Cilium package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Cilium \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-25 23:56+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../operations/performance/scalability/report.rst:3
#: d6bc426eee184043916588ffe6162d8b
msgid ""
"WARNING: You are looking at unreleased Cilium documentation. Please use "
"the official rendered version released here: https://docs.cilium.io"
msgstr ""

#: ../../operations/performance/scalability/report.rst:11
#: 42c142ea07b14211893df629601333e9
msgid "Scalability report"
msgstr ""

#: ../../operations/performance/scalability/report.rst:13
#: 8089bfe5c7d84b8c819f005e74d407a8
msgid ""
"This report is intended for users planning to run Cilium on clusters with"
" more than 200 nodes in CRD mode (without a kvstore available). In our "
"development cycle we have deployed Cilium on large clusters and these "
"were the options that were suitable for our testing:"
msgstr ""

#: ../../operations/performance/scalability/report.rst:20
#: 7e626a5d837448cd90f21477589fcc41
msgid "Setup"
msgstr ""

#: ../../operations/performance/scalability/report.rst:36
#: 83a5099a45484e61badb8120bcc0b163
msgid ""
"``--set endpointHealthChecking.enabled=false`` and ``--set "
"healthChecking=false`` disable endpoint health checking entirely. However"
" it is recommended that those features be enabled initially on a smaller "
"cluster (3-10 nodes) where it can be used to detect potential packet loss"
" due to firewall rules or hypervisor settings."
msgstr ""

#: ../../operations/performance/scalability/report.rst:42
#: 933f404051bc4c9daa389c1abc017168
msgid ""
"``--set ipam.mode=kubernetes`` is set to ``\"kubernetes\"`` since our "
"cloud provider has pod CIDR allocation enabled in ``kube-controller-"
"manager``."
msgstr ""

#: ../../operations/performance/scalability/report.rst:45
#: 39277f137d3c4799b3e8ec051413fa10
msgid ""
"``--set k8sServiceHost`` and ``--set k8sServicePort`` were set with the "
"IP address of the loadbalancer that was in front of ``kube-apiserver``. "
"This allows Cilium to not depend on kube-proxy to connect to ``kube-"
"apiserver``."
msgstr ""

#: ../../operations/performance/scalability/report.rst:49
#: dcf2891b102040659ed576993fee24a6
msgid ""
"``--set prometheus.enabled=true`` and ``--set "
"operator.prometheus.enabled=true`` were just set because we had a "
"Prometheus server probing for metrics in the entire cluster."
msgstr ""

#: ../../operations/performance/scalability/report.rst:53
#: cf2ca9b5d44f4140ad4ab5f171cc0e3d
msgid ""
"Our testing cluster consisted of 3 controller nodes and 1000 worker "
"nodes. We have followed the recommended settings from the `official "
"Kubernetes documentation <https://kubernetes.io/docs/setup/best-practices"
"/cluster-large/>`_ and have provisioned our machines with the following "
"settings:"
msgstr ""

#: ../../operations/performance/scalability/report.rst:58
#: 72366f623a454378aba5aacb0cb0ca2e
msgid "**Cloud provider**: Google Cloud"
msgstr ""

#: ../../operations/performance/scalability/report.rst:60
#: 0024945e6db64c54a47f9148a16dd2d0
msgid ""
"**Controllers**: 3x n1-standard-32 (32vCPU, 120GB memory and 50GB SSD, "
"kernel 5.4.0-1009-gcp)"
msgstr ""

#: ../../operations/performance/scalability/report.rst:62
#: 432053d232b84f228415832312c60848
msgid ""
"**Workers**: 1 pool of 1000x custom-2-4096 (2vCPU, 4GB memory and 10GB "
"HDD, kernel 5.4.0-1009-gcp)"
msgstr ""

#: ../../operations/performance/scalability/report.rst:64
#: d27b39e4305a4f0582280637f7d0f561
msgid ""
"**Metrics**: 1x n1-standard-32 (32vCPU, 120GB memory and 10GB HDD + 500GB"
" HDD) this is a dedicated node for prometheus and grafana pods."
msgstr ""

#: ../../operations/performance/scalability/report.rst:69
#: 0539338e3f874d43814d3d5194954dba
msgid "All 3 controller nodes were behind a GCE load balancer."
msgstr ""

#: ../../operations/performance/scalability/report.rst:71
#: fafc8d39bf254ca896061fed65075fcf
msgid ""
"Each controller contained ``etcd``, ``kube-apiserver``, ``kube-"
"controller-manager`` and ``kube-scheduler`` instances."
msgstr ""

#: ../../operations/performance/scalability/report.rst:74
#: 334cc7501e974460adaa01d6d35dae66
msgid ""
"The CPU, memory and disk size set for the workers might be different for "
"your use case. You might have pods that require more memory or CPU "
"available so you should design your workers based on your requirements."
msgstr ""

#: ../../operations/performance/scalability/report.rst:78
#: 2a288de901cd48eaade12dacbffa81ec
msgid ""
"During our testing we had to set the ``etcd`` option ``quota-backend-"
"bytes=17179869184`` because ``etcd`` failed once it reached around "
"``2GiB`` of allocated space."
msgstr ""

#: ../../operations/performance/scalability/report.rst:82
#: 292e1051f8e740ba83b6105e2a163091
msgid ""
"We provisioned our worker nodes without ``kube-proxy`` since Cilium is "
"capable of performing all functionalities provided by ``kube-proxy``. We "
"created a load balancer in front of ``kube-apiserver`` to allow Cilium to"
" access ``kube-apiserver`` without ``kube-proxy``, and configured Cilium "
"with the options ``--set k8sServiceHost=<KUBE-APISERVER-LB-IP-ADDRESS>`` "
"and ``--set k8sServicePort=<KUBE-APISERVER-LB-PORT-NUMBER>``."
msgstr ""

#: ../../operations/performance/scalability/report.rst:89
#: e905806f30f94967bf1826c806b96487
msgid ""
"Our ``DaemonSet`` ``updateStrategy`` had the ``maxUnavailable`` set to "
"250 pods instead of 2, but this value highly depends on your requirements"
" when you are performing a rolling update of Cilium."
msgstr ""

#: ../../operations/performance/scalability/report.rst:95
#: dbe638860e974efb94f278ff7b03b3b6
msgid "Steps"
msgstr ""

#: ../../operations/performance/scalability/report.rst:97
#: 792f3401441446a1a2e86a26209a1082
msgid ""
"For each step we took, we provide more details below, with our findings "
"and expected behaviors."
msgstr ""

#: ../../operations/performance/scalability/report.rst:102
#: f6dbe419044c4b7a9e9aa7bf4581046e
msgid "1. Install Kubernetes v1.18.3 with EndpointSlice feature enabled"
msgstr ""

#: ../../operations/performance/scalability/report.rst:104
#: 288e101e69ef40a1a7a301ae8d7f5f0d
msgid ""
"To test the most up-to-date functionalities from Kubernetes and Cilium, "
"we have performed our testing with Kubernetes v1.18.3 and the "
"EndpointSlice feature enabled to improve scalability."
msgstr ""

#: ../../operations/performance/scalability/report.rst:108
#: 4e90526687e744f89f531f951cdddbc8
msgid "Since Kubernetes requires an ``etcd`` cluster, we have deployed v3.4.9."
msgstr ""

#: ../../operations/performance/scalability/report.rst:112
#: a8001b13993b4cd7b88cde30af04d4c5
msgid "2. Deploy Prometheus, Grafana and Cilium"
msgstr ""

#: ../../operations/performance/scalability/report.rst:114
#: 6b49142f809d4a909bf07312e2b677a2
msgid ""
"We have used Prometheus v2.18.1 and Grafana v7.0.1 to retrieve and "
"analyze ``etcd``, ``kube-apiserver``, ``cilium`` and ``cilium-operator`` "
"metrics."
msgstr ""

#: ../../operations/performance/scalability/report.rst:119
#: c18163672d2443caaf4328e06fac8deb
msgid "3. Provision 2 worker nodes"
msgstr ""

#: ../../operations/performance/scalability/report.rst:121
#: 49c0a902466b42bc8c246b6d2e741099
msgid ""
"This helped us to understand if our testing cluster was correctly "
"provisioned and all metrics were being gathered."
msgstr ""

#: ../../operations/performance/scalability/report.rst:126
#: fb494ed676cf45c187bfeca27d8453e1
msgid "4. Deploy 5 namespaces with 25 deployments on each namespace"
msgstr ""

#: ../../operations/performance/scalability/report.rst:128
#: e5b041b86a104fa1be9e05e8ccb9ac93
msgid "Each deployment had 1 replica (125 pods in total)."
msgstr ""

#: ../../operations/performance/scalability/report.rst:130
#: d723c70a722d4b9dafe2a2d17cebd6bb
msgid ""
"To measure **only** the resources consumed by Cilium, all deployments "
"used the same base image ``k8s.gcr.io/pause:3.2``. This image does not "
"have any CPU or memory overhead."
msgstr ""

#: ../../operations/performance/scalability/report.rst:134
#: d5c7075a3b9544839c5e48bd2496896d
msgid ""
"We provision a small number of pods in a small cluster to understand the "
"CPU usage of Cilium:"
msgstr ""

#: ../../operations/performance/scalability/report.rst:139
#: cce7acde45dc473a95862f4b824ff9c9
msgid ""
"The mark shows when the creation of 125 pods started. As expected, we can"
" see a slight increase of the CPU usage on both Cilium agents running and"
" in the Cilium operator. The agents peaked at 6.8% CPU usage on a 2vCPU "
"machine."
msgstr ""

#: ../../operations/performance/scalability/report.rst:146
#: 445770b269c94c04b3d4df125c506793
msgid ""
"For the memory usage, we have not seen a significant memory growth in the"
" Cilium agent. On the eBPF memory side, we do see it increasing due to "
"the initialization of some eBPF maps for the new pods."
msgstr ""

#: ../../operations/performance/scalability/report.rst:152
#: 9248972916d5406ab8f1cabec48016d1
msgid "5. Provision 998 additional nodes (total 1000 nodes)"
msgstr ""

#: ../../operations/performance/scalability/report.rst:156
#: 3bae5df2ba444b3b86dfe5170d08344b
#, python-format
msgid ""
"The first mark represents the action of creating nodes, the second mark "
"when 1000 Cilium pods were in ready state. The CPU usage increase is "
"expected since each Cilium agent receives events from Kubernetes whenever"
" a new node is provisioned in the cluster. Once all nodes were deployed "
"the CPU usage was 0.15% on average on a 2vCPU node."
msgstr ""

#: ../../operations/performance/scalability/report.rst:164
#: a733b6a0564e48298dd016ef5a3231cd
msgid ""
"As we have increased the number of nodes in the cluster to 1000, it is "
"expected to see a small growth of the memory usage in all metrics. "
"However, it is relevant to point out that **an increase in the number of "
"nodes does not cause any significant increase in Ciliumâ€™s memory "
"consumption in both control and dataplane.**"
msgstr ""

#: ../../operations/performance/scalability/report.rst:172
#: cb5d70d3c5b14fec97da8424055b8f1a
msgid "6. Deploy 25 more deployments on each namespace"
msgstr ""

#: ../../operations/performance/scalability/report.rst:174
#: a0ae437a0ca942d9adbf03590aebc68c
msgid ""
"This will now bring us a total of ``5 namespaces * (25 old deployments + "
"25 new deployments)=250`` deployments in the entire cluster. We did not "
"install 250 deployments from the start since we only had 2 nodes and that"
" would create 125 pods on each worker node. According to the Kubernetes "
"documentation the maximum recommended number of pods per node is 100."
msgstr ""

#: ../../operations/performance/scalability/report.rst:183
#: 54a5eabe9d16497e9562bc6c1defc3cd
msgid "7. Scale each deployment to 200 replicas (50000 pods in total)"
msgstr ""

#: ../../operations/performance/scalability/report.rst:185
#: 0c4786b59a7d48f5abac80bd13a0fcbb
msgid ""
"Having 5 namespaces with 50 deployments means that we have 250 different "
"unique security identities. Having a low cardinality in the labels "
"selected by Cilium helps scale the cluster. By default, Cilium has a "
"limit of 16k security identities, but it can be increased with ``bpf-"
"policy-map-max`` in the Cilium ``ConfigMap``."
msgstr ""

#: ../../operations/performance/scalability/report.rst:193
#: 7371b9fa1a54424097a3ae061b35522e
msgid ""
"The first mark represents the action of scaling up the deployments, the "
"second mark when 50000 pods were in ready state."
msgstr ""

#: ../../operations/performance/scalability/report.rst:196
#: 14936796f3924d3a902c9e7044fe5812
msgid ""
"It is expected to see the CPU usage of Cilium increase since, on each "
"node, Cilium agents receive events from Kubernetes when a new pod is "
"scheduled and started."
msgstr ""

#: ../../operations/performance/scalability/report.rst:200
#: 2ab49ed3ec5c441abbb4cfb9eb3285ef
#, python-format
msgid ""
"The average CPU consumption of all Cilium agents was 3.38% on a 2vCPU "
"machine. At one point, roughly around minute 15:23, one of those Cilium "
"agents picked 27.94% CPU usage."
msgstr ""

#: ../../operations/performance/scalability/report.rst:204
#: cd1be9fdb2dc4ec898e449ced480b8d4
msgid ""
"Cilium Operator had a stable 5% CPU consumption while the pods were being"
" created."
msgstr ""

#: ../../operations/performance/scalability/report.rst:209
#: ee513038f21040daa80c76f46ab7ba57
msgid ""
"Similar to the behavior seen while increasing the number of worker nodes,"
" adding new pods also increases Cilium memory consumption."
msgstr ""

#: ../../operations/performance/scalability/report.rst:212
#: 08c8e41ab8874f30895e2bc21d0bce14
msgid ""
"As we increased the number of pods from 250 to 50000, we saw a maximum "
"memory usage of 573MiB for one of the Cilium agents while the average was"
" 438 MiB."
msgstr ""

#: ../../operations/performance/scalability/report.rst:214
#: f16260b3950c48f49832594b0f64291e
msgid "For the eBPF memory usage we saw a max usage of 462.7MiB"
msgstr ""

#: ../../operations/performance/scalability/report.rst:215
#: 7354c2168e584becb57e09c94de5470d
msgid ""
"This means that each **Cilium agent's memory increased by 10.5KiB per new"
" pod in the cluster.**"
msgstr ""

#: ../../operations/performance/scalability/report.rst:220
#: 5addff6006cc4c7b8af41ab3f6dbaffb
msgid "8. Deploy 250 policies for 1 namespace"
msgstr ""

#: ../../operations/performance/scalability/report.rst:222
#: 47cafc1a58714122ad142fd2f4029bf1
msgid ""
"Here we have created 125 L4 network policies and 125 L7 policies. Each "
"policy selected all pods on this namespace and was allowed to send "
"traffic to another pod on this namespace. Each of the 250 policies allows"
" access to a disjoint set of ports. In the end we will have 250 different"
" policies selecting 10000 pods."
msgstr ""

#: ../../operations/performance/scalability/report.rst:275
#: 4031cd89aacf459dba05729259b5bbdb
#, python-format
msgid ""
"In this case we saw one of the Cilium agents jumping to 100% CPU usage "
"for 15 seconds while the average peak was 40% during a period of 90 "
"seconds."
msgstr ""

#: ../../operations/performance/scalability/report.rst:280
#: 0c6cd4d2d6704c65a22e1d80b1a6ae5e
msgid ""
"As expected, **increasing the number of policies does not have a "
"significant impact on the memory usage of Cilium since the eBPF policy "
"maps have a constant size** once a pod is initialized."
msgstr ""

#: ../../operations/performance/scalability/report.rst:288
#: 1091f1817aa9404cb12f4fd44528d0af
msgid ""
"The first mark represents the point in time when we ran ``kubectl "
"create`` to create the ``CiliumNetworkPolicies``. Since we created the "
"250 policies sequentially, we cannot properly compute the convergence "
"time. To do that, we could use a single CNP with multiple policy rules "
"defined under the ``specs`` field (instead of the ``spec`` field)."
msgstr ""

#: ../../operations/performance/scalability/report.rst:294
#: 5e9fe6d12d4f4b34ba077e7758f9dc62
msgid ""
"Nevertheless, we can see the time it took the last Cilium agent to "
"increment its Policy Revision, which is incremented individually on each "
"Cilium agent every time a CiliumNetworkPolicy (CNP) is received, between "
"second ``15:45:44`` and ``15:45:46`` and see when was the last time an "
"Endpoint was regenerated by checking the 99th percentile of the "
"\"Endpoint regeneration time\". In this manner, that it took less than "
"5s. We can also verify **the maximum time was less than 600ms for an "
"endpoint to have the policy enforced.**"
msgstr ""

#: ../../operations/performance/scalability/report.rst:305
#: 6150a88946a041d3b3799dd286bff18b
msgid "9. Deploy 250 policies for CiliumClusterwideNetworkPolicies (CCNP)"
msgstr ""

#: ../../operations/performance/scalability/report.rst:307
#: 80c0de83238f4c77ab5050a48f52b9b0
msgid ""
"The difference between these policies and the previous ones installed is "
"that these select all pods in all namespaces. To recap, this means that "
"we will now have **250 different network policies selecting 10000 pods "
"and 250 different network policies selecting 50000 pods on a cluster with"
" 1000 nodes.** Similarly to the previous step we will deploy 125 L4 "
"policies and another 125 L7 policies."
msgstr ""

#: ../../operations/performance/scalability/report.rst:316
#: 7fbb750880ac43dfbd6c41c2ae257746
msgid ""
"Similar to the creation of the previous 250 CNPs, there was also an "
"increase in CPU usage during the creation of the CCNPs. The CPU usage was"
" similar even though the policies were effectively selecting more pods."
msgstr ""

#: ../../operations/performance/scalability/report.rst:322
#: 1f7279ab529d4ec2874bcfd111a2f854
msgid ""
"As all pods running in a node are selected by **all 250 CCNPs created**, "
"we see an increase of the **Endpoint regeneration time** which **peaked a"
" little above 3s.**"
msgstr ""

#: ../../operations/performance/scalability/report.rst:329
#: 32a1e521f65d4fc3bc6163aef72671f5
msgid "10. \"Accidentally\" delete 10000 pods"
msgstr ""

#: ../../operations/performance/scalability/report.rst:331
#: 343d4c33900d4b5fac6eecaf5f29ac7d
msgid ""
"In this step we have \"accidentally\" deleted 10000 random pods. "
"Kubernetes will then recreate 10000 new pods so it will help us "
"understand what the convergence time is for all the deployed network "
"polices."
msgstr ""

#: ../../operations/performance/scalability/report.rst:339
#: c0be07d9bba24c9b9f87421ec3839489
msgid ""
"The first mark represents the point in time when pods were \"deleted\" "
"and the second mark represents the point in time when Kubernetes finished"
" recreating 10k pods."
msgstr ""

#: ../../operations/performance/scalability/report.rst:343
#: fa4f5bad66974b74bc6ae5e52f835911
msgid ""
"Besides the CPU usage slightly increasing while pods are being scheduled "
"in the cluster, we did see some interesting data points in the eBPF "
"memory usage. As each endpoint can have one or more dedicated eBPF maps, "
"the eBPF memory usage is directly proportional to the number of pods "
"running in a node. **If the number of pods per node decreases so does the"
" eBPF memory usage.**"
msgstr ""

#: ../../operations/performance/scalability/report.rst:351
#: 2a01a3d5ab31482493dda783ac7a8b34
msgid ""
"We inferred the time it took for all the endpoints to get regenerated by "
"looking at the number of Cilium endpoints with the policy enforced over "
"time. Luckily enough we had another metric that was showing how many "
"Cilium endpoints had policy being enforced:"
msgstr ""

#: ../../operations/performance/scalability/report.rst:360
#: fd9cbf580ecf43cca37ce67a42be78d8
msgid "11. Control plane metrics over the test run"
msgstr ""

#: ../../operations/performance/scalability/report.rst:362
#: 6e64a7557b164e0a82e27240e1a942ee
msgid ""
"The focus of this test was to study the Cilium agent resource consumption"
" at scale. However, we also monitored some metrics of the control plane "
"nodes such as etcd metrics and CPU usage of the k8s-controllers and we "
"present them in the next figures."
msgstr ""

#: ../../operations/performance/scalability/report.rst:369
#: b4f6776ca14f4fdbb13673a5b0db5e20
msgid ""
"Memory consumption of the 3 etcd instances during the entire scalability "
"testing."
msgstr ""

#: ../../operations/performance/scalability/report.rst:374
#: b58015b7fd114d7da0747bcab829599d
msgid ""
"CPU usage for the 3 controller nodes, average latency per request type in"
" the etcd cluster as well as the number of operations per second made to "
"etcd."
msgstr ""

#: ../../operations/performance/scalability/report.rst:379
#: 862e5dbbfeaf48a19d286fa18bb67c72
msgid ""
"All etcd metrics, from left to right, from top to bottom: database size, "
"disk sync duration, client traffic in, client traffic out, peer traffic "
"in, peer traffic out."
msgstr ""

#: ../../operations/performance/scalability/report.rst:385
#: 0d66b4596a6f43df871fd1f53fb25360
msgid "Final Remarks"
msgstr ""

#: ../../operations/performance/scalability/report.rst:387
#: 47797b957cd844808481713c0c96abf7
msgid ""
"These experiments helped us develop a better understanding of Cilium "
"running in a large cluster entirely in CRD mode and without depending on "
"etcd. There is still some work to be done to optimize the memory "
"footprint of eBPF maps even further, as well as reducing the memory "
"footprint of the Cilium agent. We will address those in the next Cilium "
"version."
msgstr ""

#: ../../operations/performance/scalability/report.rst:393
#: f27c9e7d45ae4074906060d93c3867a5
msgid ""
"We can also determine that it is scalable to run Cilium in CRD mode on a "
"cluster with more than 200 nodes. However, it is worth pointing out that "
"we need to run more tests to verify Cilium's behavior when it loses the "
"connectivity with ``kube-apiserver``, as can happen during a control "
"plane upgrade for example. This will also be our focus in the next Cilium"
" version."
msgstr ""

