name: 100 Nodes Scale Test

on:
  schedule:
    - cron: '39 0 * * 1-5'

# For testing uncomment following lines:
  push:
    branches:
      - thorn3rCES

permissions:
  # To be able to access the repository with actions/checkout
  contents: read
  # To be able to request the JWT from GitHub's OIDC provider
  id-token: write

concurrency:
  # Structure:
  # - Workflow name
  # - Event type
  # - A unique identifier depending on event type:
  #   - schedule: SHA
  #   - workflow_dispatch: PR number
  #
  # This structure ensures a unique concurrency group name is generated for each
  # type of testing, such that re-runs will cancel the previous run.
  group: |
    ${{ github.workflow }}
    ${{ github.event_name }}
    ${{
      (github.event_name == 'schedule' && github.sha) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.PR-number)
    }}
  cancel-in-progress: true

env:
  # renovate: datasource=golang-version depName=go
  go_version: 1.22.4
  # Adding k8s.local to the end makes kops happy-
  # has stricter DNS naming requirements.
  test_name: scale-100
  cluster_name: ${{ github.run_id }}-${{ github.run_attempt }}
  # renovate: datasource=docker depName=google/cloud-sdk
  gcloud_version: 479.0.0
  kubectl_version: v1.30.1

jobs:
  install-and-scaletest:
    runs-on: ubuntu-latest-8cores-32gb-22.04
    name: Install and Scale Test
    timeout-minutes: 360
    steps:
      - name: Checkout context ref (trusted)
        uses: actions/checkout@a5ac7e51b41094c92402da3b24376905380afc29 # v4.1.6
        with:
          ref: ${{ inputs.context-ref || github.sha }}
          persist-credentials: false

      - name: Set Environment Variables
        uses: ./.github/actions/set-env-variables

      - name: Set up job variables
        id: vars
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] ; then
            SHA="${{ inputs.image-tag }}"
          else
            SHA="${{ github.sha }}"
          fi

          CILIUM_INSTALL_DEFAULTS="--chart-directory=install/kubernetes/cilium \
            --set pprof.enabled=true \
            --helm-set=prometheus.enabled=true \
            --helm-set=cluster.name=${{ env.cluster_name }} \
            --wait=false"

          # only add SHA to the image tags if it was set
          if [ -n "${SHA}" ]; then
            echo sha=${SHA} >> $GITHUB_OUTPUT
            CILIUM_INSTALL_DEFAULTS+=" --helm-set=image.repository=quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/cilium-ci \
            --helm-set=image.useDigest=false \
            --helm-set=image.tag=${SHA} \
            --helm-set=operator.image.repository=quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/operator \
            --helm-set=operator.image.suffix=-ci \
            --helm-set=operator.image.tag=${SHA} \
            --helm-set=operator.image.useDigest=false \
            --helm-set=operator.extraArgs=\"{--k8s-client-qps=100,--k8s-client-burst=200}\" \
            --helm-set=clustermesh.apiserver.image.repository=quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/clustermesh-apiserver-ci \
            --helm-set=clustermesh.apiserver.image.tag=${SHA} \
            --helm-set=clustermesh.apiserver.image.useDigest=false \
            --helm-set=hubble.relay.image.repository=quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/hubble-relay-ci \
            --helm-set=hubble.relay.image.tag=${SHA} \
            --helm-set=hubble.relay.image.useDigest=false \
            --helm-set=endpointHealthChecking.enabled=false \
            --helm-set=healthChecking=false \
            --helm-set=ciliumEndpointSlice.enabled=true \
            --helm-set=ciliumEndpointSlice.sliceMode="cesSliceModeFCFS" \
            --helm-set=ciliumEndpointSlice.rateLimits[0].nodes=1 \
            --helm-set=ciliumEndpointSlice.rateLimits[0].limit=100 \
            --helm-set=ciliumEndpointSlice.rateLimits[0].burst=200 \
            --helm-set=kubeProxyReplacement=true"
          fi

          # Adding k8s.local to the end makes kops happy
          # has stricter DNS naming requirements.
          CLUSTER_NAME="${{ env.test_name }}-${{ env.cluster_name }}.k8s.local"

          echo SHA=${SHA} >> $GITHUB_OUTPUT
          echo cilium_install_defaults=${CILIUM_INSTALL_DEFAULTS} >> $GITHUB_OUTPUT
          echo CLUSTER_NAME=${CLUSTER_NAME} >> $GITHUB_OUTPUT

      - name: Wait for images to be available
        timeout-minutes: 30
        shell: bash
        run: |
          for image in cilium-ci operator-generic-ci hubble-relay-ci ; do
            until docker manifest inspect quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/$image:${{ steps.vars.outputs.SHA }} &> /dev/null; do sleep 45s; done
          done

      - name: Install Go
        uses: actions/setup-go@cdcb36043654635271a94b9a6d1392de5bb323a7 # v5.0.1
        with:
          go-version: ${{ env.go_version }}

      - name: Install Cilium CLI
        uses: cilium/cilium-cli@511f0173c21db1c3c959b96fd68eef18f83a0a9f # v0.16.10
        with:
          repository: ${{ env.CILIUM_CLI_RELEASE_REPO }}
          release-version: ${{ env.CILIUM_CLI_VERSION }}

      - name: Install Kops
        uses: cilium/scale-tests-action/install-kops@238d773bd07754bfd693a6b22c94eddf3a12778d # main

      - name: Setup gcloud credentials
        uses: google-github-actions/auth@71fee32a0bb7e97b4d33d548e7d957010649d8fa # v2.1.3
        with:
          workload_identity_provider: ${{ secrets.GCP_PERF_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_PERF_SA }}
          create_credentials_file: true
          export_environment_variables: true

      - name: Install kubectl
        run: |
          curl -sLO "https://dl.k8s.io/release/${{ env.kubectl_version }}/bin/linux/amd64/kubectl"
          curl -sLO "https://dl.k8s.io/${{ env.kubectl_version }}/bin/linux/amd64/kubectl.sha256"
          echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Setup gcloud CLI
        uses: google-github-actions/setup-gcloud@98ddc00a17442e89a24bbf282954a3b65ce6d200 # v2.1.0
        with:
          project_id: ${{ secrets.GCP_PERF_PROJECT_ID }}
          version: ${{ env.gcloud_version }}

      - name: Clone ClusterLoader2
        uses: actions/checkout@a5ac7e51b41094c92402da3b24376905380afc29 # v4.1.6
        with:
          repository: kubernetes/perf-tests
          # Avoid using renovate to update this dependency because: (1)
          # perf-tests does not tag or release, so renovate will pull
          # all updates to the default branch and (2) continually
          # updating CL2 may impact the stability of the scale test
          # results.
          ref: 6eb52ac89d5de15a0ad13cfeb2b2026e57ce4f64
          persist-credentials: false
          sparse-checkout: clusterloader2
          path: perf-tests

      - name: Display version info of installed tools
        run: |
          echo "--- go ---"
          go version
          echo "--- cilium-cli ---"
          cilium version --client
          echo "--- kops ---"
          ./kops version
          echo "--- gcloud ---"
          gcloud version

      - name: Deploy cluster
        id: deploy-cluster
        uses: thorn3r/scale-tests-action/create-cluster@efbf8e395175459d4badde52d4473050d6986d60    # updates for 5k
        timeout-minutes: 60
        with:
          cluster_name: ${{ steps.vars.outputs.cluster_name }}
          control_plane_size: n2-standard-64
          control_plane_count: 5
          control_plane_volume_size: 64
          node_size: e2-medium
          node_count: 1000
          node_volume_size: 50
          kops_state: ${{ secrets.GCP_PERF_KOPS_STATE_STORE }}
          project_id: ${{ secrets.GCP_PERF_PROJECT_ID }}
          kube_proxy_enabled: false
          allocate_node_cidrs: false

      - name: Create 2nd Instance Group
        uses: thorn3r/scale-tests-action/create-instance-group@01b7f29b6eb04336793880301806c154e8bbd85f # updates for 5k
        timeout-minutes: 60
        with:
          cluster_name: ${{ steps.vars.outputs.cluster_name }}
          node_size: e2-medium
          node_count: 1000
          ig_name: ig2
          kops_state: ${{ secrets.GCP_PERF_KOPS_STATE_STORE }}

      - name: Create 3rd Instance Group
        uses: thorn3r/scale-tests-action/create-instance-group@01b7f29b6eb04336793880301806c154e8bbd85f # updates for 5k
        timeout-minutes: 60
        with:
          cluster_name: ${{ steps.vars.outputs.cluster_name }}
          node_size: e2-medium
          node_count: 1000
          ig_name: ig3
          kops_state: ${{ secrets.GCP_PERF_KOPS_STATE_STORE }}

      - name: Create Instance Group for resource heavy deployments
        uses: cilium/scale-tests-action/create-instance-group@238d773bd07754bfd693a6b22c94eddf3a12778d # main
        timeout-minutes: 30
        with:
          cluster_name: ${{ steps.vars.outputs.cluster_name }}
          node_size: e2-standard-32
          node_count: 1
          ig_name: heapster
          kops_state: ${{ secrets.GCP_PERF_KOPS_STATE_STORE }}

      - name: Setup firewall rules
        uses: cilium/scale-tests-action/setup-firewall@238d773bd07754bfd693a6b22c94eddf3a12778d # main
        with:
          cluster_name: ${{ steps.vars.outputs.cluster_name }}

      - name: Get apiserver loadbalancer address
        id: apiserver-vars
        run: |
          APISERVER_ADDRESS="$(gcloud compute addresses list --filter="addressType:external AND name~'${{ steps.vars.outputs.cluster_name }}'" --format json | jq '.[].address')"
          CILIUM_INSTALL_APISERVER=" --set k8sServiceHost=${APISERVER_ADDRESS} \
          --set k8sServicePort=443"

          echo cilium_install_apiserver=${CILIUM_INSTALL_APISERVER} >> $GITHUB_OUTPUT

      - name: Install Cilium
        run: |
          cilium install --dry-run-helm-values \
          ${{ steps.vars.outputs.cilium_install_defaults }} \
          ${{ steps.apiserver-vars.outputs.cilium_install_apiserver }}

          cilium install \
          ${{ steps.vars.outputs.cilium_install_defaults }} \
          ${{ steps.apiserver-vars.outputs.cilium_install_apiserver }}

      - name: Wait for cluster to be ready
        uses: thorn3r/scale-tests-action/validate-cluster@0dbf5b413892a408f947d9e6d8d89d21595757df # forked to allow configurable timeout
        timeout-minutes: 90
        with:
          cluster_name: ${{ steps.vars.outputs.cluster_name }}
          kops_state: ${{ secrets.GCP_PERF_KOPS_STATE_STORE }}
          timeout: "90m"

      - name: Wait for Cilium status to be ready
        run: |
          cilium status --wait --wait-duration=20m

      - name: Run CL2
        id: run-cl2
        working-directory: ./perf-tests/clusterloader2
        shell: bash
        timeout-minutes: 360
        run: |
          mkdir ./report
          export CL2_PROMETHEUS_PVC_ENABLED=false
          export CL2_ENABLE_PVS=false
          export CL2_ENABLE_NETWORKPOLICIES=true
          export CL2_ALLOWED_SLOW_API_CALLS=1
          export CL2_SCHEDULER_THROUGHPUT_THRESHOLD=0
          export CL2_PROMETHEUS_SCRAPE_CILIUM_OPERATOR=true
          export CL2_PROMETHEUS_SCRAPE_CILIUM_AGENT=true
          export CL2_PROMETHEUS_MEMORY_LIMIT_FACTOR=20.0
          export CL2_PROMETHEUS_MEMORY_SCALE_FACTOR=18.0
          export CL2_PROMETHEUS_SCRAPE_CILIUM_AGENT_INTERVAL="60s"
          export CL2_LOAD_TEST_THROUGHPUT=50.0
          export CL2_DELETE_TEST_THROUGHPUT=50.0

          # CL2 hardcodes module paths to live in ./testing/load, even
          # if the path given is relative.
          cp ../../.github/actions/cl2-modules/cilium-agent-pprofs.yaml ./testing/load/
          cp ../../.github/actions/cl2-modules/cilium-metrics.yaml ./testing/load/
          echo \
            '{"CL2_ADDITIONAL_MEASUREMENT_MODULES": ["./cilium-agent-pprofs.yaml", "./cilium-metrics.yaml"]}' \
            > modules.yaml

          # CL2 needs ssh access to control plane nodes
          gcloud compute config-ssh

          go run ./cmd/clusterloader.go \
            -v=2 \
            --testconfig=./testing/load/config.yaml \
            --provider=gce \
            --enable-prometheus-server \
            --tear-down-prometheus-server=false \
            --nodes=3000 \
            --report-dir=./report \
            --experimental-prometheus-snapshot-to-report-dir=true \
            --kubeconfig=$HOME/.kube/config \
            --testoverrides=./testing/experiments/use_simple_latency_query.yaml \
            --testoverrides=./testing/prometheus/not-scrape-kube-proxy.yaml \
            --testoverrides=./modules.yaml \
            2>&1 | tee cl2-output.txt

      - name: Cleanup cluster
        if: ${{ always() }}
        uses: cilium/scale-tests-action/cleanup-cluster@238d773bd07754bfd693a6b22c94eddf3a12778d # main
        with:
          cluster_name: ${{ steps.vars.outputs.cluster_name }}
          kops_state: ${{ secrets.GCP_PERF_KOPS_STATE_STORE }}

      - name: Export results and sysdump to GS bucket
        if: ${{ always() && steps.run-cl2.outcome != 'skipped' && steps.run-cl2.outcome != 'cancelled' }}
        uses: cilium/scale-tests-action/export-results@238d773bd07754bfd693a6b22c94eddf3a12778d # main
        with:
          test_name: ${{ env.test_name }}
          results_bucket: ${{ env.GCP_PERF_RESULTS_BUCKET }}
          artifacts: ./perf-tests/clusterloader2/report/*
          other_files: ./perf-tests/clusterloader2/cl2-output.txt
