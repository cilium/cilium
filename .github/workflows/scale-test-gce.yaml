name: Scale Test GCE

on:
  schedule:
    - cron: '39 0 * * 1,4'

permissions:
  # To be able to access the repository with actions/checkout
  contents: read

concurrency:
  # Structure:
  # - Workflow name
  # - Event type
  # - A unique identifier depending on event type:
  #   - schedule: SHA
  #   - workflow_dispatch: PR number
  #
  # This structure ensures a unique concurrency group name is generated for each
  # type of testing, such that re-runs will cancel the previous run.
  group: |
    ${{ github.workflow }}
    ${{ github.event_name }}
    ${{
      (github.event_name == 'schedule' && github.sha) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.PR-number)
    }}
  cancel-in-progress: true

env:
  # renovate: datasource=github-releases depName=cilium/cilium-cli
  cilium_cli_version: v0.15.13
  # renovate: datasource=github-releases depName=kubernetes/kops
  kops_version: v1.28.1
  # renovate: datasource=golang-version depName=go
  go_version: 1.21.4
  # Adding k8s.local to the end makes kops happy-
  # has stricter DNS naming requirements.
  cluster_name: ${{ github.repository_owner }}-${{ github.event.repository.name }}-${{ github.run_id }}-${{ github.run_attempt }}.k8s.local
  cl2_config: ${{ github.workspace }}/.github/actions/scale-test-gce/cl2-config.yaml
  CILIUM_CLI_MODE: helm

jobs:
  install-and-scaletest:
    runs-on: ubuntu-latest
    name: Install and Scale Test
    timeout-minutes: 120
    steps:
      - name: Checkout context ref (trusted)
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          ref: ${{ inputs.context-ref || github.sha }}
          persist-credentials: false

      - name: Set Environment Variables
        uses: ./.github/actions/set-env-variables
      
      - name: Get Cilium's default values
        id: default_vars
        uses: ./.github/actions/helm-default
        with:
          image-tag: ${{ github.sha }}

      - name: Set up job variables
        id: vars
        run: |
          SHA="${{ github.sha }}"

          # Setup Cilium install options
          CILIUM_INSTALL_DEFAULTS="${{ steps.default_vars.outputs.cilium_install_defaults }} \
            --wait=false"

          echo SHA=${SHA} >> $GITHUB_OUTPUT
          echo cilium_install_defaults=${CILIUM_INSTALL_DEFAULTS} >> $GITHUB_OUTPUT

      - name: Wait for images to be available
        timeout-minutes: 30
        shell: bash
        run: |
          for image in cilium-ci operator-generic-ci hubble-relay-ci ; do
            until docker manifest inspect quay.io/${{ env.QUAY_ORGANIZATION_DEV }}/$image:${{ steps.vars.outputs.SHA }} &> /dev/null; do sleep 45s; done
          done

      - name: Install Go
        uses: actions/setup-go@93397bea11091df50f3d7e59dc26a7711a8bcfbe # v4.1.0
        with:
          go-version: ${{ env.go_version }}

      - name: Install Cilium CLI
        uses: cilium/cilium-cli@6361bfd0c291800e6c1041ea3c6ed6d178242e6a # v0.15.13
        with:
          release-version: ${{ env.cilium_cli_version }}

      - name: Install Kops
        run: |
          curl -L --remote-name-all https://github.com/kubernetes/kops/releases/download/${{ env.kops_version }}/kops-linux-amd64{,.sha256}
          sed -i 's/$/ kops-linux-amd64/' kops-linux-amd64.sha256
          sha256sum --check kops-linux-amd64.sha256
          mv kops-linux-amd64 kops
          chmod +x kops

      - name: Setup gcloud credentials
        uses: google-github-actions/auth@35b0e87d162680511bf346c299f71c9c5c379033 # v1.1.1
        with:
          credentials_json: '${{ secrets.GCP_PERF_SA_KEY }}'

      - name: Setup gcloud CLI
        uses: google-github-actions/setup-gcloud@e30db14379863a8c79331b04a9969f4c1e225e0b # v1.1.1
        with:
          project_id: ${{ secrets.GCP_PERF_PROJECT_ID }}
          version: "405.0.0"
      
      - name: Clone ClusterLoader2
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          repository: learnitall/perf-tests
          ref: d7a97a6b7b9efc29ccb496f3ff54f42588006e96 # tinker/learnitall/100-node-scale-test
          persist-credentials: false
          sparse-checkout: clusterloader2
          path: perf-tests
      
      - name: Display version info of installed tools
        run: |
          echo "--- go ---"
          go version
          echo "--- cilium-cli ---"
          cilium version --client
          echo "--- kops ---"
          ./kops version
          echo "--- gcloud ---"
          gcloud version
      
      # Control plane node size corresponds to related test in k8s OSS:
      # https://github.com/kubernetes/test-infra/blob/275098e2666698d7e78b39c3716f202ea62f4b80/config/jobs/kubernetes/sig-scalability/sig-scalability-presets.yaml#L269
      - name: Create cluster
        run: |
          ./kops create cluster \
            ${{ env.cluster_name }} \
            --state ${{ secrets.GCP_PERF_KOPS_STATE_STORE }} \
            --zones us-west1-a \
            --project=${{ secrets.GCP_PERF_PROJECT_ID }} \
            --control-plane-count 1 \
            --control-plane-size n2-standard-4 \
            --node-count 1 \
            --node-size e2-standard-8 \
            --networking cni \
            --yes
      
      - name: Dump cluster config
        run: |
          ./kops get clusters \
            ${{ env.cluster_name }} \
            --state ${{ secrets.GCP_PERF_KOPS_STATE_STORE }} \
            -o yaml

      - name: Ensure cluster state matches config
        id: deploy-cluster
        run: |
          ./kops update cluster \
            ${{ env.cluster_name }} \
            --state ${{ secrets.GCP_PERF_KOPS_STATE_STORE }} \
            --yes
  
      - name: Open Firewall rule for node-to-cp traffic
        run: |
          rule_name="node-to-master-${{ env.cluster_name }}"
          rule_name="${rule_name//./-}"
          gcloud compute firewall-rules update \
            $rule_name \
            --allow tcp,udp,icmp,esp,ah,sctp
          # See https://github.com/kubernetes/perf-tests/issues/2319
          # and https://docs.cilium.io/en/stable/operations/system_requirements/#firewall-rules.
          # Rules could be added individually, but this is a bit simpler,
          # matches the rules used by kops to manage cp-to-node and node-to-node
          # traffic, and is an acceptable risk since the cluster is ephemeral and not
          # running in prod.
      
      - name: Get kubeconfig
        run: |
          ./kops export kubeconfig \
            ${{ env.cluster_name }} \
            --state ${{ secrets.GCP_PERF_KOPS_STATE_STORE }} \
            --admin \
      
      - name: Wait for cluster control plane
        timeout-minutes: 20
        run: |
          until kubectl get nodes; do sleep 45s; done
      
      - name: Install Cilium
        run: |
          cilium install ${{ steps.vars.outputs.cilium_install_defaults }}

      - name: Wait for cluster to be ready
        run: |
          ./kops validate cluster \
            ${{ env.cluster_name }} \
            --state ${{ secrets.GCP_PERF_KOPS_STATE_STORE }} \
            --wait 20m \
            --count 3

      - name: Wait for Cilium status to be ready
        run: |
          cilium status --wait
      
      - name: Run CL2
        id: run-cl2
        working-directory: ./perf-tests/clusterloader2
        # --enable-exec-service=false to reduce number of pods so 100 pods can fit in node
        # POD_STARTUP_LATENCY_THRESHOLD=60s so the test doesn't fail, currently we have ~30s pods startup latency
        run: |
          mkdir ./report
          echo POD_STARTUP_LATENCY_THRESHOLD: 60s >> ./testoverrides.yaml
          go run ./cmd/clusterloader.go \
            -v=4 \
            --testconfig=./testing/node-throughput/config.yaml \
            --testoverrides=./testoverrides.yaml \
            --enable-exec-service=false \
            --provider=gce \
            --enable-prometheus-server \
            --tear-down-prometheus-server=false \
            --report-dir=./report \
            --kubeconfig=$HOME/.kube/config \
            2>&1 | tee cl2-output.txt

      - name: Get sysdump
        if: ${{ always() && steps.run-cl2.outcome != 'skipped' }}
        run: |
          cilium status
          cilium sysdump --output-filename cilium-sysdump-final

      - name: Cleanup cluster
        if: ${{ always() && steps.deploy-cluster.outcome != 'skipped' }}
        run: |
          ./kops delete cluster \
            ${{ env.cluster_name }} \
            --state ${{ secrets.GCP_PERF_KOPS_STATE_STORE }} \
            --yes

      - name: Export results and sysdump to GS bucket
        if: ${{ always() && steps.run-cl2.outcome != 'skipped' }}
        run: |
          echo ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }} >> ./run-url.txt
          echo ${{ github.sha }} >> sha.txt
          # BUILD_ID needs to be an integer
          BUILD_ID="$(date +%s)"
          EXPORT_DIR="${{ secrets.GCP_PERF_RESULTS_BUCKET }}/logs/scale-test-${{ github.ref_name }}/${BUILD_ID}"
          gsutil -m cp -R cilium-sysdump-final.zip ./perf-tests/clusterloader2/report/* $EXPORT_DIR/artifacts/
          gsutil -m cp sha.txt run-url.txt ./perf-tests/clusterloader2/cl2-output.txt  $EXPORT_DIR/
