diff --git a/.github/actions/e2e/configs.yaml b/.github/actions/e2e/configs.yaml
index 6ca6263df4..fbb0718c5c 100644
--- a/.github/actions/e2e/configs.yaml
+++ b/.github/actions/e2e/configs.yaml
@@ -1,6 +1,6 @@
 - name: '1'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: 'rhel8.6-20250521.025913'
+  kernel: 'rhel8.6-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'vxlan'
@@ -8,7 +8,7 @@
   misc: 'bpfClockProbe=false,cni.uninstall=false,tls.readSecretsOnlyFromSecretsNamespace=false,tls.secretSync.enabled=false'
 - name: '2'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.10-20250521.025913'
+  kernel: '5.10-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'disabled'
@@ -16,7 +16,7 @@
   misc: 'bpfClockProbe=false,cni.uninstall=false,tls.readSecretsOnlyFromSecretsNamespace=false,tls.secretSync.enabled=false'
 - name: '3'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.10-20250521.025913'
+  kernel: '5.10-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'disabled'
@@ -24,7 +24,7 @@
   kvstore: 'true'
 - name: '4'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.10-20250521.025913'
+  kernel: '5.10-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -36,7 +36,7 @@
   ingress-controller: 'true'
 - name: '5'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.15-20250521.025913'
+  kernel: '5.15-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -49,10 +49,11 @@
   ingress-controller: 'true'
 - name: '6'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.1-20250521.025913'
+  kernel: '6.1-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
+  secondary-network: 'true'
   tunnel: 'vxlan'
   lb-mode: 'snat'
   egress-gateway: 'true'
@@ -62,7 +63,7 @@
   bgp-control-plane: 'true'
 - name: '7'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -77,7 +78,7 @@
   misc: 'bpfClockProbe=false,cni.uninstall=false,tls.secretsBackend=k8s,tls.secretSync.enabled=true'
 - name: '8'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'geneve'
@@ -87,7 +88,7 @@
   node-local-dns: 'true'
 - name: '9'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.10-20250521.025913'
+  kernel: '5.10-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -101,7 +102,7 @@
   ingress-controller: 'true'
 - name: '10'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.15-20250521.025913'
+  kernel: '5.15-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'disabled'
@@ -114,7 +115,7 @@
   kvstore: 'true'
 - name: '11'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.1-20250521.025913'
+  kernel: '6.1-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -130,7 +131,7 @@
   node-local-dns: 'true'
 - name: '12'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -144,7 +145,7 @@
   ingress-controller: 'true'
 - name: '13'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: 'rhel8.6-20250521.025913'
+  kernel: 'rhel8.6-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'vxlan'
@@ -155,7 +156,7 @@
   # explains why 5.4 might cause north-south-loadbalancing tests to
   # fail.
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.15-20250521.025913'
+  kernel: '5.15-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -167,7 +168,7 @@
   ingress-controller: 'true'
 - name: '15'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -179,7 +180,7 @@
   ciliumendpointslice: 'true'
 - name: '16'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.15-20250521.025913'
+  kernel: '5.15-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -193,7 +194,7 @@
   ingress-controller: 'true'
 - name: '17'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   misc: 'bpf.datapathMode=netkit,enableIPv4BIGTCP=true,enableIPv6BIGTCP=true'
   kube-proxy: 'none'
   kpr: 'true'
@@ -204,7 +205,7 @@
   ingress-controller: 'true'
 - name: '18'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   misc: 'bpf.datapathMode=netkit-l2,enableIPv4BIGTCP=true,enableIPv6BIGTCP=true'
   kube-proxy: 'none'
   kpr: 'true'
@@ -215,7 +216,7 @@
   ingress-controller: 'true'
 - name: '19'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   misc: 'bpf.datapathMode=netkit'
   kube-proxy: 'none'
   kpr: 'true'
@@ -226,7 +227,7 @@
   kvstore: 'true'
 - name: '20'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   misc: 'bpf.datapathMode=netkit-l2'
   kube-proxy: 'none'
   kpr: 'true'
@@ -236,7 +237,7 @@
   ingress-controller: 'true'
 - name: '21'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   misc: 'bpf.datapathMode=netkit'
   kube-proxy: 'none'
   kpr: 'true'
@@ -246,7 +247,7 @@
   ingress-controller: 'true'
 - name: '22'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   misc: 'bpf.datapathMode=netkit-l2'
   kube-proxy: 'none'
   kpr: 'true'
@@ -256,7 +257,7 @@
   ingress-controller: 'true'
 - name: '23'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   misc: 'bpf.datapathMode=netkit'
   kube-proxy: 'none'
   kpr: 'true'
@@ -268,10 +269,12 @@
   host-fw: 'true'
 - name: '24'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   misc: 'bpf.datapathMode=netkit,enableIPv4BIGTCP=true,enableIPv6BIGTCP=true'
   kube-proxy: 'none'
   kpr: 'true'
+  devices: '{eth0,eth1}'
+  secondary-network: 'true'
   ipv6: 'true'
   tunnel: 'disabled'
   ciliumendpointslice: 'true'
@@ -279,10 +282,12 @@
   ingress-controller: 'true'
 - name: '25'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   misc: 'bpf.datapathMode=netkit-l2,enableIPv4BIGTCP=true,enableIPv6BIGTCP=true'
   kube-proxy: 'none'
   kpr: 'true'
+  devices: '{eth0,eth1}'
+  secondary-network: 'true'
   ipv6: 'true'
   tunnel: 'disabled'
   ciliumendpointslice: 'true'
@@ -290,7 +295,7 @@
   ingress-controller: 'true'
 - name: '26'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.6-20250521.025913'
+  kernel: '6.6-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -301,7 +306,7 @@
   ingress-controller: 'true'
 - name: '27'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.6-20250521.025913'
+  kernel: '6.6-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -312,7 +317,7 @@
   ingress-controller: 'true'
 - name: '28'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.6-20250521.025913'
+  kernel: '6.6-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -324,7 +329,7 @@
   ingress-controller: 'true'
 - name: '29'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -335,7 +340,7 @@
   ingress-controller: 'true'
 - name: '30'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -346,7 +351,7 @@
   ingress-controller: 'true'
 - name: '31'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
   devices: '{eth0,eth1}'
@@ -358,9 +363,11 @@
   ingress-controller: 'true'
 - name: '32'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.15-20250521.025913'
+  kernel: '5.15-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
+  devices: '{eth0,eth1}'
+  secondary-network: 'true'
   ipv6: 'true'
   ipv4: 'false'
   tunnel: 'vxlan'
@@ -368,7 +375,7 @@
   skip-upgrade: 'true'
 - name: '33'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   ipv6: 'true'
diff --git a/.github/actions/e2e/ipsec_configs.yaml b/.github/actions/e2e/ipsec_configs.yaml
index 4496899396..a8a10ec156 100644
--- a/.github/actions/e2e/ipsec_configs.yaml
+++ b/.github/actions/e2e/ipsec_configs.yaml
@@ -1,6 +1,6 @@
 - name: 'ipsec-1'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: 'rhel8.6-20250521.025913'
+  kernel: 'rhel8.6-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'vxlan'
@@ -9,7 +9,7 @@
   key-two: 'gcm(aes)'
 - name: 'ipsec-2'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.10-20250521.025913'
+  kernel: '5.10-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'disabled'
@@ -18,7 +18,7 @@
   key-two: 'cbc(aes)'
 - name: 'ipsec-3'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.10-20250521.025913'
+  kernel: '5.10-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'disabled'
@@ -29,7 +29,7 @@
   kvstore: 'true'
 - name: 'ipsec-4'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'geneve'
@@ -40,18 +40,22 @@
   kvstore: 'true'
 - name: 'ipsec-5'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.10-20250521.025913'
+  kernel: '5.10-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
+  devices: '{eth0,eth1}'
+  secondary-network: 'true'
   tunnel: 'disabled'
   encryption: 'ipsec'
   key-one: 'cbc(aes)'
   key-two: 'cbc(aes)'
 - name: 'ipsec-6'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.10-20250521.025913'
+  kernel: '5.10-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
+  devices: '{eth0,eth1}'
+  secondary-network: 'true'
   tunnel: 'disabled'
   endpoint-routes: 'true'
   egress-gateway: 'true'
@@ -61,7 +65,7 @@
   key-two: 'gcm(aes)'
 - name: 'ipsec-7'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '5.15-20250521.025913'
+  kernel: '5.15-20250527.055456'
   kube-proxy: 'iptables'
   kpr: 'false'
   tunnel: 'vxlan'
@@ -73,9 +77,11 @@
   skip-upgrade: 'true'
 - name: 'ipsec-8'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
+  devices: '{eth0,eth1}'
+  secondary-network: 'true'
   tunnel: 'vxlan'
   egress-gateway: 'true'
   ingress-controller: 'true'
@@ -84,9 +90,11 @@
   key-two: 'gcm(aes)'
 - name: 'ipsec-9'
   # renovate: datasource=docker depName=quay.io/lvh-images/kind
-  kernel: '6.12-20250521.025913'
+  kernel: '6.12-20250527.055456'
   kube-proxy: 'none'
   kpr: 'true'
+  devices: '{eth0,eth1}'
+  secondary-network: 'true'
   tunnel: 'vxlan'
   ipv4: 'false'
   underlay: 'ipv6'
diff --git a/.github/actions/ginkgo/main-k8s-versions.yaml b/.github/actions/ginkgo/main-k8s-versions.yaml
index 027af73c07..b9d5499365 100644
--- a/.github/actions/ginkgo/main-k8s-versions.yaml
+++ b/.github/actions/ginkgo/main-k8s-versions.yaml
@@ -6,7 +6,7 @@ include:
     # renovate: datasource=docker
     kube-image: "quay.io/cilium/kindest-node:v1.33.0@sha256:03032c2e474eef9d706416a077f2eeb14600653eecbdfada9fc2d81a4e0461f9"
     # renovate: datasource=docker depName=quay.io/lvh-images/kind
-    kernel: "6.12-20250521.025913"
+    kernel: "6.12-20250527.055456"
     kernel-type: "latest"
 
   - k8s-version: "1.32"
@@ -14,7 +14,7 @@ include:
     # renovate: datasource=docker
     kube-image: "quay.io/cilium/kindest-node:v1.32.0@sha256:22cf2864f90cfab0d442fda2decf2eae107edd03483053a902614dec637eff76"
     # renovate: datasource=docker depName=quay.io/lvh-images/kind
-    kernel: "rhel8.6-20250521.025913"
+    kernel: "rhel8.6-20250527.055456"
     kernel-type: "oldstable"
 
   - k8s-version: "1.31"
@@ -22,7 +22,7 @@ include:
     # renovate: datasource=docker
     kube-image: "quay.io/cilium/kindest-node:v1.31.0@sha256:d2b2a8cd6fa282b9a4126938341a4d2924dfa96f60b1f983d519498c9cde1a99"
     # renovate: datasource=docker depName=quay.io/lvh-images/kind
-    kernel: "rhel8.6-20250521.025913"
+    kernel: "rhel8.6-20250527.055456"
     kernel-type: "oldstable"
 
   - k8s-version: "1.30"
@@ -30,5 +30,5 @@ include:
     # renovate: datasource=docker
     kube-image: "quay.io/cilium/kindest-node:v1.30.0@sha256:edcb457c0b2ecc69a0fa9b0878bdcfd4a0f1205340cf08bf36a03d3a94a16dd9"
     # renovate: datasource=docker depName=quay.io/lvh-images/kind
-    kernel: "5.10-20250521.025913"
+    kernel: "5.10-20250527.055456"
     kernel-type: "stable"
diff --git a/.github/actions/helm-default/action.yaml b/.github/actions/helm-default/action.yaml
index c580d1005b..6d4c134eae 100644
--- a/.github/actions/helm-default/action.yaml
+++ b/.github/actions/helm-default/action.yaml
@@ -49,7 +49,9 @@ runs:
           --helm-set=hubble.relay.image.tag=${{ inputs.image-tag }} \
           --helm-set=hubble.relay.image.useDigest=false \
           --set-string=extraEnv[0].name=CILIUM_FEATURE_METRICS_WITH_DEFAULTS \
-          --set-string=extraEnv[0].value=true"
+          --set-string=extraEnv[0].value=true \
+          --set-string=extraEnv[1].name=CILIUM_INVALID_METRIC_VALUE_DETECTOR \
+          --set-string=extraEnv[1].value=true"
 
         if [ "${{ inputs.debug }}" == "true" ]; then
           CILIUM_INSTALL_DEFAULTS+=" --helm-set=debug.enabled=true \
@@ -57,8 +59,8 @@ runs:
         fi
 
         if [ "${{ inputs.mutation-detection }}" == "true" ]; then
-          CILIUM_INSTALL_DEFAULTS+="--set-string=extraEnv[1].name=KUBE_CACHE_MUTATION_DETECTOR \
-            --set-string=extraEnv[1].value=true"
+          CILIUM_INSTALL_DEFAULTS+="--set-string=extraEnv[2].name=KUBE_CACHE_MUTATION_DETECTOR \
+            --set-string=extraEnv[2].value=true"
         fi
 
         echo cilium_install_defaults=${CILIUM_INSTALL_DEFAULTS} >> $GITHUB_OUTPUT
diff --git a/.github/actions/multi-pool/configs.yaml b/.github/actions/multi-pool/configs.yaml
index 83af1b4e3e..6aaa37ba0f 100644
--- a/.github/actions/multi-pool/configs.yaml
+++ b/.github/actions/multi-pool/configs.yaml
@@ -4,12 +4,6 @@
 - name: 'Tunnel Mode'
   tunnel: 'vxlan'
   encryption: 'disabled'
-- name: 'Direct Routing with KVStore'
-  tunnel: 'disabled'
-  encryption: 'disabled'
-- name: 'Tunnel Mode with KVStore'
-  tunnel: 'vxlan'
-  encryption: 'disabled'
 - name: 'Tunnel Mode with IPSec Encryption'
   tunnel: 'vxlan'
   encryption: 'ipsec'
diff --git a/.github/ariane-config.yaml b/.github/ariane-config.yaml
index c5be71848a..4579f3765c 100644
--- a/.github/ariane-config.yaml
+++ b/.github/ariane-config.yaml
@@ -92,38 +92,38 @@ triggers:
 
 workflows:
   conformance-aks.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   conformance-aws-cni.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   conformance-clustermesh.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   conformance-delegated-ipam.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   conformance-ipsec-e2e.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   conformance-eks.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   conformance-gateway-api.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   conformance-ginkgo.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|cilium-cli|Documentation|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|cilium-cli|Documentation|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md)$)
   conformance-gke.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   conformance-ingress.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   conformance-multi-pool.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   conformance-runtime.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|cilium-cli|Documentation|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|cilium-cli|Documentation|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md)$)
   integration-test.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|Documentation|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|Documentation|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md)$)
   tests-clustermesh-upgrade.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   tests-datapath-verifier.yaml:
     paths-regex: (bpf|test/verifier|vendor|images|.github/actions/cl2-modules)/
   tests-e2e-upgrade.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   tests-ipsec-upgrade.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md|.*_test\.go)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|hubble|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md|.*_test\.go)$)
   hubble-cli-integration-test.yaml:
-    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|USERS.md)$)
+    paths-ignore-regex: ((bpf/complexity-tests|bpf/tests|test|Documentation|.github/actions/cl2-modules)/|(.github/renovate\.json5|README.rst|CODEOWNERS|.*\.md)$)
diff --git a/.github/workflows/build-go-caches.yaml b/.github/workflows/build-go-caches.yaml
index 9be8b388b0..e27af35e63 100644
--- a/.github/workflows/build-go-caches.yaml
+++ b/.github/workflows/build-go-caches.yaml
@@ -102,6 +102,7 @@ jobs:
         env:
           BUILDER_GOCACHE_DIR: "/tmp/.cache/go/.cache/go-build"
           BUILDER_GOMODCACHE_DIR: "/tmp/.cache/go/pkg"
+          RUN_AS_ROOT: "true"
         if: ${{ steps.go-cache.outputs.cache-hit != 'true' &&
               steps.check.outputs.build != ''
            }}
diff --git a/.github/workflows/conformance-ipsec-e2e.yaml b/.github/workflows/conformance-ipsec-e2e.yaml
index 994992f755..7f84e4c1de 100644
--- a/.github/workflows/conformance-ipsec-e2e.yaml
+++ b/.github/workflows/conformance-ipsec-e2e.yaml
@@ -161,6 +161,11 @@ jobs:
             SHA="${{ github.sha }}"
           fi
 
+          EXTRA_CLI_FLAGS=()
+          if [ "${{ matrix.secondary-network }}" = "true" ]; then
+            EXTRA_CLI_FLAGS+=("\"--secondary-network-iface=eth1\"")
+          fi
+
           CONNECTIVITY_TEST_DEFAULTS="--include-unsafe-tests \
                                       --collect-sysdump-on-failure \
                                       --sysdump-hubble-flows-count=1000000 \
@@ -168,7 +173,8 @@ jobs:
                                       --log-code-owners \
                                       --code-owners=${CILIUM_CLI_CODE_OWNERS_PATHS} \
                                       --exclude-code-owners=${CILIUM_CLI_EXCLUDE_OWNERS} \
-                                      --flush-ct"
+                                      --flush-ct \
+                                      ${EXTRA_CLI_FLAGS[*]}"
 
           echo sha=${SHA} >> $GITHUB_OUTPUT
           echo connectivity_test_defaults=${CONNECTIVITY_TEST_DEFAULTS} >> $GITHUB_OUTPUT
diff --git a/.github/workflows/conformance-k8s-network-policies.yaml b/.github/workflows/conformance-k8s-network-policies.yaml
index 106697b6a4..f47dff71e2 100644
--- a/.github/workflows/conformance-k8s-network-policies.yaml
+++ b/.github/workflows/conformance-k8s-network-policies.yaml
@@ -104,8 +104,10 @@ jobs:
             --set debug.verbose=envoy \
             --set-string=extraEnv[0].name=CILIUM_FEATURE_METRICS_WITH_DEFAULTS \
             --set-string=extraEnv[0].value=true \
-            --set-string=extraEnv[1].name=KUBE_CACHE_MUTATION_DETECTOR \
+            --set-string=extraEnv[1].name=CILIUM_INVALID_METRIC_VALUE_DETECTOR \
             --set-string=extraEnv[1].value=true \
+            --set-string=extraEnv[2].name=KUBE_CACHE_MUTATION_DETECTOR \
+            --set-string=extraEnv[2].value=true \
             --set nodeinit.enabled=true \
             --set kubeProxyReplacement=false \
             --set socketLB.enabled=false \
diff --git a/.github/workflows/lint-bpf-checks.yaml b/.github/workflows/lint-bpf-checks.yaml
index 65153e0c67..2e06d94e1b 100644
--- a/.github/workflows/lint-bpf-checks.yaml
+++ b/.github/workflows/lint-bpf-checks.yaml
@@ -45,12 +45,14 @@ jobs:
               - 'images/**'
               - 'Makefile*'
               - 'contrib/scripts/builder.sh'
+              - '!**/*.md'
 
             coccinelle:
               - 'contrib/coccinelle/**'
             bpf-tests-runner:
               - 'bpf/tests/bpftest/**'
               - 'pkg/bpf/**'
+              - '!**/*.md'
             workflow-description:
               - '.github/workflows/lint-bpf-checks.yaml'
 
diff --git a/.github/workflows/lint-build-commits.yaml b/.github/workflows/lint-build-commits.yaml
index 124535d2d5..bc5b1dc647 100644
--- a/.github/workflows/lint-build-commits.yaml
+++ b/.github/workflows/lint-build-commits.yaml
@@ -69,6 +69,7 @@ jobs:
           filters: |
             src:
               - 'bpf/**'
+              - '!**/*.md'
 
       - name: Check test code changes
         uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3.0.2
@@ -80,6 +81,7 @@ jobs:
             src:
               - 'pkg/**'
               - 'test/**'
+              - '!**/*.md'
 
   build-commits-cilium:
     name: Check if cilium builds for every commit
diff --git a/.github/workflows/net-perf-gke.yaml b/.github/workflows/net-perf-gke.yaml
index defae837bf..5b14b014a2 100644
--- a/.github/workflows/net-perf-gke.yaml
+++ b/.github/workflows/net-perf-gke.yaml
@@ -112,6 +112,11 @@ jobs:
       fail-fast: false
       matrix:
         include:
+          - index: 0
+            name: "baseline"
+            mode: "baseline"
+            encryption: "baseline"
+
           - index: 1
             name: "native"
             mode: "gke"
@@ -221,6 +226,11 @@ jobs:
       - name: Create GKE cluster
         id: create-cluster
         run: |
+          if [ "${{ matrix.mode }}" = "baseline" ] ; then
+            TAINTS=""
+          else
+            TAINTS="--node-taints ignore-taint.cluster-autoscaler.kubernetes.io/cilium-agent-not-ready=true:NoExecute"
+          fi
           gcloud container clusters create ${{ env.clusterName }}-${{ matrix.index }} \
             --labels "usage=${{ github.repository_owner }}-${{ github.event.repository.name }},owner=${{ steps.vars.outputs.owner }}" \
             --zone ${{ env.gcp_zone }} \
@@ -234,7 +244,7 @@ jobs:
             --machine-type n2-standard-2 \
             --disk-type pd-standard \
             --disk-size 20GB \
-            --node-taints ignore-taint.cluster-autoscaler.kubernetes.io/cilium-agent-not-ready=true:NoExecute
+            ${TAINTS}
 
           native_cidr="$(gcloud container clusters describe ${{ env.clusterName }}-${{ matrix.index }} --zone ${{ env.gcp_zone }} --format 'value(clusterIpv4Cidr)')"
           echo native_cidr=${native_cidr} >> $GITHUB_OUTPUT
@@ -266,12 +276,14 @@ jobs:
               --from-literal=keys="3+ rfc4106(gcm(aes)) $(dd if=/dev/urandom count=20 bs=1 2> /dev/null | xxd -p -c 64) 128"
 
       - name: Install Cilium
+        if: ${{ matrix.mode != 'baseline' }}
         id: install-cilium
         run: |
           cilium install --dry-run-helm-values ${{ steps.vars.outputs.cilium_install_defaults }} --helm-set=ipv4NativeRoutingCIDR=${{ steps.create-cluster.outputs.native_cidr }}
           cilium install ${{ steps.vars.outputs.cilium_install_defaults }} --helm-set=ipv4NativeRoutingCIDR=${{ steps.create-cluster.outputs.native_cidr }}
 
       - name: Wait for Cilium to be ready
+        if: ${{ matrix.mode != 'baseline' }}
         run: |
           cilium status --wait --interactive=false --wait-duration=10m
           kubectl get pods -n kube-system
@@ -281,7 +293,7 @@ jobs:
         id: run-perf
         run: |
           mkdir output
-          cilium connectivity perf --duration=30s --host-net=true --pod-net=true --report-dir=./output --unsafe-capture-kernel-profiles
+          cilium connectivity perf --duration=30s --host-net=true --pod-net=true --crr=true --report-dir=./output --unsafe-capture-kernel-profiles
           sudo chmod -R +r ./output
 
       - name: Features tested
@@ -291,7 +303,7 @@ jobs:
           json-filename: "${{ env.job_name }} (${{ join(matrix.*, ', ') }})"
 
       - name: Get sysdump
-        if: ${{ always() && steps.run-perf.outcome != 'skipped' && steps.run-perf.outcome != 'cancelled' }}
+        if: ${{ always() && steps.run-perf.outcome != 'skipped' && steps.run-perf.outcome != 'cancelled' && matrix.mode != 'baseline' }}
         run: |
           cilium status
           cilium sysdump --output-filename cilium-sysdump-final
@@ -320,7 +332,7 @@ jobs:
           test_name: ${{ env.test_name }}-${{ matrix.name }}
           results_bucket: ${{ env.GCP_PERF_RESULTS_BUCKET }}
           artifacts: ./output/*
-          other_files: cilium-sysdump-final.zip
+          other_files: ${{ matrix.mode != 'baseline' && 'cilium-sysdump-final.zip' || '' }}
 
   merge-upload:
     if: ${{ always() }}
diff --git a/.github/workflows/tests-datapath-verifier.yaml b/.github/workflows/tests-datapath-verifier.yaml
index 01e9a255ec..f295827e93 100644
--- a/.github/workflows/tests-datapath-verifier.yaml
+++ b/.github/workflows/tests-datapath-verifier.yaml
@@ -87,25 +87,25 @@ jobs:
       matrix:
         include:
           # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
-          - kernel: 'rhel8.6-20250521.025913'
+          - kernel: 'rhel8.6-20250527.055456'
             ci-kernel: '54'
           # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
-          - kernel: '5.10-20250521.025913'
+          - kernel: '5.10-20250527.055456'
             ci-kernel: '510'
           # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
-          - kernel: '5.15-20250521.025913'
+          - kernel: '5.15-20250527.055456'
             ci-kernel: '510'
           # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
-          - kernel: '6.1-20250521.025913'
+          - kernel: '6.1-20250527.055456'
             ci-kernel: '61'
           # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
-          - kernel: '6.6-20250521.025913'
+          - kernel: '6.6-20250527.055456'
             ci-kernel: '61'
           # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
-          - kernel: '6.12-20250521.025913'
+          - kernel: '6.12-20250527.055456'
             ci-kernel: '61'
           # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
-          - kernel: 'bpf-next-20250521.025913'
+          - kernel: 'bpf-next-20250527.055456'
             ci-kernel: 'netnext'
     timeout-minutes: 60
     steps:
diff --git a/.github/workflows/tests-smoke-ipv6.yaml b/.github/workflows/tests-smoke-ipv6.yaml
index f7d3b92408..3c94fdcd07 100644
--- a/.github/workflows/tests-smoke-ipv6.yaml
+++ b/.github/workflows/tests-smoke-ipv6.yaml
@@ -44,6 +44,7 @@ jobs:
           filters: |
             src:
               - '!(test|Documentation)/**'
+              - '!**/*.md'
 
   conformance-test-ipv6:
     env:
diff --git a/.github/workflows/tests-smoke.yaml b/.github/workflows/tests-smoke.yaml
index 9344bf4fd4..1e092e1780 100644
--- a/.github/workflows/tests-smoke.yaml
+++ b/.github/workflows/tests-smoke.yaml
@@ -46,6 +46,7 @@ jobs:
           filters: |
             src:
               - '!(test|Documentation)/**'
+              - '!**/*.md'
 
   preflight-clusterrole:
     runs-on: ubuntu-24.04
diff --git a/CODEOWNERS b/CODEOWNERS
index f13210fd29..c9c44cc43c 100644
--- a/CODEOWNERS
+++ b/CODEOWNERS
@@ -388,6 +388,7 @@ Makefile* @cilium/build
 /Documentation/configuration/index.rst @cilium/docs-structure
 /Documentation/contributing/ @cilium/contributing @cilium/docs-structure
 /Documentation/contributing/development/reviewers_committers/review_vendor.rst @cilium/vendor
+/Documentation/contributing/testing/scalability.rst @cilium/sig-scalability
 /Documentation/crdlist.rst
 /Documentation/Dockerfile @cilium/docs-structure
 /Documentation/gettingstarted/demo.rst @cilium/docs-structure
diff --git a/Documentation/cmdref/cilium-agent.md b/Documentation/cmdref/cilium-agent.md
index 6bc6ec0047..6ab5993aa1 100644
--- a/Documentation/cmdref/cilium-agent.md
+++ b/Documentation/cmdref/cilium-agent.md
@@ -156,7 +156,6 @@ cilium-agent [flags]
       --enable-ipv6-ndp                                           Enable IPv6 NDP support
       --enable-k8s                                                Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                                  Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                                 Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-l2-announcements                                   Enable L2 announcements
       --enable-l2-neigh-discovery                                 Enables L2 neighbor discovery used by kube-proxy-replacement and IPsec (default true)
       --enable-l2-pod-announcements                               Enable announcing Pod IPs with Gratuitous ARP
@@ -273,7 +272,7 @@ cilium-agent [flags]
       --ipv4-node string                                          IPv4 address of node (default "auto")
       --ipv4-pod-subnets strings                                  List of IPv4 pod subnets to preconfigure for encryption
       --ipv4-range string                                         Per-node IPv4 endpoint prefix, e.g. 10.16.0.0/16 (default "auto")
-      --ipv4-service-loopback-address string                      IPv4 address for service loopback SNAT (default "169.254.42.1")
+      --ipv4-service-loopback-address string                      IPv4 source address to use for SNAT when a Pod talks to itself over a Service. (default "169.254.42.1")
       --ipv4-service-range string                                 Kubernetes IPv4 services CIDR if not inside cluster prefix (default "auto")
       --ipv6-cluster-alloc-cidr string                            IPv6 /64 CIDR used to allocate per node endpoint /96 CIDR (default "f00d::/64")
       --ipv6-mcast-device string                                  Device that joins a Solicited-Node multicast group for IPv6
@@ -342,6 +341,7 @@ cilium-agent [flags]
       --policy-accounting                                         Enable policy accounting (default true)
       --policy-audit-mode                                         Enable policy audit (non-drop) mode
       --policy-cidr-match-mode strings                            The entities that can be selected by CIDR policy. Supported values: 'nodes'
+      --policy-default-local-cluster                              Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-queue-size uint                                    Size of queue for policy-related events (default 100)
       --policy-secrets-namespace string                           PolicySecretsNamesapce is the namespace having secrets used in CNP and CCNP
       --policy-secrets-only-from-secrets-namespace                Configures the agent to only read policy Secrets from the policy-secrets-namespace
diff --git a/Documentation/cmdref/cilium-agent_hive.md b/Documentation/cmdref/cilium-agent_hive.md
index 050901040a..1d5ab59ec2 100644
--- a/Documentation/cmdref/cilium-agent_hive.md
+++ b/Documentation/cmdref/cilium-agent_hive.md
@@ -66,7 +66,6 @@ cilium-agent hive [flags]
       --enable-ipv6-big-tcp                                       Enable IPv6 BIG TCP option which increases device's maximum GRO/GSO limits for IPv6
       --enable-k8s                                                Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                                  Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                                 Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-l2-pod-announcements                               Enable announcing Pod IPs with Gratuitous ARP
       --enable-monitor                                            Enable the monitor unix domain socket server (default true)
       --enable-policy-secrets-sync                                Enables Envoy secret sync for Secrets used in CiliumNetworkPolicy and CiliumClusterwideNetworkPolicy
@@ -167,6 +166,7 @@ cilium-agent hive [flags]
       --nat-map-stats-interval duration                           Interval upon which nat maps are iterated for stats (default 30s)
       --node-port-range strings                                   Set the min/max NodePort port range (default [30000,32767])
       --nodeport-addresses strings                                A whitelist of CIDRs to limit which IPs are used for NodePort. If not set, primary IPv4 and/or IPv6 address of each native device is used.
+      --policy-default-local-cluster                              Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-queue-size uint                                    Size of queue for policy-related events (default 100)
       --policy-secrets-namespace string                           PolicySecretsNamesapce is the namespace having secrets used in CNP and CCNP
       --policy-secrets-only-from-secrets-namespace                Configures the agent to only read policy Secrets from the policy-secrets-namespace
diff --git a/Documentation/cmdref/cilium-agent_hive_dot-graph.md b/Documentation/cmdref/cilium-agent_hive_dot-graph.md
index 2e3d910c68..b5a74ff23c 100644
--- a/Documentation/cmdref/cilium-agent_hive_dot-graph.md
+++ b/Documentation/cmdref/cilium-agent_hive_dot-graph.md
@@ -72,7 +72,6 @@ cilium-agent hive dot-graph [flags]
       --enable-ipv6-big-tcp                                       Enable IPv6 BIG TCP option which increases device's maximum GRO/GSO limits for IPv6
       --enable-k8s                                                Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                                  Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                                 Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-l2-pod-announcements                               Enable announcing Pod IPs with Gratuitous ARP
       --enable-monitor                                            Enable the monitor unix domain socket server (default true)
       --enable-policy-secrets-sync                                Enables Envoy secret sync for Secrets used in CiliumNetworkPolicy and CiliumClusterwideNetworkPolicy
@@ -172,6 +171,7 @@ cilium-agent hive dot-graph [flags]
       --nat-map-stats-interval duration                           Interval upon which nat maps are iterated for stats (default 30s)
       --node-port-range strings                                   Set the min/max NodePort port range (default [30000,32767])
       --nodeport-addresses strings                                A whitelist of CIDRs to limit which IPs are used for NodePort. If not set, primary IPv4 and/or IPv6 address of each native device is used.
+      --policy-default-local-cluster                              Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-queue-size uint                                    Size of queue for policy-related events (default 100)
       --policy-secrets-namespace string                           PolicySecretsNamesapce is the namespace having secrets used in CNP and CCNP
       --policy-secrets-only-from-secrets-namespace                Configures the agent to only read policy Secrets from the policy-secrets-namespace
diff --git a/Documentation/cmdref/cilium-operator-alibabacloud.md b/Documentation/cmdref/cilium-operator-alibabacloud.md
index 187ccbd457..1b356b1eab 100644
--- a/Documentation/cmdref/cilium-operator-alibabacloud.md
+++ b/Documentation/cmdref/cilium-operator-alibabacloud.md
@@ -51,7 +51,6 @@ cilium-operator-alibabacloud [flags]
       --enable-ipv6                                          Enable IPv6 support (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-l7-proxy                                      Enable L7 proxy for L7 policy enforcement (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-metrics                                       Enable Prometheus metrics
@@ -123,6 +122,7 @@ cilium-operator-alibabacloud [flags]
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
       --parallel-alloc-workers int                           Maximum number of parallel IPAM workers (default 50)
       --pod-restart-selector string                          cilium-operator will delete/restart any pods with these labels if the pod is not managed by Cilium. If this option is empty, then all pods may be restarted (default "k8s-app=kube-dns")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --remove-cilium-node-taints                            Remove node taint "node.cilium.io/agent-not-ready" from Kubernetes nodes once Cilium is up and running (default true)
       --set-cilium-is-up-condition                           Set CiliumIsUp Node condition to mark a Kubernetes Node that a Cilium pod is up and running in that node (default true)
diff --git a/Documentation/cmdref/cilium-operator-alibabacloud_hive.md b/Documentation/cmdref/cilium-operator-alibabacloud_hive.md
index c4743c194b..775b953999 100644
--- a/Documentation/cmdref/cilium-operator-alibabacloud_hive.md
+++ b/Documentation/cmdref/cilium-operator-alibabacloud_hive.md
@@ -36,7 +36,6 @@ cilium-operator-alibabacloud hive [flags]
       --enable-ingress-secrets-sync                          Enables fan-in TLS secrets from multiple namespaces to singular namespace (specified by ingress-secrets-namespace flag) (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-node-ipam                                     Enable Node IPAM
       --enable-policy-secrets-sync                           Enables fan-in TLS secrets sync from multiple namespaces to singular namespace (specified by policy-secrets-namespace flag)
@@ -86,6 +85,7 @@ cilium-operator-alibabacloud hive [flags]
       --operator-pprof-address string                        Address that pprof listens on (default "localhost")
       --operator-pprof-port uint16                           Port that pprof listens on (default 6061)
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --skip-crd-creation                                    When true, Kubernetes Custom Resource Definitions will not be created
       --validate-network-policy                              Whether to enable or disable the informational network policy validator (default true)
diff --git a/Documentation/cmdref/cilium-operator-alibabacloud_hive_dot-graph.md b/Documentation/cmdref/cilium-operator-alibabacloud_hive_dot-graph.md
index 60156f3933..8fe3f44c14 100644
--- a/Documentation/cmdref/cilium-operator-alibabacloud_hive_dot-graph.md
+++ b/Documentation/cmdref/cilium-operator-alibabacloud_hive_dot-graph.md
@@ -42,7 +42,6 @@ cilium-operator-alibabacloud hive dot-graph [flags]
       --enable-ingress-secrets-sync                          Enables fan-in TLS secrets from multiple namespaces to singular namespace (specified by ingress-secrets-namespace flag) (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-node-ipam                                     Enable Node IPAM
       --enable-policy-secrets-sync                           Enables fan-in TLS secrets sync from multiple namespaces to singular namespace (specified by policy-secrets-namespace flag)
@@ -91,6 +90,7 @@ cilium-operator-alibabacloud hive dot-graph [flags]
       --operator-pprof-address string                        Address that pprof listens on (default "localhost")
       --operator-pprof-port uint16                           Port that pprof listens on (default 6061)
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --skip-crd-creation                                    When true, Kubernetes Custom Resource Definitions will not be created
       --validate-network-policy                              Whether to enable or disable the informational network policy validator (default true)
diff --git a/Documentation/cmdref/cilium-operator-aws.md b/Documentation/cmdref/cilium-operator-aws.md
index 112f0b31df..72b601bd54 100644
--- a/Documentation/cmdref/cilium-operator-aws.md
+++ b/Documentation/cmdref/cilium-operator-aws.md
@@ -54,7 +54,6 @@ cilium-operator-aws [flags]
       --enable-ipv6                                          Enable IPv6 support (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-l7-proxy                                      Enable L7 proxy for L7 policy enforcement (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-metrics                                       Enable Prometheus metrics
@@ -130,6 +129,7 @@ cilium-operator-aws [flags]
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
       --parallel-alloc-workers int                           Maximum number of parallel IPAM workers (default 50)
       --pod-restart-selector string                          cilium-operator will delete/restart any pods with these labels if the pod is not managed by Cilium. If this option is empty, then all pods may be restarted (default "k8s-app=kube-dns")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --remove-cilium-node-taints                            Remove node taint "node.cilium.io/agent-not-ready" from Kubernetes nodes once Cilium is up and running (default true)
       --set-cilium-is-up-condition                           Set CiliumIsUp Node condition to mark a Kubernetes Node that a Cilium pod is up and running in that node (default true)
diff --git a/Documentation/cmdref/cilium-operator-aws_hive.md b/Documentation/cmdref/cilium-operator-aws_hive.md
index 67f5793e88..ce121998d3 100644
--- a/Documentation/cmdref/cilium-operator-aws_hive.md
+++ b/Documentation/cmdref/cilium-operator-aws_hive.md
@@ -36,7 +36,6 @@ cilium-operator-aws hive [flags]
       --enable-ingress-secrets-sync                          Enables fan-in TLS secrets from multiple namespaces to singular namespace (specified by ingress-secrets-namespace flag) (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-node-ipam                                     Enable Node IPAM
       --enable-policy-secrets-sync                           Enables fan-in TLS secrets sync from multiple namespaces to singular namespace (specified by policy-secrets-namespace flag)
@@ -86,6 +85,7 @@ cilium-operator-aws hive [flags]
       --operator-pprof-address string                        Address that pprof listens on (default "localhost")
       --operator-pprof-port uint16                           Port that pprof listens on (default 6061)
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --skip-crd-creation                                    When true, Kubernetes Custom Resource Definitions will not be created
       --validate-network-policy                              Whether to enable or disable the informational network policy validator (default true)
diff --git a/Documentation/cmdref/cilium-operator-aws_hive_dot-graph.md b/Documentation/cmdref/cilium-operator-aws_hive_dot-graph.md
index c18b085fe5..af5d207157 100644
--- a/Documentation/cmdref/cilium-operator-aws_hive_dot-graph.md
+++ b/Documentation/cmdref/cilium-operator-aws_hive_dot-graph.md
@@ -42,7 +42,6 @@ cilium-operator-aws hive dot-graph [flags]
       --enable-ingress-secrets-sync                          Enables fan-in TLS secrets from multiple namespaces to singular namespace (specified by ingress-secrets-namespace flag) (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-node-ipam                                     Enable Node IPAM
       --enable-policy-secrets-sync                           Enables fan-in TLS secrets sync from multiple namespaces to singular namespace (specified by policy-secrets-namespace flag)
@@ -91,6 +90,7 @@ cilium-operator-aws hive dot-graph [flags]
       --operator-pprof-address string                        Address that pprof listens on (default "localhost")
       --operator-pprof-port uint16                           Port that pprof listens on (default 6061)
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --skip-crd-creation                                    When true, Kubernetes Custom Resource Definitions will not be created
       --validate-network-policy                              Whether to enable or disable the informational network policy validator (default true)
diff --git a/Documentation/cmdref/cilium-operator-azure.md b/Documentation/cmdref/cilium-operator-azure.md
index e64e8e7fa8..800efedd3e 100644
--- a/Documentation/cmdref/cilium-operator-azure.md
+++ b/Documentation/cmdref/cilium-operator-azure.md
@@ -54,7 +54,6 @@ cilium-operator-azure [flags]
       --enable-ipv6                                          Enable IPv6 support (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-l7-proxy                                      Enable L7 proxy for L7 policy enforcement (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-metrics                                       Enable Prometheus metrics
@@ -126,6 +125,7 @@ cilium-operator-azure [flags]
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
       --parallel-alloc-workers int                           Maximum number of parallel IPAM workers (default 50)
       --pod-restart-selector string                          cilium-operator will delete/restart any pods with these labels if the pod is not managed by Cilium. If this option is empty, then all pods may be restarted (default "k8s-app=kube-dns")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --remove-cilium-node-taints                            Remove node taint "node.cilium.io/agent-not-ready" from Kubernetes nodes once Cilium is up and running (default true)
       --set-cilium-is-up-condition                           Set CiliumIsUp Node condition to mark a Kubernetes Node that a Cilium pod is up and running in that node (default true)
diff --git a/Documentation/cmdref/cilium-operator-azure_hive.md b/Documentation/cmdref/cilium-operator-azure_hive.md
index 652c396411..0bcbd49648 100644
--- a/Documentation/cmdref/cilium-operator-azure_hive.md
+++ b/Documentation/cmdref/cilium-operator-azure_hive.md
@@ -36,7 +36,6 @@ cilium-operator-azure hive [flags]
       --enable-ingress-secrets-sync                          Enables fan-in TLS secrets from multiple namespaces to singular namespace (specified by ingress-secrets-namespace flag) (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-node-ipam                                     Enable Node IPAM
       --enable-policy-secrets-sync                           Enables fan-in TLS secrets sync from multiple namespaces to singular namespace (specified by policy-secrets-namespace flag)
@@ -86,6 +85,7 @@ cilium-operator-azure hive [flags]
       --operator-pprof-address string                        Address that pprof listens on (default "localhost")
       --operator-pprof-port uint16                           Port that pprof listens on (default 6061)
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --skip-crd-creation                                    When true, Kubernetes Custom Resource Definitions will not be created
       --validate-network-policy                              Whether to enable or disable the informational network policy validator (default true)
diff --git a/Documentation/cmdref/cilium-operator-azure_hive_dot-graph.md b/Documentation/cmdref/cilium-operator-azure_hive_dot-graph.md
index a9f537fddb..63aa714fce 100644
--- a/Documentation/cmdref/cilium-operator-azure_hive_dot-graph.md
+++ b/Documentation/cmdref/cilium-operator-azure_hive_dot-graph.md
@@ -42,7 +42,6 @@ cilium-operator-azure hive dot-graph [flags]
       --enable-ingress-secrets-sync                          Enables fan-in TLS secrets from multiple namespaces to singular namespace (specified by ingress-secrets-namespace flag) (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-node-ipam                                     Enable Node IPAM
       --enable-policy-secrets-sync                           Enables fan-in TLS secrets sync from multiple namespaces to singular namespace (specified by policy-secrets-namespace flag)
@@ -91,6 +90,7 @@ cilium-operator-azure hive dot-graph [flags]
       --operator-pprof-address string                        Address that pprof listens on (default "localhost")
       --operator-pprof-port uint16                           Port that pprof listens on (default 6061)
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --skip-crd-creation                                    When true, Kubernetes Custom Resource Definitions will not be created
       --validate-network-policy                              Whether to enable or disable the informational network policy validator (default true)
diff --git a/Documentation/cmdref/cilium-operator-generic.md b/Documentation/cmdref/cilium-operator-generic.md
index c24755ef00..15058313e8 100644
--- a/Documentation/cmdref/cilium-operator-generic.md
+++ b/Documentation/cmdref/cilium-operator-generic.md
@@ -50,7 +50,6 @@ cilium-operator-generic [flags]
       --enable-ipv6                                          Enable IPv6 support (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-l7-proxy                                      Enable L7 proxy for L7 policy enforcement (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-metrics                                       Enable Prometheus metrics
@@ -122,6 +121,7 @@ cilium-operator-generic [flags]
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
       --parallel-alloc-workers int                           Maximum number of parallel IPAM workers (default 50)
       --pod-restart-selector string                          cilium-operator will delete/restart any pods with these labels if the pod is not managed by Cilium. If this option is empty, then all pods may be restarted (default "k8s-app=kube-dns")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --remove-cilium-node-taints                            Remove node taint "node.cilium.io/agent-not-ready" from Kubernetes nodes once Cilium is up and running (default true)
       --set-cilium-is-up-condition                           Set CiliumIsUp Node condition to mark a Kubernetes Node that a Cilium pod is up and running in that node (default true)
diff --git a/Documentation/cmdref/cilium-operator-generic_hive.md b/Documentation/cmdref/cilium-operator-generic_hive.md
index 6b6904fb46..0565ef413e 100644
--- a/Documentation/cmdref/cilium-operator-generic_hive.md
+++ b/Documentation/cmdref/cilium-operator-generic_hive.md
@@ -36,7 +36,6 @@ cilium-operator-generic hive [flags]
       --enable-ingress-secrets-sync                          Enables fan-in TLS secrets from multiple namespaces to singular namespace (specified by ingress-secrets-namespace flag) (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-node-ipam                                     Enable Node IPAM
       --enable-policy-secrets-sync                           Enables fan-in TLS secrets sync from multiple namespaces to singular namespace (specified by policy-secrets-namespace flag)
@@ -86,6 +85,7 @@ cilium-operator-generic hive [flags]
       --operator-pprof-address string                        Address that pprof listens on (default "localhost")
       --operator-pprof-port uint16                           Port that pprof listens on (default 6061)
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --skip-crd-creation                                    When true, Kubernetes Custom Resource Definitions will not be created
       --validate-network-policy                              Whether to enable or disable the informational network policy validator (default true)
diff --git a/Documentation/cmdref/cilium-operator-generic_hive_dot-graph.md b/Documentation/cmdref/cilium-operator-generic_hive_dot-graph.md
index f14278dbcc..b62cb68414 100644
--- a/Documentation/cmdref/cilium-operator-generic_hive_dot-graph.md
+++ b/Documentation/cmdref/cilium-operator-generic_hive_dot-graph.md
@@ -42,7 +42,6 @@ cilium-operator-generic hive dot-graph [flags]
       --enable-ingress-secrets-sync                          Enables fan-in TLS secrets from multiple namespaces to singular namespace (specified by ingress-secrets-namespace flag) (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-node-ipam                                     Enable Node IPAM
       --enable-policy-secrets-sync                           Enables fan-in TLS secrets sync from multiple namespaces to singular namespace (specified by policy-secrets-namespace flag)
@@ -91,6 +90,7 @@ cilium-operator-generic hive dot-graph [flags]
       --operator-pprof-address string                        Address that pprof listens on (default "localhost")
       --operator-pprof-port uint16                           Port that pprof listens on (default 6061)
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --skip-crd-creation                                    When true, Kubernetes Custom Resource Definitions will not be created
       --validate-network-policy                              Whether to enable or disable the informational network policy validator (default true)
diff --git a/Documentation/cmdref/cilium-operator.md b/Documentation/cmdref/cilium-operator.md
index 8f6b8ec506..7c36303947 100644
--- a/Documentation/cmdref/cilium-operator.md
+++ b/Documentation/cmdref/cilium-operator.md
@@ -59,7 +59,6 @@ cilium-operator [flags]
       --enable-ipv6                                          Enable IPv6 support (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-l7-proxy                                      Enable L7 proxy for L7 policy enforcement (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-metrics                                       Enable Prometheus metrics
@@ -135,6 +134,7 @@ cilium-operator [flags]
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
       --parallel-alloc-workers int                           Maximum number of parallel IPAM workers (default 50)
       --pod-restart-selector string                          cilium-operator will delete/restart any pods with these labels if the pod is not managed by Cilium. If this option is empty, then all pods may be restarted (default "k8s-app=kube-dns")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --remove-cilium-node-taints                            Remove node taint "node.cilium.io/agent-not-ready" from Kubernetes nodes once Cilium is up and running (default true)
       --set-cilium-is-up-condition                           Set CiliumIsUp Node condition to mark a Kubernetes Node that a Cilium pod is up and running in that node (default true)
diff --git a/Documentation/cmdref/cilium-operator_hive.md b/Documentation/cmdref/cilium-operator_hive.md
index 99b79f2e5d..b6dadeb19e 100644
--- a/Documentation/cmdref/cilium-operator_hive.md
+++ b/Documentation/cmdref/cilium-operator_hive.md
@@ -36,7 +36,6 @@ cilium-operator hive [flags]
       --enable-ingress-secrets-sync                          Enables fan-in TLS secrets from multiple namespaces to singular namespace (specified by ingress-secrets-namespace flag) (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-node-ipam                                     Enable Node IPAM
       --enable-policy-secrets-sync                           Enables fan-in TLS secrets sync from multiple namespaces to singular namespace (specified by policy-secrets-namespace flag)
@@ -86,6 +85,7 @@ cilium-operator hive [flags]
       --operator-pprof-address string                        Address that pprof listens on (default "localhost")
       --operator-pprof-port uint16                           Port that pprof listens on (default 6061)
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --skip-crd-creation                                    When true, Kubernetes Custom Resource Definitions will not be created
       --validate-network-policy                              Whether to enable or disable the informational network policy validator (default true)
diff --git a/Documentation/cmdref/cilium-operator_hive_dot-graph.md b/Documentation/cmdref/cilium-operator_hive_dot-graph.md
index 6055fd630f..c0135caa76 100644
--- a/Documentation/cmdref/cilium-operator_hive_dot-graph.md
+++ b/Documentation/cmdref/cilium-operator_hive_dot-graph.md
@@ -42,7 +42,6 @@ cilium-operator hive dot-graph [flags]
       --enable-ingress-secrets-sync                          Enables fan-in TLS secrets from multiple namespaces to singular namespace (specified by ingress-secrets-namespace flag) (default true)
       --enable-k8s                                           Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                             Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                            Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --enable-lb-ipam                                       Enable LB IPAM (default true)
       --enable-node-ipam                                     Enable Node IPAM
       --enable-policy-secrets-sync                           Enables fan-in TLS secrets sync from multiple namespaces to singular namespace (specified by policy-secrets-namespace flag)
@@ -91,6 +90,7 @@ cilium-operator hive dot-graph [flags]
       --operator-pprof-address string                        Address that pprof listens on (default "localhost")
       --operator-pprof-port uint16                           Port that pprof listens on (default 6061)
       --operator-prometheus-serve-addr string                Address to serve Prometheus metrics (default ":9963")
+      --policy-default-local-cluster                         Control whether policy rules assume by default the local cluster if not explicitly selected
       --policy-secrets-namespace string                      Namespace where secrets used in TLS Interception will be synced to. (default "cilium-secrets")
       --skip-crd-creation                                    When true, Kubernetes Custom Resource Definitions will not be created
       --validate-network-policy                              Whether to enable or disable the informational network policy validator (default true)
diff --git a/Documentation/cmdref/clustermesh-apiserver_clustermesh.md b/Documentation/cmdref/clustermesh-apiserver_clustermesh.md
index 4774c0d768..7b27179426 100644
--- a/Documentation/cmdref/clustermesh-apiserver_clustermesh.md
+++ b/Documentation/cmdref/clustermesh-apiserver_clustermesh.md
@@ -23,7 +23,6 @@ clustermesh-apiserver clustermesh [flags]
       --enable-gops                                  Enable gops server (default true)
       --enable-k8s                                   Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                     Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                    Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --gops-port uint16                             Port for gops server to listen on (default 9892)
       --health-port int                              TCP port for ClusterMesh health API (default 9880)
   -h, --help                                         help for clustermesh
diff --git a/Documentation/cmdref/clustermesh-apiserver_clustermesh_hive.md b/Documentation/cmdref/clustermesh-apiserver_clustermesh_hive.md
index 3432168708..c96065c048 100644
--- a/Documentation/cmdref/clustermesh-apiserver_clustermesh_hive.md
+++ b/Documentation/cmdref/clustermesh-apiserver_clustermesh_hive.md
@@ -23,7 +23,6 @@ clustermesh-apiserver clustermesh hive [flags]
       --enable-gops                                  Enable gops server (default true)
       --enable-k8s                                   Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                     Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                    Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --gops-port uint16                             Port for gops server to listen on (default 9892)
       --health-port int                              TCP port for ClusterMesh health API (default 9880)
   -h, --help                                         help for hive
diff --git a/Documentation/cmdref/clustermesh-apiserver_clustermesh_hive_dot-graph.md b/Documentation/cmdref/clustermesh-apiserver_clustermesh_hive_dot-graph.md
index 528f556ea0..b1c14bd4ad 100644
--- a/Documentation/cmdref/clustermesh-apiserver_clustermesh_hive_dot-graph.md
+++ b/Documentation/cmdref/clustermesh-apiserver_clustermesh_hive_dot-graph.md
@@ -29,7 +29,6 @@ clustermesh-apiserver clustermesh hive dot-graph [flags]
       --enable-gops                                  Enable gops server (default true)
       --enable-k8s                                   Enable the k8s clientset (default true)
       --enable-k8s-api-discovery                     Enable discovery of Kubernetes API groups and resources with the discovery API
-      --enable-k8s-endpoint-slice                    Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it (default true)
       --gops-port uint16                             Port for gops server to listen on (default 9892)
       --health-port int                              TCP port for ClusterMesh health API (default 9880)
       --k8s-api-server-urls strings                  Kubernetes API server URLs
diff --git a/Documentation/contributing/development/dev_setup.rst b/Documentation/contributing/development/dev_setup.rst
index a8342dd919..0041b7568c 100644
--- a/Documentation/contributing/development/dev_setup.rst
+++ b/Documentation/contributing/development/dev_setup.rst
@@ -440,17 +440,6 @@ Minor version
    - ``contrib/scripts/devcontainer-setup.sh``
    - ``.github/actions/ginkgo/main-focus.yaml``
 
-#. Add the new coredns files specific for the Kubernetes version,
-   for ``1.19`` is ``test/provision/manifest/1.19``. The coredns deployment
-   files can be found upstream as mentioned in the previous k8s version
-   coredns files. Perform a diff with the previous versions to check which
-   changes are required for our CI and which changes were added upstream.
-
-#. Update the constraint in the function ``getK8sSupportedConstraints``, that
-   exists in the ``test/helpers/utils.go``, with the new Kubernetes version that
-   Cilium supports. It is possible that a new ``IsCiliumV1*`` var in that file
-   is required as well.
-
 #. Bump the kindest/node version in
    ``.github/actions/ginkgo/main-k8s-versions.yaml``.
 
diff --git a/Documentation/contributing/testing/e2e_legacy.rst b/Documentation/contributing/testing/e2e_legacy.rst
index 1c6c13f577..7b7658711f 100644
--- a/Documentation/contributing/testing/e2e_legacy.rst
+++ b/Documentation/contributing/testing/e2e_legacy.rst
@@ -357,8 +357,6 @@ framework in the ``test/`` directory and interact with ginkgo directly:
             Pass the environment invoking ginkgo, including PATH, to subcommands
       -cilium.provision
             Provision Vagrant boxes and Cilium before running test (default true)
-      -cilium.runQuarantined
-            Run tests that are under quarantine.
       -cilium.showCommands
             Output which commands are ran to stdout
       -cilium.skipLogs
diff --git a/Documentation/contributing/testing/index.rst b/Documentation/contributing/testing/index.rst
index 91f77ee514..49a33033fa 100644
--- a/Documentation/contributing/testing/index.rst
+++ b/Documentation/contributing/testing/index.rst
@@ -26,6 +26,7 @@ information about Cilium's CI infrastructure.
    ci
    e2e
    e2e_legacy
+   scalability
    unit
    bpf
 
diff --git a/Documentation/contributing/testing/scalability.rst b/Documentation/contributing/testing/scalability.rst
new file mode 100644
index 0000000000..bdd2863cf8
--- /dev/null
+++ b/Documentation/contributing/testing/scalability.rst
@@ -0,0 +1,294 @@
+.. only:: not (epub or latex or html)
+
+    WARNING: You are looking at unreleased Cilium documentation.
+    Please use the official rendered version released here:
+    https://docs.cilium.io
+
+.. _scalability_testing:
+
+Scalability and Performance Testing
+===================================
+
+Introduction
+~~~~~~~~~~~~
+
+Cilium scalability and performance tests leverage `ClusterLoader2 <CL2_>`_.
+For an overview of ClusterLoader2, please refer to the `Readme <CL2_README_>`_ and `Getting Started <CL2_GETTING_STARTED_>`_.
+At a high level, ClusterLoader2 allows for specifying states of the cluster, how to transition between them
+and what metrics to measure during the test run.
+Additionally, it allows for failing the test if the metrics are not within the expected thresholds.
+
+Overview of existing tests
+~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Tests based on kOps and GCP VMs:
+
+* 100 nodes scale test - ``/scale-100`` `Workflow <SCALE_100_WORKFLOW_>`_ that executes two test scenarios:
+
+    * `Upstream load test <UPSTREAM_LOAD_TEST_>`_
+
+    * `Network policy scale test <NETPOL_SCALE_TEST_>`_
+
+
+* FQDN performance test - ``/fqdn-perf`` `Workflow <FQDN_PERF_WORKFLOW_>`_
+  is a simple two-node test that deploys pods with FQDN policies
+  and measures the time it takes to resolve FQDNs from a client point of view.
+
+* ClusterMesh scale test - ``/scale-clustermesh`` `Workflow <CLUSTERMESH_WORKFLOW_>`_ leverages
+  a `mock Clustermesh control plane <CLUSTERMESH_MOCK_>`_ that simulates large deployments of ClusterMesh.
+
+Test based on EKS:
+
+* Egress Gateway scale test - ``/scale-egw``. `Workflow <EGW_WORKFLOW_>`_ tests Egress Gateway on a small cluster,
+  but with synthetically created Endpoints and Nodes to simulate a large cluster.
+
+Whenever developing a new test, consider if you want to add a test to an already existing workflow,
+create a new one, or extend some existing test.
+If you are unsure, you can always ask in the ``#sig-scalabilty`` `Slack channel <SLACK_CHANNEL_>`_.
+For example, if you want to run a test on a large cluster,
+you might consider adding it as a separate test scenario to the already existing 100-nodes scale test
+to reduce the cost of CI, because spinning up a new cluster and tearing it down is quite a long process.
+For some use cases, it might be better to simulate only a large cluster but execute the test on a small cluster,
+like in the case of the Egress Gateway scale test or the ClusterMesh scale test.
+
+Running CL2 tests locally
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Each CL2 test should be designed in a way that scales with the number of nodes.
+This allows for running a specific test case scenario in a local environment, to validate the test case.
+For example, let's run the network policy scale test in a local Kind cluster.
+First, set up a Kind cluster with Cilium, as documented in :ref:`dev_env`.
+Build the ClusterLoader2 binary from the `perf-tests repository <CL2_>`_.
+Then you can run:
+
+.. code-block:: bash
+
+    export CL2_PROMETHEUS_PVC_ENABLED=false
+    export CL2_PROMETHEUS_SCRAPE_CILIUM_OPERATOR=true
+    export CL2_PROMETHEUS_SCRAPE_CILIUM_AGENT=true
+    export CL2_PROMETHEUS_SCRAPE_CILIUM_AGENT_INTERVAL=5s
+
+    ./clusterloader \
+    -v=2 \
+    --testconfig=.github/actions/cl2-modules/netpol/config.yaml \
+    --provider=kind \
+    --enable-prometheus-server \
+    --nodes=1 \
+    --report-dir=./report \
+    --prometheus-scrape-kube-proxy=false \
+    --prometheus-apiserver-scrape-port=6443 \
+    --kubeconfig=$HOME/.kube/config
+
+
+Some additional options worth mentioning are:
+
+* ``--tear-down-prometheus-server=false`` - Leaves Prometheus and Grafana running after the test finishes, this helps speed up the test run
+  when running multiple tests in a row, but also for exploring the metrics in Grafana.
+* ``--experimental-prometheus-snapshot-to-report-dir=true`` - Creates a snapshot of the Prometheus data and saves it to the report directory
+
+By setting ``deleteAutomanagedNamespaces: false`` in the test config, you can also leave
+the test namespaces after the test finishes. This is especially useful for checking if your test
+created the expected resources.
+
+At the end of output, the test should end successfully with::
+
+    clusterloader.go:252] --------------------------------------------------------------------------------
+    clusterloader.go:253] Test Finished
+    clusterloader.go:254]   Test: .github/actions/cl2-modules/netpol/config.yaml
+    clusterloader.go:255]   Status: Success
+    clusterloader.go:259] --------------------------------------------------------------------------------
+
+
+All the test results are saved in the report directory, ``./report`` in this case.
+Most importantly, it contains:
+
+* ``generatedConfig_netpol.yaml`` - Rendered test scenario
+* ``'GenericPrometheusQuery NetPol Average CPU Usage_netpol_.*.json'`` - ``GenericPrometheusQuery`` 
+  contains results of the Prometheus queries executed during the test.
+  In this example, it contains the CPU usage of the Cilium agents. 
+  All of the Prometheus Queries will be automatically visualized in :ref:`perfdash <perfdashdocs>`.
+* ``'PodPeriodicCommand.*Profiles-stdout.*'`` - Contains memory and CPU profiles gathered during the test run. 
+  To understand how to interpret them, refer to the :ref:`profiling` subsection.
+
+
+Accessing Grafana and Prometheus during the test run
+""""""""""""""""""""""""""""""""""""""""""""""""""""
+
+During the test execution, ClusterLoader2 deploys Prometheus and Grafana to the cluster.
+You can access Grafana and Prometheus by running:
+
+.. code-block:: bash
+
+    kubectl port-forward -n monitoring svc/grafana 3000
+    kubectl port-forward -n monitoring svc/prometheus-k8s 9090
+
+This can be especially useful for exploring the metrics and adding additional queries to the test.
+
+Metrics-based testing and alerting
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Sometimes, you might want to scrape additional targets during test execution on top of the default ones.
+In this case, you can simply create a Pod or Service monitor `example monitor <EXAMPLE_MONITOR_>`_.
+Then you need to pass it as an additional argument to ClusterLoader2:
+
+.. code-block:: bash
+
+    ./clusterloader \
+    --prometheus-additional-monitors-path=../../.github/actions/cl2-modules/egw/prom-extra-podmons
+    ...
+
+Now you can use the additional metrics in your test, by leveraging regular ``GenericPrometheusQuery`` measurement.
+For example, Egress Gateway ensures that various percentiles of masquerade latency observed by clients are
+`below specific thresholds <EGW_MASQ_METRICS_>`_. This can be achieved by the following measurement in ClusterLoader2:
+
+.. code-block:: yaml
+
+  - Identifier: MasqueradeDelay{{ .metricsSuffix }}
+    Method: GenericPrometheusQuery
+    Params:
+      action: {{ .action }}
+      metricName: Masquerade Delay {{ .metricsSuffix }}
+      metricVersion: v1
+      unit: s
+      enableViolations: true
+      queries:
+      - name: P95
+        query: quantile(0.95, egw_scale_test_masquerade_delay_seconds_total{k8s_instance="{{ .instance }}"})
+        threshold: {{ $MASQ_DELAY_THRESHOLD }}
+
+
+Running tests in CI
+~~~~~~~~~~~~~~~~~~~
+
+Once you are happy with the test and validated it locally, you can create a PR with the test.
+You can base your GitHub workflow on the existing tests, or add a test scenario to an already existing workflow.
+
+
+Accessing test results from PR or CI runs
+"""""""""""""""""""""""""""""""""""""""""
+
+You can run the specific scalability or performance test in your PR, some example commands are::
+
+    /scale-100
+    /scale-clustermesh
+    /scale-egw
+    /fqdn-perf
+
+After the test run, all results will be saved in the Google Storage bucket.
+In the workflow run, you will see a link to the test results at the bottom.
+For example, open `test runs <TEST_RUN_>`_ and pick one of the runs.
+You should see a link like this:
+
+::
+
+    EXPORT_DIR: gs://cilium-scale-results/logs/scale-100-main/1745287079
+
+To see how to install gsutil check `Install gsutil <GSUTIL_INSTALL>`_ section.
+To see the results, you can run:
+
+.. code-block:: bash
+
+    gsutil ls -r gs://cilium-scale-results/logs/scale-100-main/1745287079
+
+You can also copy results to your local machine by running:
+
+.. code-block:: bash
+
+    gsutil -m cp -r gs://cilium-scale-results/logs/scale-100-main/1745287079 .
+
+
+.. _perfdashdocs:
+
+Visualizing results in Perfdash
+"""""""""""""""""""""""""""""""
+
+Perfdash leverages exported results from ClusterLoader2 and visualizes them.
+Currently, we do not host a publicly available instance of Perfdash.
+To visualize the results, please check the `Scaffolding repository <PERFDASH_>`_.
+As an example, you can check CPU usage of the Cilium agent:
+
+.. image:: /images/perfdash.png
+    :align: center
+
+Note that clicking on the graph redirects you to the Google Cloud Storage page containing all of the results
+for the specific test run.
+
+Accessing Prometheus snapshot
+"""""""""""""""""""""""""""""
+
+Each test run creates a snapshot of the Prometheus data and saves it to the report directory.
+This is enabled by setting ``--experimental-prometheus-snapshot-to-report-dir=true``.
+Prometheus snapshots help with debugging, give a good overview of the cluster state
+during the test run and can be used to further improve alerting in CI based on existing metrics.
+
+For example, a snapshot can be found in the directory
+``gs://cilium-scale-results/logs/scale-100-main/1745287079/artifacts/prometheus_snapshot.tar.gz``.
+You need to extract it and run Prometheus locally:
+
+.. code-block:: console
+
+    $ tar xvf ./prometheus_snapshot.tar.gz
+    prometheus/snapshots/20250422T013829Z-3ee723086c84c32a/
+    prometheus/snapshots/20250422T013829Z-3ee723086c84c32a/01JSDJB32JAM1FQ6SN8ESFNDN0/
+    prometheus/snapshots/20250422T013829Z-3ee723086c84c32a/01JSDJB32JAM1FQ6SN8ESFNDN0/meta.json
+    prometheus/snapshots/20250422T013829Z-3ee723086c84c32a/01JSDJB32JAM1FQ6SN8ESFNDN0/tombstones
+    prometheus/snapshots/20250422T013829Z-3ee723086c84c32a/01JSDJB32JAM1FQ6SN8ESFNDN0/index
+    prometheus/snapshots/20250422T013829Z-3ee723086c84c32a/01JSDJB32JAM1FQ6SN8ESFNDN0/chunks/
+    prometheus/snapshots/20250422T013829Z-3ee723086c84c32a/01JSDJB32JAM1FQ6SN8ESFNDN0/chunks/000001
+
+    $ prometheus --storage.tsdb.path=./prometheus/snapshots/20250422T013829Z-3ee723086c84c32a/ --web.listen-address="0.0.0.0:9092"
+
+To visualize the data, you can run Grafana locally and connect it to the Prometheus instance.
+
+.. _profiling:
+
+Accessing CPU and memory profiles
+"""""""""""""""""""""""""""""""""
+
+All of the scalability tests collect CPU and memory profiles.
+They are collected under file names like ``PodPeriodicCommand.*Profiles-stdout.*``.
+Each profile is taken periodically during the test run.
+The simplest way to visualize them is to leverage `pprof-merge <PPROF_MERGE_>`_.
+Example commands to aggregate CPU and memory profiles from the whole test run:
+
+.. code-block:: bash
+
+    gsutil -m cp gs://cilium-scale-results/logs/scale-100-main/1745287079/artifacts/PodPeriodicCommand*Profiles-stdout* ./
+    for file in *.txt; do mv "$file" "${file%.txt}.tar.gz"; tar xvf "${file%.txt}.tar.gz"; done
+    pprof-merge cilium-bugtool*/cmd/pprof-cpu && mv merged.data cpu.pprof
+    pprof-merge cilium-bugtool*/cmd/pprof-heap && mv merged.data heap.pprof
+    rm -r cilium-bugtool* PodPeriodicCommand*
+
+Then you can visualize the aggregated CPU and memory profiles by running:
+
+.. code-block:: bash
+
+    go tool pprof -http=localhost:8080 cpu.pprof
+    go tool pprof -http=localhost:8080 heap.pprof
+
+
+If you want to compare the profiles, you can compare them against the baseline extracted from different test run:
+
+.. code-block:: bash
+
+    go tool pprof -http=localhost:8080 --base=baseline_cpu.pprof cpu.pprof
+    go tool pprof -http=localhost:8080 --base=baseline_heap.pprof heap.pprof
+
+
+.. _CL2: https://github.com/kubernetes/perf-tests/tree/master/clusterloader2
+.. _CL2_GETTING_STARTED: https://github.com/kubernetes/perf-tests/blob/master/clusterloader2/docs/GETTING_STARTED.md
+.. _CL2_README: https://github.com/kubernetes/perf-tests/blob/master/clusterloader2/README.md
+.. _CLUSTERMESH_MOCK: https://github.com/cilium/scaffolding/tree/main/cmapisrv-mock
+.. _CLUSTERMESH_WORKFLOW: https://github.com/cilium/cilium/blob/main/.github/workflows/scale-test-clustermesh.yaml
+.. _EGW_MASQ_METRICS: https://github.com/cilium/cilium/blob/main/.github/actions/cl2-modules/egw/modules/masq-metrics.yaml
+.. _EGW_WORKFLOW: https://github.com/cilium/cilium/blob/main/.github/workflows/scale-test-egw.yaml
+.. _EXAMPLE_MONITOR: https://github.com/cilium/cilium/blob/main/.github/actions/cl2-modules/egw/prom-extra-podmons/podmonitor.yaml
+.. _FQDN_PERF_WORKFLOW: https://github.com/cilium/cilium/blob/main/.github/workflows/fqdn-perf.yaml
+.. _GSUTIL_INSTALL: https://cloud.google.com/storage/docs/gsutil_install
+.. _NETPOL_SCALE_TEST: https://github.com/cilium/cilium/tree/main/.github/actions/cl2-modules/netpol
+.. _PERFDASH: https://github.com/cilium/scaffolding/tree/main/scale-tests
+.. _PPROF_MERGE: https://github.com/rakyll/pprof-merge
+.. _SCALE_100_WORKFLOW: https://github.com/cilium/cilium/blob/main/.github/workflows/scale-test-100-gce.yaml
+.. _SLACK_CHANNEL: https://slack.cilium.io
+.. _TEST_RUN: https://github.com/cilium/cilium/actions/workflows/scale-test-100-gce.yaml
+.. _UPSTREAM_LOAD_TEST: https://github.com/kubernetes/perf-tests/tree/master/clusterloader2/testing/load
diff --git a/Documentation/helm-values.rst b/Documentation/helm-values.rst
index bd6fbc2d31..18488fdcc5 100644
--- a/Documentation/helm-values.rst
+++ b/Documentation/helm-values.rst
@@ -932,6 +932,10 @@
      - The maximum number of clusters to support in a ClusterMesh. This value cannot be changed on running clusters, and all clusters in a ClusterMesh must be configured with the same value. Values > 255 will decrease the maximum allocatable cluster-local identities. Supported values are 255 and 511.
      - int
      - ``255``
+   * - :spelling:ignore:`clustermesh.policyDefaultLocalCluster`
+     - Control whether policy rules assume by default the local cluster if not explicitly selected
+     - bool
+     - ``false``
    * - :spelling:ignore:`clustermesh.useAPIServer`
      - Deploy clustermesh-apiserver for clustermesh
      - bool
@@ -1164,10 +1168,6 @@
      - Enable Non-Default-Deny policies
      - bool
      - ``true``
-   * - :spelling:ignore:`enableRuntimeDeviceDetection`
-     - Enables experimental support for the detection of new and removed datapath devices. When devices change the eBPF datapath is reloaded and services updated. If "devices" is set then only those devices, or devices matching a wildcard will be considered.  This option has been deprecated and is a no-op.
-     - bool
-     - ``true``
    * - :spelling:ignore:`enableXTSocketFallback`
      - Enables the fallback compatibility solution for when the xt_socket kernel module is missing and it is needed for the datapath L7 redirection to work properly. See documentation for details on when this can be disabled: https://docs.cilium.io/en/stable/operations/system_requirements/#linux-kernel.
      - bool
@@ -1584,10 +1584,6 @@
      - Enable use of TLS/SSL for connectivity to etcd.
      - bool
      - ``false``
-   * - :spelling:ignore:`externalIPs.enabled`
-     - Enable ExternalIPs service support.
-     - bool
-     - ``false``
    * - :spelling:ignore:`extraArgs`
      - Additional agent container arguments.
      - list
@@ -1700,10 +1696,6 @@
      - Enables the enforcement of host policies in the eBPF datapath.
      - bool
      - ``false``
-   * - :spelling:ignore:`hostPort.enabled`
-     - Enable hostPort service support.
-     - bool
-     - ``false``
    * - :spelling:ignore:`hubble.annotations`
      - Annotations to be added to all top-level hubble objects (resources under templates/hubble)
      - object
diff --git a/Documentation/images/perfdash.png b/Documentation/images/perfdash.png
new file mode 100644
index 0000000000..548d33fbeb
Binary files /dev/null and b/Documentation/images/perfdash.png differ
diff --git a/Documentation/network/clustermesh/policy.rst b/Documentation/network/clustermesh/policy.rst
index 3b4f3886e8..3695b794ea 100644
--- a/Documentation/network/clustermesh/policy.rst
+++ b/Documentation/network/clustermesh/policy.rst
@@ -35,13 +35,38 @@ between two clusters. The cluster name refers to the name given via the
     metadata:
       name: "allow-cross-cluster"
     spec:
-      description: "Allow x-wing in cluster1 to contact rebel-base in cluster2"
+      description: "Allow x-wing to be deployed in the local cluster to contact rebel-base in cluster2"
       endpointSelector:
         matchLabels:
           name: x-wing
-          io.cilium.k8s.policy.cluster: cluster1
       egress:
       - toEndpoints:
         - matchLabels:
             name: rebel-base
             io.cilium.k8s.policy.cluster: cluster2
+
+
+Note that by default policies automatically select endpoints from all the clusters unless it is explicitly specified.
+To restrict endpoint selection to the local cluster by default you can enable the option ``--policy-default-local-cluster``
+via the ConfigMap option ``policy-default-local-cluster`` or the Helm value ``clustermesh.policyDefaultLocalCluster``.
+
+The following policy illustrates how to explicitly allow pods to communicate to all clusters.
+
+.. code-block:: yaml
+
+    apiVersion: "cilium.io/v2"
+    kind: CiliumNetworkPolicy
+    metadata:
+      name: "allow-cross-cluster-any"
+    spec:
+      description: "Allow x-wing to be deployed in the local cluster to contact rebel-base in any cluster"
+      endpointSelector:
+        matchLabels:
+          name: x-wing
+      egress:
+      - toEndpoints:
+        - matchLabels:
+            name: rebel-base
+          matchExpressions:
+            - key: io.cilium.k8s.policy.cluster
+              operator: Exists
diff --git a/Documentation/network/l2-announcements.rst b/Documentation/network/l2-announcements.rst
index 68eb41e652..b704be1f6f 100644
--- a/Documentation/network/l2-announcements.rst
+++ b/Documentation/network/l2-announcements.rst
@@ -171,7 +171,7 @@ are announced. They are both set to ``false`` by default, so a functional policy
 have one or both set to ``true``.
 
 If ``externalIPs`` is ``true`` all IPs in `.spec.externalIPs <https://kubernetes.io/docs/concepts/services-networking/service/#external-ips>`__
-field are announced. These IPs are are managed by service authors.
+field are announced. These IPs are managed by service authors.
 
 If ``loadBalancerIPs`` is ``true`` all IPs in the service's ``.status.loadbalancer.ingress`` field
 are announced. These can be assigned by :ref:`lb_ipam` which can be configured
@@ -351,8 +351,8 @@ default limit is quickly reached when utilizing L2 announcements and thus users
 should size the client rate limit accordingly.
 
 In a worst case scenario, services are distributed unevenly, so we will assume
-a peek load based on the renew deadline. In complex scenarios with multiple 
-policies over disjunct sets of node, max QPS per node will be lower.
+a peak load based on the renew deadline. In complex scenarios with multiple 
+policies over disjointed sets of node, max QPS per node will be lower.
 
 .. code-block:: text
 
@@ -421,7 +421,7 @@ Next, ensure you have at least one policy configured, L2 announcements will not
     NAME      AGE
     policy1   6m16s
 
-L2 announcements should not create a lease for very service matched by the policy. We can check the leases like so:
+L2 announcements should now create a lease for every service matched by the policy. We can check the leases like so:
 
 .. code-block:: shell-session
 
diff --git a/Documentation/observability/metrics.rst b/Documentation/observability/metrics.rst
index 61803acd09..0cb9af595e 100644
--- a/Documentation/observability/metrics.rst
+++ b/Documentation/observability/metrics.rst
@@ -359,8 +359,6 @@ Node Connectivity
 ============================================= ====================================================================================================================================================================== ========== ==================================================================================================================================================================================================================
 Name                                          Labels                                                                                                                                                                 Default    Description
 ============================================= ====================================================================================================================================================================== ========== ==================================================================================================================================================================================================================
-``node_connectivity_status``                  ``source_cluster``, ``source_node_name``, ``target_cluster``, ``target_node_name``, ``target_node_type``, ``type``                                                     Enabled    Deprecated, will be removed in Cilium 1.18 - use ``node_health_connectivity_status`` instead. The last observed status of both ICMP and HTTP connectivity between the current Cilium agent and other Cilium nodes
-``node_connectivity_latency_seconds``         ``address_type``, ``protocol``, ``source_cluster``, ``source_node_name``, ``target_cluster``, ``target_node_ip``, ``target_node_name``, ``target_node_type``, ``type`` Enabled    Deprecated, will be removed in Cilium 1.18 - use ``node_health_connectivity_latency_seconds`` instead. The last observed latency between the current Cilium agent and other Cilium nodes in seconds
 ``node_health_connectivity_status``           ``source_cluster``, ``source_node_name``, ``type``, ``status``                                                                                                         Enabled    Number of endpoints with last observed status of both ICMP and HTTP connectivity between the current Cilium agent and other Cilium nodes
 ``node_health_connectivity_latency_seconds``  ``source_cluster``, ``source_node_name``, ``type``, ``address_type``, ``protocol``                                                                                     Enabled    Histogram of the last observed latency between the current Cilium agent and other Cilium nodes in seconds
 ============================================= ====================================================================================================================================================================== ========== ==================================================================================================================================================================================================================
diff --git a/Documentation/operations/upgrade.rst b/Documentation/operations/upgrade.rst
index a390302963..04164fe84e 100644
--- a/Documentation/operations/upgrade.rst
+++ b/Documentation/operations/upgrade.rst
@@ -331,6 +331,7 @@ Removed Options
 * The previously deprecated flag ``--enable-k8s-terminating-endpoint`` has been removed.
   The K8s terminating endpoints feature is unconditionally enabled.
 * The previously deprecated ``CONNTRACK_LOCAL`` option has been removed
+* The previously deprecated ``enableRuntimeDeviceDetection`` option has been removed
 
 Deprecated Options
 ~~~~~~~~~~~~~~~~~~
@@ -357,6 +358,8 @@ Deprecated Options
 * The flags ``--enable-node-port``, ``--enable-host-port``, ``--enable-external-ips`` have been deprecated
   and will be removed in Cilium 1.19. The kube-proxy replacement features will be only enabled when
   ``--kube-proxy-replacent`` is set to ``true``.
+* The flag ``--enable-k8s-endpoint-slice`` have been deprecated and will be removed in Cilium 1.19.
+  The K8s Endpoint Slice feature will be unconditionally enabled.
 
 Helm Options
 ~~~~~~~~~~~~
@@ -401,6 +404,11 @@ Added Metrics
 Removed Metrics
 ~~~~~~~~~~~~~~~
 
+The following deprecated metrics were removed:
+
+* ``node_connectivity_status``
+* ``node_connectivity_latency_seconds``
+
 Changed Metrics
 ~~~~~~~~~~~~~~~
 
diff --git a/Documentation/security/grpc.rst b/Documentation/security/grpc.rst
index 74db84d803..bd338cc0c8 100644
--- a/Documentation/security/grpc.rst
+++ b/Documentation/security/grpc.rst
@@ -213,19 +213,22 @@ However, if we then again try to invoke ``SetAccessCode``, it is denied:
     Traceback (most recent call last):
       File "/cloudcity/cc_door_client.py", line 71, in <module>
         run()
-      File "/cloudcity/cc_door_client.py", line 53, in run
-        door_id=int(arg2), access_code=int(arg3)))
-      File "/usr/local/lib/python3.4/dist-packages/grpc/_channel.py", line 492, in __call__
-        return _end_unary_response_blocking(state, call, False, deadline)
-      File "/usr/local/lib/python3.4/dist-packages/grpc/_channel.py", line 440, in _end_unary_response_blocking
-        raise _Rendezvous(state, None, None, deadline)
-    grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.CANCELLED, Received http2 header with status: 403)>
+      File "/cloudcity/cc_door_client.py", line 52, in run
+        response = stub.SetAccessCode(cloudcity_pb2.DoorAccessCodeRequest(
+      File "/usr/local/lib/python3.8/dist-packages/grpc/_channel.py", line 826, in __call__
+        return _end_unary_response_blocking(state, call, False, None)
+      File "/usr/local/lib/python3.8/dist-packages/grpc/_channel.py", line 729, in _end_unary_response_blocking
+        raise _InactiveRpcError(state)
+    grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
+        status = StatusCode.PERMISSION_DENIED
+        details = "Access denied"
+	      debug_error_string = "{"created":"@1748342370.953859451","description":"Error received from peer ipv4:10.96.86.105:50051","file":"src/core/lib/surface/call.cc","file_line":1055,"grpc_message":"Access denied\r\n","grpc_status":7}"
 
 
 This is now blocked, thanks to the Cilium network policy. And notice that unlike
 a traditional firewall which would just drop packets in a way indistinguishable
 from a network failure, because Cilium operates at the API-layer, it can
-explicitly reply with an custom HTTP 403 Unauthorized error, indicating that the
+explicitly reply with a custom gRPC status code 7 PERMISSION_DENIED, indicating that the
 request was intentionally denied for security reasons.
 
 Thank goodness that the empire IT staff hadn't had time to deploy Cilium on
diff --git a/Documentation/spelling_wordlist.txt b/Documentation/spelling_wordlist.txt
index 9cdb74bfa8..02b05d6bed 100644
--- a/Documentation/spelling_wordlist.txt
+++ b/Documentation/spelling_wordlist.txt
@@ -99,6 +99,7 @@ O'Reilly
 PaaS
 Palantir
 Pepelnjak
+Perfdash
 Pfaff
 PodCIDR
 Polytechnique
@@ -374,6 +375,7 @@ goroutine
 goroutines
 grafana
 graphviz
+gsutil
 gve
 hairpinned
 hardcode
@@ -459,6 +461,7 @@ json
 k8sServiceHost
 k8sServiceLookupConfigMapName
 k8sServiceLookupNamespace
+kOps
 kTLS
 kafka
 kallsyms
diff --git a/bpf/bpf_host.c b/bpf/bpf_host.c
index b00c8b2c32..66300cc449 100644
--- a/bpf/bpf_host.c
+++ b/bpf/bpf_host.c
@@ -365,7 +365,8 @@ handle_ipv6_cont(struct __ctx_buff *ctx, __u32 secctx, const bool from_host,
 	if (info && info->flag_has_tunnel_ep) {
 		return encap_and_redirect_with_nodeid(ctx, info, secctx,
 						      info->sec_identity,
-						      &trace);
+						      &trace,
+						      bpf_htons(ETH_P_IPV6));
 	}
 skip_tunnel:
 #endif
@@ -796,7 +797,8 @@ handle_ipv4_cont(struct __ctx_buff *ctx, __u32 secctx, const bool from_host,
 			fake_info.flag_has_tunnel_ep = true;
 			return __encap_and_redirect_with_nodeid(ctx, &fake_info,
 								secctx, WORLD_IPV4_ID,
-								WORLD_IPV4_ID, &trace);
+								WORLD_IPV4_ID, &trace,
+								bpf_htons(ETH_P_IP));
 		}
 	}
 skip_vtep:
@@ -811,7 +813,8 @@ skip_vtep:
 	if (info && info->flag_has_tunnel_ep) {
 		return encap_and_redirect_with_nodeid(ctx, info, secctx,
 						      info->sec_identity,
-						      &trace);
+						      &trace,
+						      bpf_htons(ETH_P_IP));
 	}
 skip_tunnel:
 #endif
@@ -1000,7 +1003,7 @@ do_netdev_encrypt_encap(struct __ctx_buff *ctx, __be16 proto, __u32 src_id)
 
 	ctx->mark = 0;
 
-	return encap_and_redirect_with_nodeid(ctx, ep, src_id, 0, &trace);
+	return encap_and_redirect_with_nodeid(ctx, ep, src_id, 0, &trace, proto);
 }
 #endif /* ENABLE_IPSEC && TUNNEL_MODE */
 
@@ -1069,7 +1072,7 @@ do_netdev(struct __ctx_buff *ctx, __u16 proto, __u32 __maybe_unused identity,
      defined ENABLE_L2_ANNOUNCEMENTS
 	case bpf_htons(ETH_P_ARP):
 		send_trace_notify(ctx, obs_point, UNKNOWN_ID, UNKNOWN_ID, TRACE_EP_ID_UNKNOWN,
-				  ctx->ingress_ifindex, trace.reason, trace.monitor);
+				  ctx->ingress_ifindex, trace.reason, trace.monitor, proto);
 		#ifdef ENABLE_L2_ANNOUNCEMENTS
 			ret = handle_l2_announcement(ctx);
 		#else
@@ -1107,7 +1110,7 @@ do_netdev(struct __ctx_buff *ctx, __u16 proto, __u32 __maybe_unused identity,
 # endif /* ENABLE_WIREGUARD */
 
 		send_trace_notify(ctx, obs_point, ipcache_srcid, UNKNOWN_ID, TRACE_EP_ID_UNKNOWN,
-				  ctx->ingress_ifindex, trace.reason, trace.monitor);
+				  ctx->ingress_ifindex, trace.reason, trace.monitor, proto);
 
 		ret = tail_call_internal(ctx, from_host ? CILIUM_CALL_IPV6_FROM_HOST :
 							  CILIUM_CALL_IPV6_FROM_NETDEV,
@@ -1150,7 +1153,7 @@ do_netdev(struct __ctx_buff *ctx, __u16 proto, __u32 __maybe_unused identity,
 #endif /* ENABLE_WIREGUARD */
 
 		send_trace_notify(ctx, obs_point, ipcache_srcid, UNKNOWN_ID, TRACE_EP_ID_UNKNOWN,
-				  ctx->ingress_ifindex, trace.reason, trace.monitor);
+				  ctx->ingress_ifindex, trace.reason, trace.monitor, proto);
 
 		ret = tail_call_internal(ctx, from_host ? CILIUM_CALL_IPV4_FROM_HOST :
 							  CILIUM_CALL_IPV4_FROM_NETDEV,
@@ -1166,7 +1169,7 @@ do_netdev(struct __ctx_buff *ctx, __u16 proto, __u32 __maybe_unused identity,
 #endif /* ENABLE_IPV4 */
 	default:
 		send_trace_notify(ctx, obs_point, UNKNOWN_ID, UNKNOWN_ID, TRACE_EP_ID_UNKNOWN,
-				  ctx->ingress_ifindex, trace.reason, trace.monitor);
+				  ctx->ingress_ifindex, trace.reason, trace.monitor, proto);
 #ifdef ENABLE_HOST_FIREWALL
 		ret = send_drop_notify_error(ctx, identity, DROP_UNKNOWN_L3,
 					     METRIC_INGRESS);
@@ -1230,8 +1233,8 @@ int cil_from_netdev(struct __ctx_buff *ctx)
 		goto drop_err;
 #else
 		send_trace_notify(ctx, TRACE_TO_STACK, src_id, UNKNOWN_ID,
-				  TRACE_EP_ID_UNKNOWN,
-				  TRACE_IFINDEX_UNKNOWN, TRACE_REASON_UNKNOWN, 0);
+				  TRACE_EP_ID_UNKNOWN, TRACE_IFINDEX_UNKNOWN,
+				  TRACE_REASON_UNKNOWN, 0, proto);
 		/* Pass unknown traffic to the stack */
 		return CTX_ACT_OK;
 #endif /* ENABLE_HOST_FIREWALL */
@@ -1284,8 +1287,8 @@ int cil_from_host(struct __ctx_buff *ctx)
 					METRIC_EGRESS);
 #else
 		send_trace_notify(ctx, TRACE_TO_STACK, src_sec_identity, dst_sec_identity,
-				  TRACE_EP_ID_UNKNOWN,
-				  TRACE_IFINDEX_UNKNOWN, TRACE_REASON_UNKNOWN, 0);
+				  TRACE_EP_ID_UNKNOWN, TRACE_IFINDEX_UNKNOWN,
+				  TRACE_REASON_UNKNOWN, 0, proto);
 		/* Pass unknown traffic to the stack */
 		return CTX_ACT_OK;
 #endif /* ENABLE_HOST_FIREWALL */
@@ -1310,8 +1313,8 @@ int cil_from_host(struct __ctx_buff *ctx)
 		ret = CTX_ACT_OK;
 
 		send_trace_notify(ctx, TRACE_FROM_STACK, identity, UNKNOWN_ID,
-				  TRACE_EP_ID_UNKNOWN,
-				  ctx->ingress_ifindex, TRACE_REASON_ENCRYPTED, 0);
+				  TRACE_EP_ID_UNKNOWN, ctx->ingress_ifindex,
+				  TRACE_REASON_ENCRYPTED, 0, proto);
 
 # ifdef TUNNEL_MODE
 		ret = do_netdev_encrypt_encap(ctx, proto, identity);
@@ -1641,8 +1644,8 @@ exit:
 		goto drop_err;
 
 	send_trace_notify(ctx, TRACE_TO_NETWORK, src_sec_identity, dst_sec_identity,
-			  TRACE_EP_ID_UNKNOWN,
-			  THIS_INTERFACE_IFINDEX, trace.reason, trace.monitor);
+			  TRACE_EP_ID_UNKNOWN, THIS_INTERFACE_IFINDEX,
+			  trace.reason, trace.monitor, proto);
 
 	return ret;
 
@@ -1795,8 +1798,8 @@ out:
 
 	if (!traced)
 		send_trace_notify(ctx, TRACE_TO_STACK, src_id, UNKNOWN_ID,
-				  TRACE_EP_ID_UNKNOWN,
-				  CILIUM_HOST_IFINDEX, trace.reason, trace.monitor);
+				  TRACE_EP_ID_UNKNOWN, CILIUM_HOST_IFINDEX,
+				  trace.reason, trace.monitor, proto);
 
 	return ret;
 }
@@ -1823,8 +1826,8 @@ int tail_ipv6_host_policy_ingress(struct __ctx_buff *ctx)
 
 	if (!traced)
 		send_trace_notify(ctx, TRACE_TO_STACK, src_id, UNKNOWN_ID,
-				  TRACE_EP_ID_UNKNOWN,
-				  CILIUM_HOST_IFINDEX, trace.reason, trace.monitor);
+				  TRACE_EP_ID_UNKNOWN, CILIUM_HOST_IFINDEX,
+				  trace.reason, trace.monitor, bpf_htons(ETH_P_IPV6));
 
 	return ret;
 }
@@ -1851,8 +1854,8 @@ int tail_ipv4_host_policy_ingress(struct __ctx_buff *ctx)
 
 	if (!traced)
 		send_trace_notify(ctx, TRACE_TO_STACK, src_id, UNKNOWN_ID,
-				  TRACE_EP_ID_UNKNOWN,
-				  CILIUM_HOST_IFINDEX, trace.reason, trace.monitor);
+				  TRACE_EP_ID_UNKNOWN, CILIUM_HOST_IFINDEX,
+				  trace.reason, trace.monitor, bpf_htons(ETH_P_IP));
 
 	return ret;
 }
diff --git a/bpf/bpf_lxc.c b/bpf/bpf_lxc.c
index 5b177f96e3..f206d9aa51 100644
--- a/bpf/bpf_lxc.c
+++ b/bpf/bpf_lxc.c
@@ -583,7 +583,7 @@ static __always_inline int handle_ipv6_from_lxc(struct __ctx_buff *ctx, __u32 *d
 			send_trace_notify(ctx, TRACE_TO_PROXY, SECLABEL_IPV6,
 					  UNKNOWN_ID, TRACE_EP_ID_UNKNOWN,
 					  TRACE_IFINDEX_UNKNOWN, trace.reason,
-					  trace.monitor);
+					  trace.monitor, bpf_htons(ETH_P_IPV6));
 			/* Stack will do a socket match and deliver locally. */
 			return ctx_redirect_to_proxy6(ctx, tuple, 0, false);
 		}
@@ -632,7 +632,8 @@ ct_recreate6:
 			send_trace_notify(ctx, TRACE_TO_NETWORK, SECLABEL_IPV6,
 					  *dst_sec_identity, TRACE_EP_ID_UNKNOWN,
 					  TRACE_IFINDEX_UNKNOWN,
-					  trace.reason, trace.monitor);
+					  trace.reason, trace.monitor,
+					  bpf_htons(ETH_P_IPV6));
 			return tail_call_internal(ctx, CILIUM_CALL_IPV6_NODEPORT_REVNAT_EGRESS,
 						  ext_err);
 		}
@@ -669,7 +670,7 @@ ct_recreate6:
 		/* Trace the packet before it is forwarded to proxy */
 		send_trace_notify(ctx, TRACE_TO_PROXY, SECLABEL_IPV6, UNKNOWN_ID,
 				  bpf_ntohs(proxy_port), TRACE_IFINDEX_UNKNOWN,
-				  trace.reason, trace.monitor);
+				  trace.reason, trace.monitor, bpf_htons(ETH_P_IPV6));
 		return ctx_redirect_to_proxy6(ctx, tuple, proxy_port, false);
 	}
 
@@ -729,7 +730,8 @@ ct_recreate6:
 			 * (b) packet was redirected to tunnel device so return.
 			 */
 			ret = encap_and_redirect_lxc(ctx, info, SECLABEL_IPV6,
-						     *dst_sec_identity, &trace);
+						     *dst_sec_identity, &trace,
+						     bpf_htons(ETH_P_IPV6));
 			switch (ret) {
 			case CTX_ACT_OK:
 				goto encrypt_to_stack;
@@ -746,7 +748,7 @@ ct_recreate6:
 		if (fib_ok(ret))
 			send_trace_notify(ctx, TRACE_TO_NETWORK, SECLABEL_IPV6,
 					  *dst_sec_identity, TRACE_EP_ID_UNKNOWN, oif,
-					  trace.reason, trace.monitor);
+					  trace.reason, trace.monitor, bpf_htons(ETH_P_IPV6));
 		return ret;
 	}
 
@@ -758,8 +760,8 @@ to_host:
 #ifdef ENABLE_ROUTING
 	if (is_defined(ENABLE_HOST_FIREWALL) && *dst_sec_identity == HOST_ID) {
 		send_trace_notify(ctx, TRACE_TO_HOST, SECLABEL_IPV6, HOST_ID,
-				  TRACE_EP_ID_UNKNOWN,
-				  CILIUM_NET_IFINDEX, trace.reason, trace.monitor);
+				  TRACE_EP_ID_UNKNOWN, CILIUM_NET_IFINDEX,
+				  trace.reason, trace.monitor, bpf_htons(ETH_P_IPV6));
 		return ctx_redirect(ctx, CILIUM_NET_IFINDEX, BPF_F_INGRESS);
 	}
 #endif
@@ -784,8 +786,8 @@ pass_to_stack:
 encrypt_to_stack:
 #endif
 	send_trace_notify(ctx, TRACE_TO_STACK, SECLABEL_IPV6, *dst_sec_identity,
-			  TRACE_EP_ID_UNKNOWN,
-			  TRACE_IFINDEX_UNKNOWN, trace.reason, trace.monitor);
+			  TRACE_EP_ID_UNKNOWN, TRACE_IFINDEX_UNKNOWN,
+			  trace.reason, trace.monitor, bpf_htons(ETH_P_IPV6));
 
 	cilium_dbg_capture(ctx, DBG_CAPTURE_DELIVERY, 0);
 
@@ -1018,7 +1020,7 @@ static __always_inline int handle_ipv4_from_lxc(struct __ctx_buff *ctx, __u32 *d
 			send_trace_notify(ctx, TRACE_TO_PROXY, SECLABEL_IPV4,
 					  UNKNOWN_ID, TRACE_EP_ID_UNKNOWN,
 					  TRACE_IFINDEX_UNKNOWN, trace.reason,
-					  trace.monitor);
+					  trace.monitor, bpf_htons(ETH_P_IP));
 			/* Stack will do a socket match and deliver locally. */
 			return ctx_redirect_to_proxy4(ctx, tuple, 0, false);
 		}
@@ -1093,7 +1095,8 @@ ct_recreate4:
 			send_trace_notify(ctx, TRACE_TO_NETWORK, SECLABEL_IPV4,
 					  *dst_sec_identity, TRACE_EP_ID_UNKNOWN,
 					  TRACE_IFINDEX_UNKNOWN,
-					  trace.reason, trace.monitor);
+					  trace.reason, trace.monitor,
+					  bpf_htons(ETH_P_IP));
 			return tail_call_internal(ctx, CILIUM_CALL_IPV4_NODEPORT_REVNAT,
 						  ext_err);
 		}
@@ -1133,7 +1136,7 @@ ct_recreate4:
 		/* Trace the packet before it is forwarded to proxy */
 		send_trace_notify(ctx, TRACE_TO_PROXY, SECLABEL_IPV4, UNKNOWN_ID,
 				  bpf_ntohs(proxy_port), TRACE_IFINDEX_UNKNOWN,
-				  trace.reason, trace.monitor);
+				  trace.reason, trace.monitor, bpf_htons(ETH_P_IP));
 		return ctx_redirect_to_proxy4(ctx, tuple, proxy_port, false);
 	}
 
@@ -1169,8 +1172,8 @@ ct_recreate4:
 		__be32 daddr = ip4->daddr;
 		struct endpoint_info *ep;
 
-		/* Loopback replies are addressed to IPV4_LOOPBACK, so
-		 * an endpoint lookup with ip4->daddr won't work.
+		/* Loopback replies are addressed to config service_loopback_ipv4,
+		 * so an endpoint lookup with ip4->daddr won't work.
 		 *
 		 * But as it is loopback traffic, the clientIP and backendIP
 		 * are identical and we can just use the packet's saddr
@@ -1228,7 +1231,8 @@ ct_recreate4:
 			fake_info.flag_has_tunnel_ep = true;
 			return __encap_and_redirect_with_nodeid(ctx, &fake_info,
 								SECLABEL_IPV4, WORLD_IPV4_ID,
-								WORLD_IPV4_ID, &trace);
+								WORLD_IPV4_ID, &trace,
+								bpf_htons(ETH_P_IP));
 		}
 	}
 skip_vtep:
@@ -1273,7 +1277,8 @@ skip_vtep:
 
 		if (info && info->flag_has_tunnel_ep) {
 			ret = encap_and_redirect_lxc(ctx, info, SECLABEL_IPV4,
-						     *dst_sec_identity, &trace);
+						     *dst_sec_identity, &trace,
+						     bpf_htons(ETH_P_IP));
 			switch (ret) {
 			case CTX_ACT_OK:
 				/* IPsec, pass up to stack for XFRM processing. */
@@ -1297,7 +1302,7 @@ skip_vtep:
 		if (fib_ok(ret))
 			send_trace_notify(ctx, TRACE_TO_NETWORK, SECLABEL_IPV4,
 					  *dst_sec_identity, TRACE_EP_ID_UNKNOWN, oif,
-					  trace.reason, trace.monitor);
+					  trace.reason, trace.monitor, bpf_htons(ETH_P_IP));
 		return ret;
 	}
 
@@ -1309,8 +1314,8 @@ to_host:
 #ifdef ENABLE_ROUTING
 	if (is_defined(ENABLE_HOST_FIREWALL) && *dst_sec_identity == HOST_ID) {
 		send_trace_notify(ctx, TRACE_TO_HOST, SECLABEL_IPV4, HOST_ID,
-				  TRACE_EP_ID_UNKNOWN,
-				  CILIUM_NET_IFINDEX, trace.reason, trace.monitor);
+				  TRACE_EP_ID_UNKNOWN, CILIUM_NET_IFINDEX,
+				  trace.reason, trace.monitor, bpf_htons(ETH_P_IP));
 		return ctx_redirect(ctx, CILIUM_NET_IFINDEX, BPF_F_INGRESS);
 	}
 #endif
@@ -1335,8 +1340,8 @@ pass_to_stack:
 encrypt_to_stack:
 #endif
 	send_trace_notify(ctx, TRACE_TO_STACK, SECLABEL_IPV4, *dst_sec_identity,
-			  TRACE_EP_ID_UNKNOWN,
-			  TRACE_IFINDEX_UNKNOWN, trace.reason, trace.monitor);
+			  TRACE_EP_ID_UNKNOWN, TRACE_IFINDEX_UNKNOWN,
+			  trace.reason, trace.monitor, bpf_htons(ETH_P_IP));
 	cilium_dbg_capture(ctx, DBG_CAPTURE_DELIVERY, 0);
 	return CTX_ACT_OK;
 }
@@ -1463,7 +1468,7 @@ int tail_handle_arp(struct __ctx_buff *ctx)
 	 * of the LXC IP (to avoid specific problems, like IP duplicate address
 	 * detection checks that might run within the container).
 	 */
-	if (tip == LXC_IPV4)
+	if (tip == CONFIG(endpoint_ipv4).be32)
 		return CTX_ACT_OK;
 
 	return arp_respond(ctx, &mac, tip, &smac, sip, 0);
@@ -1481,6 +1486,7 @@ int cil_from_container(struct __ctx_buff *ctx)
 	__u32 sec_label = SECLABEL;
 	__s8 ext_err = 0;
 	int ret;
+	bool valid_ethertype = validate_ethertype(ctx, &proto);
 
 	bpf_clear_meta(ctx);
 
@@ -1496,9 +1502,9 @@ int cil_from_container(struct __ctx_buff *ctx)
 
 	send_trace_notify(ctx, TRACE_FROM_LXC, sec_label, UNKNOWN_ID,
 			  TRACE_EP_ID_UNKNOWN, TRACE_IFINDEX_UNKNOWN,
-			  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN);
+			  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN, proto);
 
-	if (!validate_ethertype(ctx, &proto)) {
+	if (!valid_ethertype) {
 		ret = DROP_UNSUPPORTED_L2;
 		goto out;
 	}
@@ -1953,10 +1959,11 @@ ipv4_policy(struct __ctx_buff *ctx, struct iphdr *ip4, __u32 src_label,
 		 * want to execute the conntrack logic so that replies can be correctly
 		 * matched.
 		 *
-		 * If ip4.saddr is IPV4_LOOPBACK, this is almost certainly a loopback
-		 * connection. Populate .loopback, so that policy enforcement is bypassed.
+		 * If ip4.saddr is config service_loopback_ipv4, this is almost certainly
+		 * a loopback connection. Populate .loopback, so that policy enforcement
+		 * is bypassed.
 		 */
-		if (ret == CT_NEW && ip4->saddr == IPV4_LOOPBACK &&
+		if (ret == CT_NEW && ip4->saddr == CONFIG(service_loopback_ipv4).be32 &&
 		    ct_has_loopback_egress_entry4(get_ct_map4(tuple), tuple)) {
 			ct_state_new.loopback = true;
 			break;
@@ -2296,7 +2303,7 @@ int cil_lxc_policy_egress(struct __ctx_buff *ctx __maybe_unused)
 	edt_set_aggregate(ctx, 0); /* do not count this traffic again */
 	send_trace_notify(ctx, TRACE_FROM_PROXY, SECLABEL, UNKNOWN_ID,
 			  TRACE_EP_ID_UNKNOWN, TRACE_IFINDEX_UNKNOWN,
-			  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN);
+			  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN, proto);
 
 	switch (proto) {
 #ifdef ENABLE_IPV6
@@ -2364,7 +2371,8 @@ int cil_to_container(struct __ctx_buff *ctx)
 		trace = TRACE_FROM_PROXY;
 
 	send_trace_notify(ctx, trace, identity, sec_label, LXC_ID,
-			  ctx->ingress_ifindex, TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN);
+			  ctx->ingress_ifindex, TRACE_REASON_UNKNOWN,
+			  TRACE_PAYLOAD_LEN, proto);
 
 #if defined(ENABLE_HOST_FIREWALL) && !defined(ENABLE_ROUTING)
 	/* If the packet comes from the hostns and per-endpoint routes are enabled,
diff --git a/bpf/bpf_network.c b/bpf/bpf_network.c
index 201c2d4ea8..1b222f7dde 100644
--- a/bpf/bpf_network.c
+++ b/bpf/bpf_network.c
@@ -84,11 +84,11 @@ int cil_from_network(struct __ctx_buff *ctx)
 out:
 	send_trace_notify(ctx, obs_point_from, UNKNOWN_ID, UNKNOWN_ID,
 			  TRACE_EP_ID_UNKNOWN, ingress_ifindex,
-			  trace.reason, trace.monitor);
+			  trace.reason, trace.monitor, proto);
 
 	send_trace_notify(ctx, obs_point_to, UNKNOWN_ID, UNKNOWN_ID,
 			  TRACE_EP_ID_UNKNOWN, ingress_ifindex,
-			  trace.reason, trace.monitor);
+			  trace.reason, trace.monitor, proto);
 
 	return ret;
 }
diff --git a/bpf/bpf_overlay.c b/bpf/bpf_overlay.c
index ea46fce9bb..9667b99ccd 100644
--- a/bpf/bpf_overlay.c
+++ b/bpf/bpf_overlay.c
@@ -154,8 +154,8 @@ static __always_inline int handle_ipv6(struct __ctx_buff *ctx,
 		ctx_change_type(ctx, PACKET_HOST);
 
 		send_trace_notify(ctx, TRACE_TO_STACK, *identity, UNKNOWN_ID,
-				  TRACE_EP_ID_UNKNOWN,
-				  ctx->ingress_ifindex, TRACE_REASON_ENCRYPTED, 0);
+				  TRACE_EP_ID_UNKNOWN, ctx->ingress_ifindex,
+				  TRACE_REASON_ENCRYPTED, 0, bpf_htons(ETH_P_IPV6));
 
 		return CTX_ACT_OK;
 	}
@@ -182,18 +182,26 @@ not_esp:
 			set_identity_mark(ctx, *identity, MARK_MAGIC_EGW_DONE);
 
 			/* to-netdev@bpf_host handles SNAT, so no need to do it here. */
-			ret = egress_gw_fib_lookup_and_redirect_v6(ctx, &snat_addr,
-								   &daddr, egress_ifindex,
-								   ext_err);
-			if (ret != CTX_ACT_OK)
-				return ret;
-
-			if (!revalidate_data(ctx, &data, &data_end, &ip6))
-				return DROP_INVALID;
+			return egress_gw_fib_lookup_and_redirect_v6(ctx, &snat_addr,
+								    &daddr, egress_ifindex,
+								    ext_err);
 		}
 	}
 #endif /* ENABLE_EGRESS_GATEWAY_COMMON */
 
+#if defined(ENABLE_DSR) && (DSR_ENCAP_MODE == DSR_ENCAP_GENEVE)
+	/* Pass incoming packets which will be returned using Geneve DSR
+	 * to host-stack for conntrack entry insertion.
+	 * Geneve DSR reply packets are processed by the host-stack,
+	 * so this logic is needed to prevent the packets from being handled
+	 * by netfilter in an unintended way.
+	 */
+	if (!is_defined(ENABLE_HOST_ROUTING) && is_dsr) {
+		ctx_change_type(ctx, PACKET_HOST);
+		return CTX_ACT_OK;
+	}
+#endif
+
 	/* Deliver to local (non-host) endpoint: */
 	ep = lookup_ip6_endpoint(ip6);
 	if (ep && !(ep->flags & ENDPOINT_MASK_HOST_DELIVERY))
@@ -465,8 +473,8 @@ skip_vtep:
 		ctx_change_type(ctx, PACKET_HOST);
 
 		send_trace_notify(ctx, TRACE_TO_STACK, *identity, UNKNOWN_ID,
-				  TRACE_EP_ID_UNKNOWN,
-				  ctx->ingress_ifindex, TRACE_REASON_ENCRYPTED, 0);
+				  TRACE_EP_ID_UNKNOWN, ctx->ingress_ifindex,
+				  TRACE_REASON_ENCRYPTED, 0, bpf_htons(ETH_P_IP));
 
 		return CTX_ACT_OK;
 	}
@@ -492,18 +500,26 @@ not_esp:
 			set_identity_mark(ctx, *identity, MARK_MAGIC_EGW_DONE);
 
 			/* to-netdev@bpf_host handles SNAT, so no need to do it here. */
-			ret = egress_gw_fib_lookup_and_redirect(ctx, snat_addr,
-								daddr, egress_ifindex,
-								ext_err);
-			if (ret != CTX_ACT_OK)
-				return ret;
-
-			if (!revalidate_data(ctx, &data, &data_end, &ip4))
-				return DROP_INVALID;
+			return egress_gw_fib_lookup_and_redirect(ctx, snat_addr,
+								 daddr, egress_ifindex,
+								 ext_err);
 		}
 	}
 #endif /* ENABLE_EGRESS_GATEWAY_COMMON */
 
+#if defined(ENABLE_DSR) && (DSR_ENCAP_MODE == DSR_ENCAP_GENEVE)
+	/* Pass incoming packets which will be returned using Geneve DSR
+	 * to host-stack for conntrack entry insertion.
+	 * Geneve DSR reply packets are processed by the host-stack,
+	 * so this logic is needed to prevent the packets from being handled
+	 * by netfilter in an unintended way.
+	 */
+	if (!is_defined(ENABLE_HOST_ROUTING) && is_dsr) {
+		ctx_change_type(ctx, PACKET_HOST);
+		return CTX_ACT_OK;
+	}
+#endif
+
 	/* Deliver to local (non-host) endpoint: */
 	ep = lookup_ip4_endpoint(ip4);
 	if (ep && !(ep->flags & ENDPOINT_MASK_HOST_DELIVERY))
@@ -579,7 +595,8 @@ int tail_handle_arp(struct __ctx_buff *ctx)
 		fake_info.flag_has_tunnel_ep = true;
 		ret = __encap_and_redirect_with_nodeid(ctx, &fake_info,
 						       LOCAL_NODE_ID, WORLD_IPV4_ID,
-						       WORLD_IPV4_ID, &trace);
+						       WORLD_IPV4_ID, &trace,
+						       bpf_htons(ETH_P_ARP));
 		if (IS_ERR(ret))
 			goto drop_err;
 
@@ -593,7 +610,7 @@ drop_err:
 pass_to_stack:
 	send_trace_notify(ctx, TRACE_TO_STACK, UNKNOWN_ID, UNKNOWN_ID,
 			  TRACE_EP_ID_UNKNOWN, ctx->ingress_ifindex,
-			  trace.reason, trace.monitor);
+			  trace.reason, trace.monitor, bpf_htons(ETH_P_ARP));
 	return CTX_ACT_OK;
 }
 #endif /* ENABLE_VTEP */
@@ -712,8 +729,8 @@ int cil_from_overlay(struct __ctx_buff *ctx)
 #ifdef ENABLE_IPSEC
 	if (is_esp(ctx, proto))
 		send_trace_notify(ctx, TRACE_FROM_OVERLAY, src_sec_identity, UNKNOWN_ID,
-				  TRACE_EP_ID_UNKNOWN,
-				  ctx->ingress_ifindex, TRACE_REASON_ENCRYPTED, 0);
+				  TRACE_EP_ID_UNKNOWN, ctx->ingress_ifindex,
+				  TRACE_REASON_ENCRYPTED, 0, proto);
 	else
 #endif
 	{
@@ -727,7 +744,7 @@ int cil_from_overlay(struct __ctx_buff *ctx)
 
 		send_trace_notify(ctx, obs_point, src_sec_identity, UNKNOWN_ID,
 				  TRACE_EP_ID_UNKNOWN, ctx->ingress_ifindex,
-				  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN);
+				  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN, proto);
 	}
 
 	switch (proto) {
diff --git a/bpf/bpf_wireguard.c b/bpf/bpf_wireguard.c
index b3765b01dd..edf7711613 100644
--- a/bpf/bpf_wireguard.c
+++ b/bpf/bpf_wireguard.c
@@ -281,7 +281,7 @@ int cil_from_wireguard(struct __ctx_buff *ctx)
 
 		send_trace_notify(ctx, TRACE_FROM_CRYPTO, identity, UNKNOWN_ID,
 				  TRACE_EP_ID_UNKNOWN, ctx->ingress_ifindex,
-				  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN);
+				  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN, proto);
 
 		ret = tail_call_internal(ctx, CILIUM_CALL_IPV6_FROM_WIREGUARD, &ext_err);
 		/* See the equivalent v4 path for comments */
@@ -299,7 +299,7 @@ int cil_from_wireguard(struct __ctx_buff *ctx)
 
 		send_trace_notify(ctx, TRACE_FROM_CRYPTO, identity, UNKNOWN_ID,
 				  TRACE_EP_ID_UNKNOWN, ctx->ingress_ifindex,
-				  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN);
+				  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN, proto);
 
 		ret = tail_call_internal(ctx, CILIUM_CALL_IPV4_FROM_WIREGUARD, &ext_err);
 		/* We are not returning an error here to always allow traffic to
@@ -315,7 +315,7 @@ int cil_from_wireguard(struct __ctx_buff *ctx)
 
 	send_trace_notify(ctx, TRACE_FROM_CRYPTO, UNKNOWN_ID, UNKNOWN_ID,
 			  TRACE_EP_ID_UNKNOWN, ctx->ingress_ifindex,
-			  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN);
+			  TRACE_REASON_UNKNOWN, TRACE_PAYLOAD_LEN, proto);
 
 	/* Pass unknown traffic to the stack */
 	return TC_ACT_OK;
@@ -355,7 +355,7 @@ out:
 
 	send_trace_notify(ctx, TRACE_TO_CRYPTO, src_sec_identity, UNKNOWN_ID,
 			  TRACE_EP_ID_UNKNOWN, THIS_INTERFACE_IFINDEX,
-			  trace.reason, trace.monitor);
+			  trace.reason, trace.monitor, proto);
 
 	return TC_ACT_OK;
 }
diff --git a/bpf/include/bpf/config/lxc.h b/bpf/include/bpf/config/lxc.h
index 009dd6a8f4..8d5f47bc6e 100644
--- a/bpf/include/bpf/config/lxc.h
+++ b/bpf/include/bpf/config/lxc.h
@@ -12,9 +12,7 @@
 DECLARE_CONFIG(__u16, endpoint_id, "The endpoint's security ID")
 #define LXC_ID CONFIG(endpoint_id) /* Backwards compatibility */
 
-DECLARE_CONFIG(__u32, endpoint_ipv4, "The endpoint's IPv4 address")
-#define LXC_IPV4 CONFIG(endpoint_ipv4) /* Backwards compatibility */
-
+DECLARE_CONFIG(union v4addr, endpoint_ipv4, "The endpoint's IPv4 address")
 DECLARE_CONFIG(union v6addr, endpoint_ipv6, "The endpoint's IPv6 address")
 
 DECLARE_CONFIG(__u64, endpoint_netns_cookie, "The endpoint's network namespace cookie")
diff --git a/bpf/include/bpf/config/node.h b/bpf/include/bpf/config/node.h
index 9bb09a23f2..ae8ce60504 100644
--- a/bpf/include/bpf/config/node.h
+++ b/bpf/include/bpf/config/node.h
@@ -15,4 +15,5 @@
 /* Legacy node config rendered at agent runtime. */
 #include <node_config.h>
 
+NODE_CONFIG(union v4addr, service_loopback_ipv4, "IPv4 source address used for SNAT when a Pod talks to itself over a Service")
 NODE_CONFIG(union v6addr, router_ipv6, "Internal IPv6 router address assigned to the cilium_host interface")
diff --git a/bpf/lib/common.h b/bpf/lib/common.h
index 11c11ce9e6..390b734804 100644
--- a/bpf/lib/common.h
+++ b/bpf/lib/common.h
@@ -80,6 +80,11 @@ enum {
 
 typedef __u64 mac_t;
 
+union v4addr {
+	__be32 be32;
+	__u8 addr[4];
+};
+
 union v6addr {
 	struct {
 		__u32 p1;
@@ -401,6 +406,8 @@ struct egress_gw_policy_entry6 {
 	union v6addr egress_ip;
 	__u32 gateway_ip;
 	__u32 reserved[3]; /* reserved for future extension, e.g. v6 gateway_ip */
+	__u32 egress_ifindex;
+	__u32 reserved2; /* for even more future extension */
 };
 
 struct srv6_vrf_key4 {
diff --git a/bpf/lib/egress_gateway.h b/bpf/lib/egress_gateway.h
index a43be36d4e..95eec5d54e 100644
--- a/bpf/lib/egress_gateway.h
+++ b/bpf/lib/egress_gateway.h
@@ -42,22 +42,14 @@ int egress_gw_fib_lookup_and_redirect(struct __ctx_buff *ctx, __be32 egress_ip,
 	__u32 oif;
 	int ret;
 
-	/* Immediate redirect to egress_ifindex requires L2 resolution.
-	 * Fall back to FIB lookup on older kernels.
-	 */
-	if (egress_ifindex && neigh_resolver_available())
+	if (egress_ifindex)
 		return redirect_neigh(egress_ifindex, NULL, 0, 0);
 
 	ret = (__s8)fib_lookup_v4(ctx, &fib_params, egress_ip, daddr, 0);
 
 	switch (ret) {
 	case BPF_FIB_LKUP_RET_SUCCESS:
-		break;
 	case BPF_FIB_LKUP_RET_NO_NEIGH:
-		/* Don't redirect if we can't update the L2 DMAC: */
-		if (!neigh_resolver_available())
-			return CTX_ACT_OK;
-
 		break;
 	default:
 		*ext_err = (__s8)ret;
@@ -264,7 +256,7 @@ int egress_gw_handle_packet(struct __ctx_buff *ctx,
 	fake_info.flag_has_tunnel_ep = true;
 	return __encap_and_redirect_with_nodeid(ctx, &fake_info,
 						src_sec_identity, dst_sec_identity,
-						NOT_VTEP_DST, trace);
+						NOT_VTEP_DST, trace, bpf_htons(ETH_P_IP));
 }
 
 #ifdef ENABLE_IPV6
@@ -404,7 +396,7 @@ int egress_gw_fib_lookup_and_redirect_v6(struct __ctx_buff *ctx,
 	__u32 oif;
 	int ret;
 
-	if (egress_ifindex && neigh_resolver_available())
+	if (egress_ifindex)
 		return redirect_neigh(egress_ifindex, NULL, 0, 0);
 
 	ret = (__s8)fib_lookup_v6(ctx, &fib_params,
@@ -413,10 +405,7 @@ int egress_gw_fib_lookup_and_redirect_v6(struct __ctx_buff *ctx,
 
 	switch (ret) {
 	case BPF_FIB_LKUP_RET_SUCCESS:
-		break;
 	case BPF_FIB_LKUP_RET_NO_NEIGH:
-		if (!neigh_resolver_available())
-			return CTX_ACT_OK;
 		break;
 	default:
 		*ext_err = (__s8)ret;
@@ -487,7 +476,8 @@ int egress_gw_handle_packet_v6(struct __ctx_buff *ctx,
 	fake_info.tunnel_endpoint.ip4 = gateway_ip;
 	fake_info.flag_has_tunnel_ep = true;
 	return __encap_and_redirect_with_nodeid(ctx, &fake_info, src_sec_identity,
-						dst_sec_identity, NOT_VTEP_DST, trace);
+						dst_sec_identity, NOT_VTEP_DST, trace,
+						bpf_htons(ETH_P_IPV6));
 }
 #endif /* ENABLE_IPV6 */
 #endif /* ENABLE_EGRESS_GATEWAY_COMMON */
diff --git a/bpf/lib/encap.h b/bpf/lib/encap.h
index fa23a49312..1a07182881 100644
--- a/bpf/lib/encap.h
+++ b/bpf/lib/encap.h
@@ -13,7 +13,8 @@ static __always_inline int
 __encap_with_nodeid4(struct __ctx_buff *ctx, __u32 src_ip, __be16 src_port,
 		     __be32 tunnel_endpoint,
 		     __u32 seclabel, __u32 dstid, __u32 vni,
-		     enum trace_reason ct_reason, __u32 monitor, int *ifindex)
+		     enum trace_reason ct_reason, __u32 monitor, int *ifindex,
+		     __be16 proto)
 {
 	/* When encapsulating, a packet originating from the local host is
 	 * being considered as a packet from a remote node as it is being
@@ -31,7 +32,7 @@ __encap_with_nodeid4(struct __ctx_buff *ctx, __u32 src_ip, __be16 src_port,
 #endif
 
 	send_trace_notify(ctx, TRACE_TO_OVERLAY, seclabel, dstid, TRACE_EP_ID_UNKNOWN,
-			  *ifindex, ct_reason, monitor);
+			  *ifindex, ct_reason, monitor, proto);
 
 	return ctx_set_encap_info4(ctx, src_ip, src_port, tunnel_endpoint, seclabel, vni,
 				   NULL, 0);
@@ -40,7 +41,7 @@ __encap_with_nodeid4(struct __ctx_buff *ctx, __u32 src_ip, __be16 src_port,
 static __always_inline int
 __encap_with_nodeid6(struct __ctx_buff *ctx, const union v6addr *tunnel_endpoint,
 		     __u32 seclabel, __u32 dstid, enum trace_reason ct_reason,
-		     __u32 monitor, int *ifindex)
+		     __u32 monitor, int *ifindex, __be16 proto)
 {
 	/* When encapsulating, a packet originating from the local host is
 	 * being considered as a packet from a remote node as it is being
@@ -56,7 +57,7 @@ __encap_with_nodeid6(struct __ctx_buff *ctx, const union v6addr *tunnel_endpoint
 #endif
 
 	send_trace_notify(ctx, TRACE_TO_OVERLAY, seclabel, dstid, TRACE_EP_ID_UNKNOWN,
-			  *ifindex, ct_reason, monitor);
+			  *ifindex, ct_reason, monitor, proto);
 
 	return ctx_set_encap_info6(ctx, tunnel_endpoint, seclabel, NULL, 0);
 }
@@ -65,7 +66,7 @@ static __always_inline int
 __encap_and_redirect_with_nodeid(struct __ctx_buff *ctx,
 				 const struct remote_endpoint_info *info,
 				 __u32 seclabel, __u32 dstid, __u32 vni,
-				 const struct trace_ctx *trace)
+				 const struct trace_ctx *trace, __be16 proto)
 {
 	int ifindex;
 	int ret = 0;
@@ -73,12 +74,12 @@ __encap_and_redirect_with_nodeid(struct __ctx_buff *ctx,
 	if (info->flag_ipv6_tunnel_ep)
 		ret = __encap_with_nodeid6(ctx, &info->tunnel_endpoint.ip6,
 					   seclabel, dstid, trace->reason,
-					   trace->monitor, &ifindex);
+					   trace->monitor, &ifindex, proto);
 	else
 		ret = __encap_with_nodeid4(ctx, 0, 0,
 					   info->tunnel_endpoint.ip4, seclabel,
 					   dstid, vni, trace->reason,
-					   trace->monitor, &ifindex);
+					   trace->monitor, &ifindex, proto);
 	if (ret != CTX_ACT_REDIRECT)
 		return ret;
 
@@ -92,10 +93,11 @@ static __always_inline int
 encap_and_redirect_with_nodeid(struct __ctx_buff *ctx,
 			       struct remote_endpoint_info *info,
 			       __u32 seclabel, __u32 dstid,
-			       const struct trace_ctx *trace)
+			       const struct trace_ctx *trace,
+			       __be16 proto)
 {
 	return __encap_and_redirect_with_nodeid(ctx, info, seclabel, dstid,
-						NOT_VTEP_DST, trace);
+						NOT_VTEP_DST, trace, proto);
 }
 
 #if defined(TUNNEL_MODE)
@@ -104,9 +106,9 @@ encap_and_redirect_with_nodeid(struct __ctx_buff *ctx,
  */
 static __always_inline int
 encap_and_redirect_lxc(struct __ctx_buff *ctx, struct remote_endpoint_info *info,
-		       __u32 seclabel, __u32 dstid, const struct trace_ctx *trace)
+		       __u32 seclabel, __u32 dstid, const struct trace_ctx *trace, __be16 proto)
 {
-	return encap_and_redirect_with_nodeid(ctx, info, seclabel, dstid, trace);
+	return encap_and_redirect_with_nodeid(ctx, info, seclabel, dstid, trace, proto);
 }
 #endif /* TUNNEL_MODE */
 
@@ -141,7 +143,7 @@ __encap_with_nodeid_opt4(struct __ctx_buff *ctx, __u32 src_ip, __be16 src_port,
 			 __u32 seclabel, __u32 dstid, __u32 vni,
 			 void *opt, __u32 opt_len,
 			 enum trace_reason ct_reason,
-			 __u32 monitor, int *ifindex)
+			 __u32 monitor, int *ifindex, __be16 proto)
 {
 	/* When encapsulating, a packet originating from the local host is
 	 * being considered as a packet from a remote node as it is being
@@ -159,7 +161,7 @@ __encap_with_nodeid_opt4(struct __ctx_buff *ctx, __u32 src_ip, __be16 src_port,
 #endif
 
 	send_trace_notify(ctx, TRACE_TO_OVERLAY, seclabel, dstid, TRACE_EP_ID_UNKNOWN,
-			  *ifindex, ct_reason, monitor);
+			  *ifindex, ct_reason, monitor, proto);
 
 	return ctx_set_encap_info4(ctx, src_ip, src_port, tunnel_endpoint, seclabel, vni, opt,
 				   opt_len);
@@ -170,7 +172,7 @@ __encap_with_nodeid_opt6(struct __ctx_buff *ctx,
 			 const union v6addr *tunnel_endpoint, __u32 seclabel,
 			 __u32 dstid, void *opt, __u32 opt_len,
 			 enum trace_reason ct_reason, __u32 monitor,
-			 int *ifindex)
+			 int *ifindex, __be16 proto)
 {
 	/* When encapsulating, a packet originating from the local host is
 	 * being considered as a packet from a remote node as it is being
@@ -186,7 +188,7 @@ __encap_with_nodeid_opt6(struct __ctx_buff *ctx,
 #endif
 
 	send_trace_notify(ctx, TRACE_TO_OVERLAY, seclabel, dstid, TRACE_EP_ID_UNKNOWN,
-			  *ifindex, ct_reason, monitor);
+			  *ifindex, ct_reason, monitor, proto);
 
 	return ctx_set_encap_info6(ctx, tunnel_endpoint, seclabel, opt, opt_len);
 }
diff --git a/bpf/lib/host_firewall.h b/bpf/lib/host_firewall.h
index ac033fd72f..90dc9bcce6 100644
--- a/bpf/lib/host_firewall.h
+++ b/bpf/lib/host_firewall.h
@@ -156,7 +156,7 @@ __ipv6_host_policy_egress(struct __ctx_buff *ctx, bool is_host_id __maybe_unused
 		/* Trace the packet before it is forwarded to proxy */
 		send_trace_notify(ctx, TRACE_TO_PROXY, SECLABEL_IPV6, UNKNOWN_ID,
 				  bpf_ntohs(proxy_port), TRACE_IFINDEX_UNKNOWN,
-				  trace->reason, trace->monitor);
+				  trace->reason, trace->monitor, bpf_htons(ETH_P_IPV6));
 		return ctx_redirect_to_proxy_host_egress(ctx, proxy_port);
 	}
 
@@ -453,7 +453,7 @@ __ipv4_host_policy_egress(struct __ctx_buff *ctx, bool is_host_id __maybe_unused
 		/* Trace the packet before it is forwarded to proxy */
 		send_trace_notify(ctx, TRACE_TO_PROXY, SECLABEL_IPV4, UNKNOWN_ID,
 				  bpf_ntohs(proxy_port), TRACE_IFINDEX_UNKNOWN,
-				  trace->reason, trace->monitor);
+				  trace->reason, trace->monitor, bpf_htons(ETH_P_IP));
 		return ctx_redirect_to_proxy_host_egress(ctx, proxy_port);
 	}
 
diff --git a/bpf/lib/ipv6.h b/bpf/lib/ipv6.h
index a84ffdfa65..7a410603b1 100644
--- a/bpf/lib/ipv6.h
+++ b/bpf/lib/ipv6.h
@@ -5,6 +5,7 @@
 
 #include <linux/ipv6.h>
 
+#include "eth.h"
 #include "dbg.h"
 #include "l4.h"
 #include "metrics.h"
@@ -35,6 +36,8 @@
 #define IPV6_SADDR_OFF		offsetof(struct ipv6hdr, saddr)
 #define IPV6_DADDR_OFF		offsetof(struct ipv6hdr, daddr)
 
+#define IPV6_ALEN               16
+
 /* Follows the structure of ipv6hdr, see ipv6_handle_fragmentation. */
 struct ipv6_frag_id {
 	__be32 id;		/* L4 datagram identifier */
@@ -191,6 +194,34 @@ static __always_inline bool ipv6_addr_equals(const union v6addr *a,
 	return a->d2 == b->d2;
 }
 
+static __always_inline
+void ipv6_mc_mac_set(const union v6addr *addr, union macaddr *mac)
+{
+	mac->addr[0] = 0x33;
+	mac->addr[1] = 0x33;
+	memcpy((__u8 *)mac + 2, (__u8 *)addr + 12, 4);
+}
+
+static __always_inline
+bool ipv6_is_mc_mac(const union v6addr *addr, const union macaddr *mac)
+{
+	union macaddr mc_mac __align_stack_8;
+
+	ipv6_mc_mac_set(addr, &mc_mac);
+	return eth_addrcmp((const union macaddr *)&mc_mac, mac) == 0;
+}
+
+static __always_inline
+void ipv6_mc_addr_set(const union v6addr *addr, union v6addr *mc_addr)
+{
+	const union v6addr base_addr = { .addr = {0xff, 0x02, 0, 0, 0, 0, 0, 0,
+					  0, 0, 0, 0x01, 0xFF, 0, 0, 0} };
+	*mc_addr = base_addr;
+	mc_addr->addr[13] = addr->addr[13];
+	mc_addr->addr[14] = addr->addr[14];
+	mc_addr->addr[15] = addr->addr[15];
+}
+
 /* Only works with contiguous masks. */
 static __always_inline int ipv6_addr_in_net(const union v6addr *addr,
 					    const union v6addr *net,
diff --git a/bpf/lib/lb.h b/bpf/lib/lb.h
index cdba27df4d..5fbd19ce8e 100644
--- a/bpf/lib/lb.h
+++ b/bpf/lib/lb.h
@@ -1915,7 +1915,7 @@ static __always_inline int lb4_local(const void *map, struct __ctx_buff *ctx,
 		 * to make it appear from an outside address.
 		 */
 	#ifdef USE_LOOPBACK_LB
-		new_saddr = IPV4_LOOPBACK;
+		new_saddr = CONFIG(service_loopback_ipv4).be32;
 		state->loopback = 1;
 	#endif
 	}
diff --git a/bpf/lib/lxc.h b/bpf/lib/lxc.h
index c6ff5e103a..73e230c783 100644
--- a/bpf/lib/lxc.h
+++ b/bpf/lib/lxc.h
@@ -31,7 +31,7 @@ static __always_inline
 int is_valid_lxc_src_ipv4(const struct iphdr *ip4 __maybe_unused)
 {
 #ifdef ENABLE_IPV4
-	return ip4->saddr == LXC_IPV4;
+	return ip4->saddr == CONFIG(endpoint_ipv4).be32;
 #else
 	/* Can't send IPv4 if no IPv4 address is configured */
 	return 0;
diff --git a/bpf/lib/nat.h b/bpf/lib/nat.h
index 03e00f5913..b8059c8f44 100644
--- a/bpf/lib/nat.h
+++ b/bpf/lib/nat.h
@@ -24,9 +24,7 @@
 #include "stubs.h"
 #include "trace.h"
 
-DECLARE_CONFIG(__u32, nat_ipv4_masquerade, "Masquerade address for IPv4 traffic")
-#define IPV4_MASQUERADE CONFIG(nat_ipv4_masquerade)
-
+DECLARE_CONFIG(union v4addr, nat_ipv4_masquerade, "Masquerade address for IPv4 traffic")
 DECLARE_CONFIG(union v6addr, nat_ipv6_masquerade, "Masquerade address for IPv6 traffic")
 
 enum  nat_dir {
@@ -636,14 +634,15 @@ snat_v4_needs_masquerade(struct __ctx_buff *ctx __maybe_unused,
 
 #if defined(ENABLE_MASQUERADE_IPV4) && defined(IS_BPF_HOST)
 	/* To prevent aliasing with masqueraded connections,
-	 * we need to track all host connections that use IPV4_MASQUERADE.
+	 * we need to track all host connections that use config
+	 * nat_ipv4_masquerade.
 	 *
 	 * This either reserves the source port (so that it's not used
 	 * for masquerading), or port-SNATs the host connection (if the sport
 	 * is already in use for a masqueraded connection).
 	 */
-	if (tuple->saddr == IPV4_MASQUERADE) {
-		target->addr = IPV4_MASQUERADE;
+	if (tuple->saddr == CONFIG(nat_ipv4_masquerade).be32) {
+		target->addr = CONFIG(nat_ipv4_masquerade).be32;
 		target->needs_ct = true;
 
 		return NAT_NEEDED;
@@ -759,7 +758,7 @@ snat_v4_needs_masquerade(struct __ctx_buff *ctx __maybe_unused,
 	}
 
 	if (local_ep) {
-		target->addr = IPV4_MASQUERADE;
+		target->addr = CONFIG(nat_ipv4_masquerade).be32;
 		return NAT_NEEDED;
 	}
 #endif /*ENABLE_MASQUERADE_IPV4 && IS_BPF_HOST */
diff --git a/bpf/lib/nodeport.h b/bpf/lib/nodeport.h
index e47d0e8418..221b14e3d6 100644
--- a/bpf/lib/nodeport.h
+++ b/bpf/lib/nodeport.h
@@ -105,7 +105,7 @@ static __always_inline int
 nodeport_add_tunnel_encap(struct __ctx_buff *ctx, __u32 src_ip, __be16 src_port,
 			  const struct remote_endpoint_info *info,
 			  __u32 src_sec_identity, enum trace_reason ct_reason,
-			  __u32 monitor, int *ifindex)
+			  __u32 monitor, int *ifindex, __be16 proto)
 {
 	/* Let kernel choose the outer source ip */
 	if (ctx_is_skb())
@@ -126,10 +126,10 @@ nodeport_add_tunnel_encap(struct __ctx_buff *ctx, __u32 src_ip, __be16 src_port,
 	if (info->flag_ipv6_tunnel_ep)
 		return __encap_with_nodeid6(ctx, &info->tunnel_endpoint.ip6,
 					    src_sec_identity, info->sec_identity,
-					    ct_reason, monitor, ifindex);
+					    ct_reason, monitor, ifindex, proto);
 	return __encap_with_nodeid4(ctx, src_ip, src_port, info->tunnel_endpoint.ip4,
 				    src_sec_identity, info->sec_identity, NOT_VTEP_DST,
-				    ct_reason, monitor, ifindex);
+				    ct_reason, monitor, ifindex, proto);
 }
 
 # if defined(ENABLE_DSR) && DSR_ENCAP_MODE == DSR_ENCAP_GENEVE
@@ -138,7 +138,7 @@ nodeport_add_tunnel_encap_opt(struct __ctx_buff *ctx, __u32 src_ip, __be16 src_p
 			      const struct remote_endpoint_info *info,
 			      __u32 src_sec_identity, void *opt, __u32 opt_len,
 			      enum trace_reason ct_reason, __u32 monitor,
-			      int *ifindex)
+			      int *ifindex, __be16 proto)
 {
 	/* Let kernel choose the outer source ip */
 	if (ctx_is_skb())
@@ -160,10 +160,10 @@ nodeport_add_tunnel_encap_opt(struct __ctx_buff *ctx, __u32 src_ip, __be16 src_p
 		return __encap_with_nodeid_opt6(ctx, &info->tunnel_endpoint.ip6,
 						src_sec_identity, info->sec_identity,
 						opt, opt_len, ct_reason, monitor,
-						ifindex);
+						ifindex, proto);
 	return __encap_with_nodeid_opt4(ctx, src_ip, src_port, info->tunnel_endpoint.ip4,
 					src_sec_identity, info->sec_identity, NOT_VTEP_DST,
-					opt, opt_len, ct_reason, monitor, ifindex);
+					opt, opt_len, ct_reason, monitor, ifindex, proto);
 }
 # endif
 #endif /* HAVE_ENCAP */
@@ -426,7 +426,8 @@ static __always_inline int encap_geneve_dsr_opt6(struct __ctx_buff *ctx,
 						     sizeof(gopt),
 						     (enum trace_reason)CT_NEW,
 						     TRACE_PAYLOAD_LEN,
-						     ifindex);
+						     ifindex,
+						     bpf_htons(ETH_P_IPV6));
 
 	return nodeport_add_tunnel_encap(ctx,
 					 IPV4_DIRECT_ROUTING,
@@ -435,7 +436,8 @@ static __always_inline int encap_geneve_dsr_opt6(struct __ctx_buff *ctx,
 					 WORLD_IPV6_ID,
 					 (enum trace_reason)CT_NEW,
 					 TRACE_PAYLOAD_LEN,
-					 ifindex);
+					 ifindex,
+					 bpf_htons(ETH_P_IPV6));
 }
 #endif /* DSR_ENCAP_MODE */
 
@@ -968,7 +970,7 @@ encap_redirect:
 
 	ret = nodeport_add_tunnel_encap(ctx, IPV4_DIRECT_ROUTING, src_port,
 					info, src_sec_identity, trace->reason,
-					trace->monitor, &ifindex);
+					trace->monitor, &ifindex, bpf_htons(ETH_P_IPV6));
 	if (IS_ERR(ret))
 		return ret;
 
@@ -1228,7 +1230,8 @@ int tail_nodeport_nat_egress_ipv6(struct __ctx_buff *ctx)
 						WORLD_IPV6_ID,
 						trace.reason,
 						trace.monitor,
-						&oif);
+						&oif,
+						bpf_htons(ETH_P_IPV6));
 		if (IS_ERR(ret))
 			goto drop_err;
 
@@ -1307,7 +1310,8 @@ static __always_inline int nodeport_svc_lb6(struct __ctx_buff *ctx,
 
 		send_trace_notify(ctx, TRACE_TO_PROXY, src_sec_identity, UNKNOWN_ID,
 				  bpf_ntohs((__u16)svc->l7_lb_proxy_port),
-				  THIS_INTERFACE_IFINDEX, TRACE_REASON_POLICY, monitor);
+				  THIS_INTERFACE_IFINDEX, TRACE_REASON_POLICY, monitor,
+				  bpf_htons(ETH_P_IPV6));
 
 #  if defined(ENABLE_TPROXY)
 		return ctx_redirect_to_proxy_hairpin_ipv6(ctx, proxy_port);
@@ -1794,7 +1798,8 @@ static __always_inline int encap_geneve_dsr_opt4(struct __ctx_buff *ctx, int l3_
 						     sizeof(gopt),
 						     (enum trace_reason)CT_NEW,
 						     TRACE_PAYLOAD_LEN,
-						     ifindex);
+						     ifindex,
+						     bpf_htons(ETH_P_IP));
 
 	return nodeport_add_tunnel_encap(ctx,
 					 IPV4_DIRECT_ROUTING,
@@ -1803,7 +1808,8 @@ static __always_inline int encap_geneve_dsr_opt4(struct __ctx_buff *ctx, int l3_
 					 src_sec_identity,
 					 (enum trace_reason)CT_NEW,
 					 TRACE_PAYLOAD_LEN,
-					 ifindex);
+					 ifindex,
+					 bpf_htons(ETH_P_IP));
 }
 #endif /* DSR_ENCAP_MODE */
 
@@ -2296,7 +2302,8 @@ redirect:
 		fake_info.sec_identity = dst_sec_identity;
 		ret = nodeport_add_tunnel_encap(ctx, IPV4_DIRECT_ROUTING, src_port,
 						&fake_info, src_sec_identity,
-						trace->reason, trace->monitor, &ifindex);
+						trace->reason, trace->monitor, &ifindex,
+						bpf_htons(ETH_P_IP));
 		if (IS_ERR(ret))
 			return ret;
 
@@ -2553,7 +2560,8 @@ int tail_nodeport_nat_egress_ipv4(struct __ctx_buff *ctx)
 						src_sec_identity,
 						trace.reason,
 						trace.monitor,
-						&oif);
+						&oif,
+						bpf_htons(ETH_P_IP));
 		if (IS_ERR(ret))
 			goto drop_err;
 
@@ -2616,7 +2624,8 @@ static __always_inline int nodeport_svc_lb4(struct __ctx_buff *ctx,
 
 		send_trace_notify(ctx, TRACE_TO_PROXY, src_sec_identity, UNKNOWN_ID,
 				  bpf_ntohs(proxy_port),
-				  THIS_INTERFACE_IFINDEX, TRACE_REASON_POLICY, monitor);
+				  THIS_INTERFACE_IFINDEX, TRACE_REASON_POLICY, monitor,
+				  bpf_htons(ETH_P_IP));
 
 #  if defined(ENABLE_TPROXY)
 		return ctx_redirect_to_proxy_hairpin_ipv4(ctx, ip4, proxy_port);
diff --git a/bpf/lib/nodeport_egress.h b/bpf/lib/nodeport_egress.h
index fe518f4f83..8b2eb8f51f 100644
--- a/bpf/lib/nodeport_egress.h
+++ b/bpf/lib/nodeport_egress.h
@@ -278,7 +278,7 @@ int tail_handle_nat_fwd_ipv6(struct __ctx_buff *ctx)
 	if (ret == CTX_ACT_OK)
 		send_trace_notify(ctx, NODEPORT_OBS_POINT_EGRESS, src_id, UNKNOWN_ID,
 				  TRACE_EP_ID_UNKNOWN, THIS_INTERFACE_IFINDEX,
-				  trace.reason, trace.monitor);
+				  trace.reason, trace.monitor, bpf_htons(ETH_P_IPV6));
 
 	return ret;
 }
@@ -599,7 +599,7 @@ int tail_handle_nat_fwd_ipv4(struct __ctx_buff *ctx)
 	if (ret == CTX_ACT_OK)
 		send_trace_notify(ctx, NODEPORT_OBS_POINT_EGRESS, src_id, UNKNOWN_ID,
 				  TRACE_EP_ID_UNKNOWN, THIS_INTERFACE_IFINDEX,
-				  trace.reason, trace.monitor);
+				  trace.reason, trace.monitor, bpf_htons(ETH_P_IP));
 
 	return ret;
 }
diff --git a/bpf/lib/srv6.h b/bpf/lib/srv6.h
index 8adf194db2..27eda422a5 100644
--- a/bpf/lib/srv6.h
+++ b/bpf/lib/srv6.h
@@ -421,8 +421,8 @@ int tail_srv6_encap(struct __ctx_buff *ctx)
 		return send_drop_notify_error(ctx, SECLABEL_IPV6, ret, METRIC_EGRESS);
 
 	send_trace_notify(ctx, TRACE_TO_STACK, SECLABEL_IPV6, UNKNOWN_ID,
-			  TRACE_EP_ID_UNKNOWN,
-			  TRACE_IFINDEX_UNKNOWN, TRACE_REASON_SRV6_ENCAP, 0);
+			  TRACE_EP_ID_UNKNOWN, TRACE_IFINDEX_UNKNOWN,
+			  TRACE_REASON_SRV6_ENCAP, 0, bpf_htons(ETH_P_IPV6));
 
 	return ret;
 }
@@ -450,7 +450,7 @@ int tail_srv6_decap(struct __ctx_buff *ctx)
 	case IPPROTO_IPIP:
 		send_trace_notify(ctx, TRACE_FROM_NETWORK, SECLABEL_IPV4, UNKNOWN_ID,
 				  TRACE_EP_ID_UNKNOWN, TRACE_IFINDEX_UNKNOWN,
-				  TRACE_REASON_SRV6_DECAP, 0);
+				  TRACE_REASON_SRV6_DECAP, 0, bpf_htons(ETH_P_IP));
 		ret = tail_call_internal(ctx, CILIUM_CALL_IPV4_FROM_NETDEV, &ext_err);
 		return send_drop_notify_error_ext(ctx, SECLABEL_IPV6, ret, ext_err,
 						  METRIC_INGRESS);
@@ -459,7 +459,7 @@ int tail_srv6_decap(struct __ctx_buff *ctx)
 	case IPPROTO_IPV6:
 		send_trace_notify(ctx, TRACE_FROM_NETWORK, SECLABEL_IPV6, UNKNOWN_ID,
 				  TRACE_EP_ID_UNKNOWN, TRACE_IFINDEX_UNKNOWN,
-				  TRACE_REASON_SRV6_DECAP, 0);
+				  TRACE_REASON_SRV6_DECAP, 0, bpf_htons(ETH_P_IPV6));
 		ret = tail_call_internal(ctx, CILIUM_CALL_IPV6_FROM_NETDEV, &ext_err);
 		return send_drop_notify_error_ext(ctx, SECLABEL_IPV6, ret, ext_err,
 						  METRIC_INGRESS);
diff --git a/bpf/lib/trace.h b/bpf/lib/trace.h
index 121508465c..785073e647 100644
--- a/bpf/lib/trace.h
+++ b/bpf/lib/trace.h
@@ -214,13 +214,11 @@ emit_trace_notify(enum trace_point obs_point, __u32 monitor)
 	return true;
 }
 
-#define send_trace_notify(ctx, obs_point, src, dst, dst_id, ifindex, reason, monitor) \
-		_send_trace_notify(ctx, obs_point, src, dst, dst_id, ifindex, reason, monitor, \
-		__MAGIC_LINE__, __MAGIC_FILE__)
 static __always_inline void
 _send_trace_notify(struct __ctx_buff *ctx, enum trace_point obs_point,
 		   __u32 src, __u32 dst, __u16 dst_id, __u32 ifindex,
-		   enum trace_reason reason, __u32 monitor, __u16 line, __u8 file)
+		   enum trace_reason reason, __u32 monitor,
+		   __be16 proto __maybe_unused, __u16 line, __u8 file)
 {
 	__u64 ctx_len = ctx_full_len(ctx);
 	__u64 cap_len = min_t(__u64, monitor ? : TRACE_PAYLOAD_LEN,
@@ -266,9 +264,10 @@ _send_trace_notify(struct __ctx_buff *ctx, enum trace_point obs_point,
 }
 
 static __always_inline void
-send_trace_notify4(struct __ctx_buff *ctx, enum trace_point obs_point,
-		   __u32 src, __u32 dst, __be32 orig_addr, __u16 dst_id,
-		   __u32 ifindex, enum trace_reason reason, __u32 monitor)
+_send_trace_notify4(struct __ctx_buff *ctx, enum trace_point obs_point,
+		    __u32 src, __u32 dst, __be32 orig_addr, __u16 dst_id,
+		    __u32 ifindex, enum trace_reason reason, __u32 monitor,
+		    __u16 line, __u8 file)
 {
 	__u64 ctx_len = ctx_full_len(ctx);
 	__u64 cap_len = min_t(__u64, monitor ? : TRACE_PAYLOAD_LEN,
@@ -282,7 +281,7 @@ send_trace_notify4(struct __ctx_buff *ctx, enum trace_point obs_point,
 	struct trace_notify msg __align_stack_8;
 	cls_flags_t flags = CLS_FLAG_NONE;
 
-	update_trace_metrics(ctx, obs_point, reason);
+	_update_trace_metrics(ctx, obs_point, reason, line, file);
 
 	if (!emit_trace_notify(obs_point, monitor))
 		return;
@@ -315,10 +314,10 @@ send_trace_notify4(struct __ctx_buff *ctx, enum trace_point obs_point,
 }
 
 static __always_inline void
-send_trace_notify6(struct __ctx_buff *ctx, enum trace_point obs_point,
-		   __u32 src, __u32 dst, const union v6addr *orig_addr,
-		   __u16 dst_id, __u32 ifindex, enum trace_reason reason,
-		   __u32 monitor)
+_send_trace_notify6(struct __ctx_buff *ctx, enum trace_point obs_point,
+		    __u32 src, __u32 dst, const union v6addr *orig_addr,
+		    __u16 dst_id, __u32 ifindex, enum trace_reason reason,
+		    __u32 monitor, __u16 line, __u8 file)
 {
 	__u64 ctx_len = ctx_full_len(ctx);
 	__u64 cap_len = min_t(__u64, monitor ? : TRACE_PAYLOAD_LEN,
@@ -332,7 +331,7 @@ send_trace_notify6(struct __ctx_buff *ctx, enum trace_point obs_point,
 	struct trace_notify msg __align_stack_8;
 	cls_flags_t flags = CLS_FLAG_NONE;
 
-	update_trace_metrics(ctx, obs_point, reason);
+	_update_trace_metrics(ctx, obs_point, reason, line, file);
 
 	if (!emit_trace_notify(obs_point, monitor))
 		return;
@@ -365,31 +364,49 @@ send_trace_notify6(struct __ctx_buff *ctx, enum trace_point obs_point,
 }
 #else
 static __always_inline void
-send_trace_notify(struct __ctx_buff *ctx, enum trace_point obs_point,
-		  __u32 src __maybe_unused, __u32 dst __maybe_unused,
-		  __u16 dst_id __maybe_unused, __u32 ifindex __maybe_unused,
-		  enum trace_reason reason, __u32 monitor __maybe_unused)
+_send_trace_notify(struct __ctx_buff *ctx, enum trace_point obs_point,
+		   __u32 src __maybe_unused, __u32 dst __maybe_unused,
+		   __u16 dst_id __maybe_unused, __u32 ifindex __maybe_unused,
+		   enum trace_reason reason, __u32 monitor __maybe_unused,
+		   __be16 proto __maybe_unused, __u16 line, __u8 file)
 {
-	update_trace_metrics(ctx, obs_point, reason);
+	_update_trace_metrics(ctx, obs_point, reason, line, file);
 }
 
 static __always_inline void
-send_trace_notify4(struct __ctx_buff *ctx, enum trace_point obs_point,
-		   __u32 src __maybe_unused, __u32 dst __maybe_unused,
-		   __be32 orig_addr __maybe_unused, __u16 dst_id __maybe_unused,
-		   __u32 ifindex __maybe_unused, enum trace_reason reason,
-		   __u32 monitor __maybe_unused)
+_send_trace_notify4(struct __ctx_buff *ctx, enum trace_point obs_point,
+		    __u32 src __maybe_unused, __u32 dst __maybe_unused,
+		    __be32 orig_addr __maybe_unused, __u16 dst_id __maybe_unused,
+		    __u32 ifindex __maybe_unused, enum trace_reason reason,
+		    __u32 monitor __maybe_unused,
+		    __u16 line, __u8 file)
 {
-	update_trace_metrics(ctx, obs_point, reason);
+	_update_trace_metrics(ctx, obs_point, reason, line, file);
 }
 
 static __always_inline void
-send_trace_notify6(struct __ctx_buff *ctx, enum trace_point obs_point,
-		   __u32 src __maybe_unused, __u32 dst __maybe_unused,
-		   union v6addr *orig_addr __maybe_unused,
-		   __u16 dst_id __maybe_unused, __u32 ifindex __maybe_unused,
-		   enum trace_reason reason, __u32 monitor __maybe_unused)
+_send_trace_notify6(struct __ctx_buff *ctx, enum trace_point obs_point,
+		    __u32 src __maybe_unused, __u32 dst __maybe_unused,
+		    union v6addr *orig_addr __maybe_unused,
+		    __u16 dst_id __maybe_unused, __u32 ifindex __maybe_unused,
+		    enum trace_reason reason, __u32 monitor __maybe_unused,
+		    __u16 line, __u8 file)
 {
-	update_trace_metrics(ctx, obs_point, reason);
+	_update_trace_metrics(ctx, obs_point, reason, line, file);
 }
 #endif /* TRACE_NOTIFY */
+
+/* send_trace_notify emits a generic trace notify. */
+#define send_trace_notify(ctx, obs_point, src, dst, dst_id, ifindex, reason, monitor, proto) \
+	_send_trace_notify(ctx, obs_point, src, dst, dst_id, ifindex, reason, monitor, proto, \
+	__MAGIC_LINE__, __MAGIC_FILE__)
+
+/* send_trace_notify4 emits a trace notify with the original IPv4 address before translation. */
+#define send_trace_notify4(ctx, obs_point, src, dst, orig_addr, dst_id, ifindex, reason, monitor) \
+	_send_trace_notify4(ctx, obs_point, src, dst, orig_addr, dst_id, ifindex, reason, monitor, \
+	__MAGIC_LINE__, __MAGIC_FILE__)
+
+/* send_trace_notify6 emits a trace notify with the original IPv6 address before translation. */
+#define send_trace_notify6(ctx, obs_point, src, dst, orig_addr, dst_id, ifindex, reason, monitor) \
+	_send_trace_notify6(ctx, obs_point, src, dst, orig_addr, dst_id, ifindex, reason, monitor, \
+	__MAGIC_LINE__, __MAGIC_FILE__)
diff --git a/bpf/node_config.h b/bpf/node_config.h
index 8066e6f4d3..56bc367bbe 100644
--- a/bpf/node_config.h
+++ b/bpf/node_config.h
@@ -83,7 +83,6 @@
 #ifdef ENABLE_IPV4
 #define IPV4_MASK 0xffff
 #define IPV4_GATEWAY 0xfffff50a
-#define IPV4_LOOPBACK 0x1ffff50a
 #define IPV4_ENCRYPT_IFACE 0xfffff50a
 # ifdef ENABLE_MASQUERADE_IPV4
 #  define IPV4_SNAT_EXCLUSION_DST_CIDR 0xffff0000
diff --git a/bpf/tests/hairpin_sctp_flow.c b/bpf/tests/hairpin_sctp_flow.c
index 0e376536e5..17e9b68fc4 100644
--- a/bpf/tests/hairpin_sctp_flow.c
+++ b/bpf/tests/hairpin_sctp_flow.c
@@ -26,7 +26,8 @@ mock_ctx_redirect_peer(const struct __sk_buff *ctx __maybe_unused, int ifindex _
 #include <bpf_lxc.c>
 
 /* Set the LXC source address to be the address of pod one */
-ASSIGN_CONFIG(__u32, endpoint_ipv4, v4_pod_one)
+ASSIGN_CONFIG(union v4addr, endpoint_ipv4, { .be32 = v4_pod_one })
+ASSIGN_CONFIG(union v4addr, service_loopback_ipv4, { .be32 = v4_svc_loopback })
 
 #include "lib/endpoint.h"
 #include "lib/ipcache.h"
@@ -128,7 +129,7 @@ int hairpin_flow_forward_check(__maybe_unused const struct __ctx_buff *ctx)
 	if ((void *)l3 + sizeof(struct iphdr) > data_end)
 		test_fatal("l3 out of bounds");
 
-	if (l3->saddr != IPV4_LOOPBACK)
+	if (l3->saddr != CONFIG(service_loopback_ipv4).be32)
 		test_fatal("src IP was not SNAT'ed");
 
 	if (l3->daddr != v4_pod_one)
@@ -165,7 +166,7 @@ int hairpin_flow_forward_ingress_pktgen(struct __ctx_buff *ctx)
 	pktgen__init(&builder, ctx);
 
 	l3 = pktgen__push_ipv4_packet(&builder, (__u8 *)src, (__u8 *)dst,
-				      IPV4_LOOPBACK, v4_pod_one);
+				      CONFIG(service_loopback_ipv4).be32, v4_pod_one);
 	if (!l3)
 		return TEST_ERROR;
 
@@ -219,7 +220,7 @@ int hairpin_flow_forward_ingress_check(__maybe_unused const struct __ctx_buff *c
 	if ((void *)l3 + sizeof(struct iphdr) > data_end)
 		test_fatal("l3 out of bounds");
 
-	if (l3->saddr != IPV4_LOOPBACK)
+	if (l3->saddr != CONFIG(service_loopback_ipv4).be32)
 		test_fatal("src IP changed");
 
 	if (l3->daddr != v4_pod_one)
@@ -256,7 +257,7 @@ int hairpin_flow_rev_setup(struct __ctx_buff *ctx)
 	pktgen__init(&builder, ctx);
 
 	l3 = pktgen__push_ipv4_packet(&builder, (__u8 *)src, (__u8 *)dst,
-				      v4_pod_one, IPV4_LOOPBACK);
+				      v4_pod_one, CONFIG(service_loopback_ipv4).be32);
 	if (!l3)
 		return TEST_ERROR;
 
@@ -307,7 +308,7 @@ int hairpin_flow_rev_check(__maybe_unused const struct __ctx_buff *ctx)
 	if (l3->saddr != v4_pod_one)
 		test_fatal("src IP changed");
 
-	if (l3->daddr != IPV4_LOOPBACK)
+	if (l3->daddr != CONFIG(service_loopback_ipv4).be32)
 		test_fatal("dest IP changed");
 
 	l4 = (void *)l3 + sizeof(struct iphdr);
@@ -337,7 +338,7 @@ int hairpin_sctp_flow_4_reverse_ingress_v4_pktgen(struct __ctx_buff *ctx)
 	pktgen__init(&builder, ctx);
 
 	l3 = pktgen__push_ipv4_packet(&builder, (__u8 *)src, (__u8 *)dst,
-				      v4_pod_one, IPV4_LOOPBACK);
+				      v4_pod_one, CONFIG(service_loopback_ipv4).be32);
 	if (!l3)
 		return TEST_ERROR;
 
diff --git a/bpf/tests/inter_cluster_snat_clusterip_backend_lxc.c b/bpf/tests/inter_cluster_snat_clusterip_backend_lxc.c
index af08cb807b..a766e66f08 100644
--- a/bpf/tests/inter_cluster_snat_clusterip_backend_lxc.c
+++ b/bpf/tests/inter_cluster_snat_clusterip_backend_lxc.c
@@ -55,7 +55,7 @@
 #include <bpf_lxc.c>
 
 /* Set the LXC source address to be the address of the backend pod */
-ASSIGN_CONFIG(__u32, endpoint_ipv4, BACKEND_IP)
+ASSIGN_CONFIG(union v4addr, endpoint_ipv4, { .be32 = BACKEND_IP})
 
 #include "lib/ipcache.h"
 #include "lib/policy.h"
diff --git a/bpf/tests/inter_cluster_snat_clusterip_client_lxc.c b/bpf/tests/inter_cluster_snat_clusterip_client_lxc.c
index a07781b961..d8f3b9217c 100644
--- a/bpf/tests/inter_cluster_snat_clusterip_client_lxc.c
+++ b/bpf/tests/inter_cluster_snat_clusterip_client_lxc.c
@@ -58,7 +58,7 @@
 #include <bpf_lxc.c>
 
 /* Set the LXC source address to be the address of pod one */
-ASSIGN_CONFIG(__u32, endpoint_ipv4, CLIENT_IP)
+ASSIGN_CONFIG(union v4addr, endpoint_ipv4, { .be32 = CLIENT_IP})
 
 #include "lib/ipcache.h"
 #include "lib/lb.h"
diff --git a/bpf/tests/ipsec_encryption_on_egress.c b/bpf/tests/ipsec_encryption_on_egress.c
index 70b03332d0..46d740e876 100644
--- a/bpf/tests/ipsec_encryption_on_egress.c
+++ b/bpf/tests/ipsec_encryption_on_egress.c
@@ -18,7 +18,6 @@
 #define ENABLE_IPV4
 #define ENABLE_IPV6
 #define ENABLE_IPSEC
-#define IPV4_LOOPBACK 0
 
 #define TO_NETDEV 0
 
diff --git a/bpf/tests/ipv6_ndp_from_netdev_test.c b/bpf/tests/ipv6_ndp_from_netdev_test.c
index fba860ba93..7569fe36ae 100644
--- a/bpf/tests/ipv6_ndp_from_netdev_test.c
+++ b/bpf/tests/ipv6_ndp_from_netdev_test.c
@@ -15,8 +15,6 @@
 
 #define FROM_NETDEV 0
 
-#define V6_ALEN 16
-
 struct {
 	__uint(type, BPF_MAP_TYPE_PROG_ARRAY);
 	__uint(key_size, sizeof(__u32));
@@ -67,7 +65,7 @@ int __ipv6_from_netdev_ns_pktgen(struct __ctx_buff *ctx,
 		return TEST_ERROR;
 
 	data = pktgen__push_data(&builder, (__u8 *)&args->icmp_ns_addr,
-				 V6_ALEN);
+				 IPV6_ALEN);
 	if (!data)
 		return TEST_ERROR;
 
@@ -122,10 +120,10 @@ int __ipv6_from_netdev_ns_check(const struct __ctx_buff *ctx,
 	if ((void *)l3 + sizeof(struct ipv6hdr) > data_end)
 		test_fatal("l3 out of bounds");
 
-	if (memcmp((__u8 *)&l3->saddr, (__u8 *)&args->ip_src, V6_ALEN) != 0)
+	if (memcmp((__u8 *)&l3->saddr, (__u8 *)&args->ip_src, IPV6_ALEN) != 0)
 		test_fatal("Incorrect ip_src");
 
-	if (memcmp((__u8 *)&l3->daddr, (__u8 *)&args->ip_dst, V6_ALEN) != 0)
+	if (memcmp((__u8 *)&l3->daddr, (__u8 *)&args->ip_dst, IPV6_ALEN) != 0)
 		test_fatal("Incorrect ip_dst");
 
 	l4 = (void *)l3 + sizeof(struct ipv6hdr);
@@ -137,14 +135,14 @@ int __ipv6_from_netdev_ns_check(const struct __ctx_buff *ctx,
 		test_fatal("Invalid ICMP type");
 
 	target_addr = (void *)l4 + sizeof(struct icmp6hdr);
-	if ((void *)target_addr + V6_ALEN > data_end)
+	if ((void *)target_addr + IPV6_ALEN > data_end)
 		test_fatal("Target addr out of bounds");
 
-	if (memcmp(target_addr, (__u8 *)&args->icmp_ns_addr, V6_ALEN) != 0)
+	if (memcmp(target_addr, (__u8 *)&args->icmp_ns_addr, IPV6_ALEN) != 0)
 		test_fatal("Incorrect icmp6 payload target addr");
 
 	if (args->llsrc_opt) {
-		opt = target_addr + V6_ALEN;
+		opt = target_addr + IPV6_ALEN;
 
 		if ((void *)opt + ICMP6_ND_OPT_LEN > data_end)
 			test_fatal("llsrc_opt out of bounds");
@@ -179,10 +177,10 @@ void __ipv6_from_netdev_ns_pod_pktgen_args(struct test_args *args,
 	memcpy((__u8 *)args->mac_src, (__u8 *)mac_one, ETH_ALEN);
 	memcpy((__u8 *)args->mac_dst, (__u8 *)mac_two, ETH_ALEN);
 
-	memcpy((__u8 *)&args->ip_src, (__u8 *)v6_pod_one, V6_ALEN);
-	memcpy((__u8 *)&args->ip_dst, (__u8 *)v6_pod_two, V6_ALEN);
+	memcpy((__u8 *)&args->ip_src, (__u8 *)v6_pod_one, IPV6_ALEN);
+	memcpy((__u8 *)&args->ip_dst, (__u8 *)v6_pod_two, IPV6_ALEN);
 
-	memcpy((__u8 *)&args->icmp_ns_addr, (__u8 *)v6_pod_three, V6_ALEN);
+	memcpy((__u8 *)&args->icmp_ns_addr, (__u8 *)v6_pod_three, IPV6_ALEN);
 
 	args->llsrc_opt = llsrc_opt;
 	args->icmp_opt.type = 0x1;
@@ -201,11 +199,11 @@ void __ipv6_from_netdev_ns_pod_check_args(struct test_args *args,
 	memcpy((__u8 *)args->mac_src, (__u8 *)&node_mac.addr, ETH_ALEN);
 	memcpy((__u8 *)args->mac_dst, (__u8 *)mac_one, ETH_ALEN);
 
-	memcpy((__u8 *)&args->ip_src, (__u8 *)v6_pod_three, V6_ALEN);
-	memcpy((__u8 *)&args->ip_dst, (__u8 *)v6_pod_one, V6_ALEN);
+	memcpy((__u8 *)&args->ip_src, (__u8 *)v6_pod_three, IPV6_ALEN);
+	memcpy((__u8 *)&args->ip_dst, (__u8 *)v6_pod_one, IPV6_ALEN);
 
 	args->icmp_type = ICMP6_NA_MSG_TYPE;
-	memcpy((__u8 *)&args->icmp_ns_addr, (__u8 *)v6_pod_three, V6_ALEN);
+	memcpy((__u8 *)&args->icmp_ns_addr, (__u8 *)v6_pod_three, IPV6_ALEN);
 
 	args->llsrc_opt = llsrc_opt;
 	args->icmp_opt.type = 0x2;
@@ -279,18 +277,15 @@ void __ipv6_from_netdev_ns_pod_pktgen_mcast_args(struct test_args *args,
 	__u8 llsrc_mac[] = {0x1, 0x1, 0x1, 0x1, 0x1, 0x1};
 
 	memcpy((__u8 *)args->mac_src, (__u8 *)mac_one, ETH_ALEN);
-	/* IPv6 mcast mac addr is 33:33 followed by 32 LSBs from target IP */
-	memcpy(args->mac_dst, (void *)mac_v6mcast_base, 2);
-	memcpy((__u8 *)args->mac_dst + 2, (__u8 *)v6_pod_three + 12, 4);
 
-	memcpy((__u8 *)&args->ip_src, (__u8 *)v6_pod_one, V6_ALEN);
-	/* IPv6 mcast addr has the 24 LSBs from the target IP */
-	memcpy(&args->ip_dst, (void *)v6_mcast_base, V6_ALEN);
-	args->ip_dst.addr[13] = v6_pod_three[13];
-	args->ip_dst.addr[14] = v6_pod_three[14];
-	args->ip_dst.addr[15] = v6_pod_three[15];
+	ipv6_mc_mac_set((union v6addr *)v6_pod_three,
+			(union macaddr *)args->mac_dst);
+
+	memcpy((__u8 *)&args->ip_src, (__u8 *)v6_pod_one, IPV6_ALEN);
 
-	memcpy((__u8 *)&args->icmp_ns_addr, (__u8 *)v6_pod_three, V6_ALEN);
+	ipv6_mc_addr_set((union v6addr *)v6_pod_three, &args->ip_dst);
+
+	memcpy((__u8 *)&args->icmp_ns_addr, (__u8 *)v6_pod_three, IPV6_ALEN);
 
 	args->llsrc_opt = llsrc_opt;
 	args->icmp_opt.type = 0x1;
@@ -371,10 +366,10 @@ void __ipv6_from_netdev_ns_node_ip_pktgen_args(struct test_args *args,
 	memcpy((__u8 *)args->mac_src, (__u8 *)mac_one, ETH_ALEN);
 	memcpy((__u8 *)args->mac_dst, (__u8 *)mac_two, ETH_ALEN);
 
-	memcpy((__u8 *)&args->ip_src, (__u8 *)v6_pod_one, V6_ALEN);
-	memcpy((__u8 *)&args->ip_dst, (__u8 *)v6_pod_two, V6_ALEN);
+	memcpy((__u8 *)&args->ip_src, (__u8 *)v6_pod_one, IPV6_ALEN);
+	memcpy((__u8 *)&args->ip_dst, (__u8 *)v6_pod_two, IPV6_ALEN);
 
-	memcpy((__u8 *)&args->icmp_ns_addr, (__u8 *)&v6_node_one, V6_ALEN);
+	memcpy((__u8 *)&args->icmp_ns_addr, (__u8 *)&v6_node_one, IPV6_ALEN);
 
 	args->icmp_type = ICMP6_NS_MSG_TYPE;
 
@@ -461,18 +456,15 @@ void __ipv6_from_netdev_ns_node_ip_pktgen_mcast_args(struct test_args *args,
 	__u8 llsrc_mac[] = {0x1, 0x1, 0x1, 0x1, 0x1, 0x1};
 
 	memcpy((__u8 *)args->mac_src, (__u8 *)mac_one, ETH_ALEN);
-	/* IPv6 mcast mac addr is 33:33 followed by 32 LSBs from target IP */
-	memcpy(args->mac_dst, (void *)mac_v6mcast_base, 2);
-	memcpy((__u8 *)args->mac_dst + 2, (__u8 *)v6_node_one + 12, 4);
-
-	memcpy((__u8 *)&args->ip_src, (__u8 *)v6_pod_one, V6_ALEN);
-	/* IPv6 mcast addr has the 24 LSBs from the target IP */
-	memcpy(&args->ip_dst, (void *)v6_mcast_base, V6_ALEN);
-	args->ip_dst.addr[13] = v6_node_one[13];
-	args->ip_dst.addr[14] = v6_node_one[14];
-	args->ip_dst.addr[15] = v6_node_one[15];
-
-	memcpy((__u8 *)&args->icmp_ns_addr, (__u8 *)&v6_node_one, V6_ALEN);
+
+	ipv6_mc_mac_set((union v6addr *)v6_pod_one,
+			(union macaddr *)args->mac_dst);
+
+	memcpy((__u8 *)&args->ip_src, (__u8 *)v6_pod_one, IPV6_ALEN);
+
+	ipv6_mc_addr_set((union v6addr *)v6_pod_one, &args->ip_dst);
+
+	memcpy((__u8 *)&args->icmp_ns_addr, (__u8 *)&v6_node_one, IPV6_ALEN);
 
 	args->icmp_type = ICMP6_NS_MSG_TYPE;
 
diff --git a/bpf/tests/ipv6_test.c b/bpf/tests/ipv6_test.c
index 04be13b272..6ec130fbab 100644
--- a/bpf/tests/ipv6_test.c
+++ b/bpf/tests/ipv6_test.c
@@ -232,4 +232,49 @@ int bpf_test(__maybe_unused struct xdp_md *ctx)
 	test_finish();
 }
 
+CHECK("tc", "test_ipv6_mc_helpers")
+int test_ipv6_mc_helpers(__maybe_unused struct __ctx_buff *ctx)
+{
+	union macaddr mac = {{0}};
+	union v6addr addr = {{0}};
+
+	test_init();
+
+	/* IPv6 mcast mac addr is 33:33 followed by 32 LSBs from target IP */
+	ipv6_mc_mac_set((union v6addr *)&v6_pod_one, &mac);
+	assert(mac.addr[0] == 0x33);
+	assert(mac.addr[1] == 0x33);
+	assert(mac.addr[2] == v6_pod_one[12]);
+	assert(mac.addr[3] == v6_pod_one[13]);
+	assert(mac.addr[4] == v6_pod_one[14]);
+	assert(mac.addr[5] == v6_pod_one[15]);
+	assert(ipv6_is_mc_mac((union v6addr *)&v6_pod_one, &mac));
+	mac.addr[5] += 0x1;
+	assert(!ipv6_is_mc_mac((union v6addr *)&v6_pod_one, &mac));
+
+	/*
+	 * IPv6 mcast addr is ff02::1:ffXX:XXXX where XX:XXXX are 24 LSBs from
+	 * the target IP
+	 */
+	ipv6_mc_addr_set((union v6addr *)&v6_pod_one, &addr);
+	assert(addr.addr[0] == 0xFF);
+	assert(addr.addr[1] == 0x02);
+	assert(addr.addr[2] == 0x00);
+	assert(addr.addr[3] == 0x00);
+	assert(addr.addr[4] == 0x00);
+	assert(addr.addr[5] == 0x00);
+	assert(addr.addr[6] == 0x00);
+	assert(addr.addr[7] == 0x00);
+	assert(addr.addr[8] == 0x00);
+	assert(addr.addr[9] == 0x00);
+	assert(addr.addr[10] == 0x00);
+	assert(addr.addr[11] == 0x01);
+	assert(addr.addr[12] == 0xFF);
+	assert(addr.addr[13] == v6_pod_one[13]);
+	assert(addr.addr[14] == v6_pod_one[14]);
+	assert(addr.addr[15] == v6_pod_one[15]);
+
+	test_finish();
+}
+
 BPF_LICENSE("Dual BSD/GPL");
diff --git a/bpf/tests/lib/policy.h b/bpf/tests/lib/policy.h
index c2b880c7bb..f75b0da740 100644
--- a/bpf/tests/lib/policy.h
+++ b/bpf/tests/lib/policy.h
@@ -23,6 +23,12 @@ policy_add_ingress_allow_entry(__u32 sec_label, __u8 protocol, __u16 dport)
 	policy_add_entry(false, sec_label, protocol, dport, false);
 }
 
+static __always_inline void
+policy_add_ingress_deny_all_entry(void)
+{
+	policy_add_entry(false, 0, 0, 0, true);
+}
+
 static __always_inline void
 policy_add_egress_allow_entry(__u32 sec_label, __u8 protocol, __u16 dport)
 {
diff --git a/bpf/tests/pktgen.h b/bpf/tests/pktgen.h
index 60183825b6..58d4424f37 100644
--- a/bpf/tests/pktgen.h
+++ b/bpf/tests/pktgen.h
@@ -31,7 +31,6 @@
 #define mac_five_addr {0x15, 0x21, 0x39, 0x45, 0x4D, 0x5D}
 #define mac_six_addr {0x08, 0x14, 0x1C, 0x32, 0x52, 0x7E}
 #define mac_zero_addr {0x0, 0x0, 0x0, 0x0, 0x0, 0x0}
-#define mac_v6mcast_base_addr {0x33, 0x33, 0x00, 0x00, 0x00, 0x00}
 
 volatile const __u8 mac_one[] = mac_one_addr;
 volatile const __u8 mac_two[] = mac_two_addr;
@@ -40,7 +39,6 @@ volatile const __u8 mac_four[] = mac_four_addr;
 volatile const __u8 mac_five[] = mac_five_addr;
 volatile const __u8 mac_six[] = mac_six_addr;
 volatile const __u8 mac_zero[] = mac_zero_addr;
-volatile const __u8 mac_v6mcast_base[] = mac_v6mcast_base_addr;
 
 /* A collection of pre-defined IP addresses, so tests can reuse them without
  *  having to come up with custom ips.
@@ -68,11 +66,9 @@ volatile const __u8 mac_v6mcast_base[] = mac_v6mcast_base_addr;
 #define v4_pod_two	IPV4(192, 168, 0, 2)
 #define v4_pod_three	IPV4(192, 168, 0, 3)
 
-#define v4_all	IPV4(0, 0, 0, 0)
+#define v4_svc_loopback	IPV4(10, 245, 255, 31)
 
-/* IPv6 mcast base address */
-#define v6_mcast_base_addr {0xff, 0x02, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0x01, 0xFF, 0, 0, 0};
-volatile const __u8 v6_mcast_base[] = v6_mcast_base_addr;
+#define v4_all	IPV4(0, 0, 0, 0)
 
 /* IPv6 addresses for pods in the cluster */
 #define v6_pod_one_addr {0xfd, 0x04, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1}
diff --git a/bpf/tests/skip_tunnel_nodeport_masq.c b/bpf/tests/skip_tunnel_nodeport_masq.c
index a9d05ed120..5aad475377 100644
--- a/bpf/tests/skip_tunnel_nodeport_masq.c
+++ b/bpf/tests/skip_tunnel_nodeport_masq.c
@@ -197,7 +197,7 @@ check_ctx(const struct __ctx_buff *ctx, bool v4, bool snat)
 			test_fatal("l3 out of bounds");
 
 		if (snat) {
-			if (l3->saddr != IPV4_MASQUERADE)
+			if (l3->saddr != CONFIG(nat_ipv4_masquerade).be32)
 				test_fatal("src IP was not snatted");
 		} else {
 			if (l3->saddr != SRC_IPV4)
diff --git a/bpf/tests/tc_geneve_dsr_v4_legacy.c b/bpf/tests/tc_geneve_dsr_v4_legacy.c
new file mode 100644
index 0000000000..6d93974182
--- /dev/null
+++ b/bpf/tests/tc_geneve_dsr_v4_legacy.c
@@ -0,0 +1,154 @@
+// SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause)
+/* Copyright Authors of Cilium */
+
+#include <bpf/ctx/skb.h>
+#include "common.h"
+#include "pktgen.h"
+
+/* Enable code paths under test */
+#define ENABLE_IPV4
+
+#define ENABLE_NODEPORT 1
+#define ENABLE_DSR 1
+#define DSR_ENCAP_IPIP 2
+#define DSR_ENCAP_GENEVE 3
+#define DSR_ENCAP_MODE DSR_ENCAP_GENEVE
+
+#define TUNNEL_PROTOCOL TUNNEL_PROTOCOL_GENEVE
+#define ENCAP_IFINDEX 42
+#define TUNNEL_MODE
+
+#define CLIENT_IP v4_pod_one
+#define CLIENT_PORT __bpf_htons(111)
+
+#define BACKEND_IP		v4_pod_two
+#define BACKEND_PORT		__bpf_htons(8080)
+
+#define NODE_IP v4_node_one
+
+static volatile const __u8 *client_mac = mac_one;
+static volatile const __u8 *server_mac = mac_two;
+
+#define skb_get_tunnel_key mock_skb_get_tunnel_key
+int mock_skb_get_tunnel_key(__maybe_unused struct __sk_buff *skb,
+			    __maybe_unused  struct bpf_tunnel_key *to,
+			    __maybe_unused __u32 size,
+			    __maybe_unused __u32 flags)
+{
+	return 0;
+}
+
+#define skb_get_tunnel_opt mock_skb_get_tunnel_opt
+int mock_skb_get_tunnel_opt(__maybe_unused struct __sk_buff *skb,
+			    void *opt, __u32 size)
+{
+	struct geneve_dsr_opt4 *gopt = opt;
+
+	gopt->hdr.opt_class = bpf_htons(DSR_GENEVE_OPT_CLASS);
+	gopt->hdr.type = DSR_GENEVE_OPT_TYPE;
+	return size;
+}
+
+#include "bpf_overlay.c"
+
+#include "lib/endpoint.h"
+
+#define FROM_OVERLAY 0
+
+struct {
+	__uint(type, BPF_MAP_TYPE_PROG_ARRAY);
+	__uint(key_size, sizeof(__u32));
+	__uint(max_entries, 1);
+	__array(values, int());
+} entry_call_map __section(".maps") = {
+	.values = {
+		[FROM_OVERLAY] = &cil_from_overlay,
+	},
+};
+
+PKTGEN("tc", "tc_geneve_dsr_v4_legacy")
+int tc_geneve_dsr_v4_legacy_pktgen(struct __ctx_buff *ctx)
+{
+	struct pktgen builder;
+	struct tcphdr *l4;
+	void *data;
+
+	/* Init packet builder */
+	pktgen__init(&builder, ctx);
+
+	l4 = pktgen__push_ipv4_tcp_packet(&builder,
+					  (__u8 *)client_mac, (__u8 *)server_mac,
+					  CLIENT_IP, BACKEND_IP,
+					  CLIENT_PORT, BACKEND_PORT);
+	if (!l4)
+		return TEST_ERROR;
+
+	data = pktgen__push_data(&builder, default_data, sizeof(default_data));
+	if (!data)
+		return TEST_ERROR;
+
+	/* Calc lengths, set protocol fields and calc checksums */
+	pktgen__finish(&builder);
+
+	return 0;
+}
+
+SETUP("tc", "tc_geneve_dsr_v4_legacy")
+int tc_geneve_dsr_v4_legacy_setup(struct __ctx_buff *ctx)
+{
+	endpoint_v4_add_entry(BACKEND_IP, 0, 0, 0, 0, 0, NULL, NULL);
+	/* Jump into the entrypoint */
+	tail_call_static(ctx, entry_call_map, FROM_OVERLAY);
+	/* Fail if we didn't jump */
+	return TEST_ERROR;
+}
+
+CHECK("tc", "tc_geneve_dsr_v4_legacy")
+int tc_geneve_dsr_v4_legacy_check(struct __ctx_buff *ctx)
+{
+	void *data, *data_end;
+	__u32 *status_code;
+	struct ct_entry *ct_entry;
+	struct ipv4_nat_entry *nat_entry;
+	struct ipv4_ct_tuple expected_tuple_for_ct = {
+		.saddr   = BACKEND_IP,
+		.daddr   = CLIENT_IP,
+		.sport   = CLIENT_PORT,
+		.dport   = BACKEND_PORT,
+		.nexthdr = IPPROTO_TCP,
+		.flags   = TUPLE_F_OUT,
+	};
+
+	struct ipv4_ct_tuple expected_tuple_for_nat = {
+		.saddr   = BACKEND_IP,
+		.daddr   = CLIENT_IP,
+		.sport   = BACKEND_PORT,
+		.dport   = CLIENT_PORT,
+		.nexthdr = IPPROTO_TCP,
+		.flags   = TUPLE_F_OUT,
+	};
+
+	test_init();
+
+	data = (void *)(long)ctx_data(ctx);
+	data_end = (void *)(long)ctx->data_end;
+
+	if (data + sizeof(__u32) > data_end)
+		test_fatal("status code out of bounds");
+
+	/* The packet must be passed to kernel-stack */
+	status_code = data;
+	assert(*status_code == CTX_ACT_OK);
+
+	/* Verify that the datapath inserted the conntrack entry */
+	ct_entry = map_lookup_elem(&cilium_ct4_global, &expected_tuple_for_ct);
+	if (!ct_entry)
+		test_fatal("No entry in conntrack map");
+
+	/* Verify that the datapath inserted the SNAT entry */
+	nat_entry = snat_v4_lookup(&expected_tuple_for_nat);
+	if (!nat_entry)
+		test_fatal("No entry in SNAT map");
+
+	test_finish();
+}
diff --git a/bpf/tests/tc_geneve_dsr_v6_legacy.c b/bpf/tests/tc_geneve_dsr_v6_legacy.c
new file mode 100644
index 0000000000..24ec6daa5f
--- /dev/null
+++ b/bpf/tests/tc_geneve_dsr_v6_legacy.c
@@ -0,0 +1,161 @@
+// SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause)
+/* Copyright Authors of Cilium */
+
+#include <bpf/ctx/skb.h>
+#include "common.h"
+#include "pktgen.h"
+
+/* Enable code paths under test */
+#define ENABLE_IPV6
+
+#define ENABLE_NODEPORT 1
+#define ENABLE_DSR 1
+#define DSR_ENCAP_IPIP 2
+#define DSR_ENCAP_GENEVE 3
+#define DSR_ENCAP_MODE DSR_ENCAP_GENEVE
+
+#define TUNNEL_PROTOCOL TUNNEL_PROTOCOL_GENEVE
+#define ENCAP_IFINDEX 42
+#define TUNNEL_MODE
+
+#define CLIENT_IP { .addr = { 0x1, 0x0, 0x0, 0x0, 0x0, 0x0 } }
+#define CLIENT_PORT __bpf_htons(111)
+
+#define BACKEND_IP		{ .addr = { 0x3, 0x0, 0x0, 0x0, 0x0, 0x0 } }
+#define BACKEND_PORT		__bpf_htons(8080)
+
+#define NODE_IP v6_node_one
+
+static volatile const __u8 *client_mac = mac_one;
+static volatile const __u8 *server_mac = mac_two;
+
+#define skb_get_tunnel_key mock_skb_get_tunnel_key
+int mock_skb_get_tunnel_key(__maybe_unused struct __sk_buff *skb,
+			    __maybe_unused  struct bpf_tunnel_key *to,
+			    __maybe_unused __u32 size,
+			    __maybe_unused __u32 flags)
+{
+	return 0;
+}
+
+#define skb_get_tunnel_opt mock_skb_get_tunnel_opt
+int mock_skb_get_tunnel_opt(__maybe_unused struct __sk_buff *skb,
+			    void *opt, __u32 size)
+{
+	struct geneve_dsr_opt6 *gopt = opt;
+
+	gopt->hdr.opt_class = bpf_htons(DSR_GENEVE_OPT_CLASS);
+	gopt->hdr.type = DSR_GENEVE_OPT_TYPE;
+	return size;
+}
+
+#include "bpf_overlay.c"
+
+#include "lib/endpoint.h"
+
+#define FROM_OVERLAY 0
+
+struct {
+	__uint(type, BPF_MAP_TYPE_PROG_ARRAY);
+	__uint(key_size, sizeof(__u32));
+	__uint(max_entries, 1);
+	__array(values, int());
+} entry_call_map __section(".maps") = {
+	.values = {
+		[FROM_OVERLAY] = &cil_from_overlay,
+	},
+};
+
+PKTGEN("tc", "tc_geneve_dsr_v6_legacy")
+int tc_geneve_dsr_v6_legacy_pktgen(struct __ctx_buff *ctx)
+{
+	struct pktgen builder;
+	struct tcphdr *l4;
+	void *data;
+	union v6addr client_ip = CLIENT_IP;
+	union v6addr backend_ip = BACKEND_IP;
+
+	/* Init packet builder */
+	pktgen__init(&builder, ctx);
+
+	l4 = pktgen__push_ipv6_tcp_packet(&builder,
+					  (__u8 *)client_mac, (__u8 *)server_mac,
+					  (__u8 *)&client_ip, (__u8 *)&backend_ip,
+					  CLIENT_PORT, BACKEND_PORT);
+	if (!l4)
+		return TEST_ERROR;
+
+	data = pktgen__push_data(&builder, default_data, sizeof(default_data));
+	if (!data)
+		return TEST_ERROR;
+
+	/* Calc lengths, set protocol fields and calc checksums */
+	pktgen__finish(&builder);
+
+	return 0;
+}
+
+SETUP("tc", "tc_geneve_dsr_v6_legacy")
+int tc_geneve_dsr_v6_legacy_setup(struct __ctx_buff *ctx)
+{
+	union v6addr backend_ip = BACKEND_IP;
+
+	endpoint_v6_add_entry(&backend_ip, 0, 0, 0, 0, NULL, NULL);
+	/* Jump into the entrypoint */
+	tail_call_static(ctx, entry_call_map, FROM_OVERLAY);
+	/* Fail if we didn't jump */
+	return TEST_ERROR;
+}
+
+CHECK("tc", "tc_geneve_dsr_v6_legacy")
+int tc_geneve_dsr_v6_legacy_check(struct __ctx_buff *ctx)
+{
+	void *data, *data_end;
+	__u32 *status_code;
+	struct ct_entry *ct_entry;
+	struct ipv6_nat_entry *nat_entry;
+
+	union v6addr backend_ip = BACKEND_IP;
+	union v6addr client_ip  = CLIENT_IP;
+	struct ipv6_ct_tuple expected_tuple_for_ct = {
+		.saddr   = backend_ip,
+		.daddr   = client_ip,
+		.sport   = CLIENT_PORT,
+		.dport   = BACKEND_PORT,
+		.nexthdr = IPPROTO_TCP,
+		.flags   = TUPLE_F_OUT,
+	};
+
+	struct ipv6_ct_tuple expected_tuple_for_nat = {
+		.saddr   = backend_ip,
+		.daddr   = client_ip,
+		.sport   = BACKEND_PORT,
+		.dport   = CLIENT_PORT,
+		.nexthdr = IPPROTO_TCP,
+		.flags   = TUPLE_F_OUT,
+	};
+
+	test_init();
+
+	data      = (void *)(long)ctx_data(ctx);
+	data_end  = (void *)(long)ctx->data_end;
+
+	if (data + sizeof(__u32) > data_end)
+		test_fatal("status code out of bounds");
+
+	/* Packet must be passed to the kernel stack */
+	status_code = data;
+	assert(*status_code == CTX_ACT_OK);
+
+	/* Verify that the datapath inserted the conntrack entry */
+	ct_entry = map_lookup_elem(&cilium_ct6_global, &expected_tuple_for_ct);
+	if (!ct_entry)
+		test_fatal("No entry in conntrack map");
+
+	/* Verify that the datapath inserted the SNAT entry */
+	nat_entry = snat_v6_lookup(&expected_tuple_for_nat);
+	if (!nat_entry)
+		test_fatal("No entry in NAT map");
+
+	test_finish();
+}
diff --git a/bpf/tests/tc_lxc_policy_drop.c b/bpf/tests/tc_lxc_policy_drop.c
index d6c348c8e8..8e52a16466 100644
--- a/bpf/tests/tc_lxc_policy_drop.c
+++ b/bpf/tests/tc_lxc_policy_drop.c
@@ -21,7 +21,7 @@ static volatile const __u8 *server_mac = mac_two;
 
 #include "bpf_lxc.c"
 
-ASSIGN_CONFIG(__u32, endpoint_ipv4, v4_pod_one)
+ASSIGN_CONFIG(union v4addr, endpoint_ipv4, { .be32 = v4_pod_one})
 
 #include "lib/policy.h"
 
diff --git a/bpf/tests/tc_nodeport_lb4_no_backend.c b/bpf/tests/tc_nodeport_lb4_no_backend.c
index 75eb0be5cb..16008ac307 100644
--- a/bpf/tests/tc_nodeport_lb4_no_backend.c
+++ b/bpf/tests/tc_nodeport_lb4_no_backend.c
@@ -26,7 +26,7 @@ static volatile const __u8 lb_mac[ETH_ALEN] = { 0xce, 0x72, 0xa7, 0x03, 0x88, 0x
 
 #include <bpf_host.c>
 
-ASSIGN_CONFIG(__u32, nat_ipv4_masquerade, FRONTEND_IP)
+ASSIGN_CONFIG(union v4addr, nat_ipv4_masquerade, { .be32 = FRONTEND_IP})
 
 #include "lib/ipcache.h"
 #include "lib/lb.h"
diff --git a/bpf/tests/tc_nodeport_test.c b/bpf/tests/tc_nodeport_test.c
index e249cfdc26..8c4b7f217e 100644
--- a/bpf/tests/tc_nodeport_test.c
+++ b/bpf/tests/tc_nodeport_test.c
@@ -26,7 +26,8 @@ mock_ctx_redirect_peer(const struct __sk_buff *ctx __maybe_unused, int ifindex _
 #include <bpf_lxc.c>
 
 /* Set the LXC source address to be the address of pod one */
-ASSIGN_CONFIG(__u32, endpoint_ipv4, v4_pod_one)
+ASSIGN_CONFIG(union v4addr, endpoint_ipv4, { .be32 = v4_pod_one})
+ASSIGN_CONFIG(union v4addr, service_loopback_ipv4, { .be32 = v4_svc_loopback })
 
 #include "lib/endpoint.h"
 #include "lib/ipcache.h"
@@ -106,6 +107,10 @@ int hairpin_flow_forward_setup(struct __ctx_buff *ctx)
 
 	endpoint_v4_add_entry(v4_pod_one, 0, 0, 0, 0, 0, NULL, NULL);
 
+	/* Hairpin should over-rule any installed network policy: */
+	policy_add_egress_deny_all_entry();
+	policy_add_ingress_deny_all_entry();
+
 	/* Jump into the entrypoint */
 	tail_call_static(ctx, entry_call_map, 0);
 	/* Fail if we didn't jump */
@@ -138,7 +143,7 @@ int hairpin_flow_forward_check(__maybe_unused const struct __ctx_buff *ctx)
 	if ((void *)l3 + sizeof(struct iphdr) > data_end)
 		test_fatal("l3 out of bounds");
 
-	if (l3->saddr != IPV4_LOOPBACK)
+	if (l3->saddr != CONFIG(service_loopback_ipv4).be32)
 		test_fatal("src IP was not SNAT'ed");
 
 	if (l3->daddr != v4_pod_one)
@@ -182,7 +187,7 @@ int hairpin_flow_forward_check(__maybe_unused const struct __ctx_buff *ctx)
 	/* Match the packet headers: */
 	tuple.flags = TUPLE_F_OUT;
 	tuple.nexthdr = IPPROTO_TCP;
-	tuple.saddr = IPV4_LOOPBACK;
+	tuple.saddr = CONFIG(service_loopback_ipv4).be32;
 	tuple.sport = tcp_src_one;
 	tuple.daddr = v4_pod_one;
 	tuple.dport = tcp_dst_one;
@@ -214,7 +219,7 @@ int hairpin_flow_forward_ingress_pktgen(struct __ctx_buff *ctx)
 
 	l4 = pktgen__push_ipv4_tcp_packet(&builder,
 					  (__u8 *)src, (__u8 *)dst,
-					  IPV4_LOOPBACK, v4_pod_one,
+					  CONFIG(service_loopback_ipv4).be32, v4_pod_one,
 					  tcp_src_one, tcp_dst_one);
 	if (!l4)
 		return TEST_ERROR;
@@ -266,7 +271,7 @@ int hairpin_flow_forward_ingress_check(__maybe_unused const struct __ctx_buff *c
 	if ((void *)l3 + sizeof(struct iphdr) > data_end)
 		test_fatal("l3 out of bounds");
 
-	if (l3->saddr != IPV4_LOOPBACK)
+	if (l3->saddr != CONFIG(service_loopback_ipv4).be32)
 		test_fatal("src IP changed");
 
 	if (l3->daddr != v4_pod_one)
@@ -295,7 +300,7 @@ int hairpin_flow_forward_ingress_check(__maybe_unused const struct __ctx_buff *c
 	/* Match the packet headers: */
 	tuple.flags = TUPLE_F_IN;
 	tuple.nexthdr = IPPROTO_TCP;
-	tuple.saddr = IPV4_LOOPBACK;
+	tuple.saddr = CONFIG(service_loopback_ipv4).be32;
 	tuple.sport = tcp_src_one;
 	tuple.daddr = v4_pod_one;
 	tuple.dport = tcp_dst_one;
@@ -326,7 +331,7 @@ int hairpin_flow_reverse_pktgen(struct __ctx_buff *ctx)
 
 	l4 = pktgen__push_ipv4_tcp_packet(&builder,
 					  (__u8 *)src, (__u8 *)dst,
-					  v4_pod_one, IPV4_LOOPBACK,
+					  v4_pod_one, CONFIG(service_loopback_ipv4).be32,
 					  tcp_dst_one, tcp_src_one);
 	if (!l4)
 		return TEST_ERROR;
@@ -383,7 +388,7 @@ int hairpin_flow_rev_check(__maybe_unused const struct __ctx_buff *ctx)
 	if (l3->saddr != v4_pod_one)
 		test_fatal("src IP changed");
 
-	if (l3->daddr != IPV4_LOOPBACK)
+	if (l3->daddr != CONFIG(service_loopback_ipv4).be32)
 		test_fatal("dest IP changed");
 
 	l4 = (void *)l3 + sizeof(struct iphdr);
@@ -417,7 +422,7 @@ int hairpin_flow_reverse_ingress_pktgen(struct __ctx_buff *ctx)
 
 	l4 = pktgen__push_ipv4_tcp_packet(&builder,
 					  (__u8 *)src, (__u8 *)dst,
-					  v4_pod_one, IPV4_LOOPBACK,
+					  v4_pod_one, CONFIG(service_loopback_ipv4).be32,
 					  tcp_dst_one, tcp_src_one);
 	if (!l4)
 		return TEST_ERROR;
diff --git a/bugtool/cmd/configuration.go b/bugtool/cmd/configuration.go
index 4e2ffb26ae..45ca4621f7 100644
--- a/bugtool/cmd/configuration.go
+++ b/bugtool/cmd/configuration.go
@@ -108,6 +108,7 @@ var bpfMapsPath = []string{
 	"tc/globals/cilium_throttle",
 	"tc/globals/cilium_encrypt_state",
 	"tc/globals/cilium_egress_gw_policy_v4",
+	"tc/globals/cilium_egress_gw_policy_v6",
 	"tc/globals/cilium_srv6_vrf_v4",
 	"tc/globals/cilium_srv6_vrf_v6",
 	"tc/globals/cilium_srv6_policy_v4",
diff --git a/cilium-cli/connectivity/check/context.go b/cilium-cli/connectivity/check/context.go
index 4750b1e8fc..b0b585f421 100644
--- a/cilium-cli/connectivity/check/context.go
+++ b/cilium-cli/connectivity/check/context.go
@@ -1324,3 +1324,12 @@ func (ct *ConnectivityTest) IsSocketLBFull() bool {
 	}
 	return false
 }
+
+func (ct *ConnectivityTest) GetPodHostIPByFamily(pod Pod, ipFam features.IPFamily) (string, error) {
+	for _, addr := range pod.Pod.Status.HostIPs {
+		if features.GetIPFamily(addr.IP) == ipFam {
+			return addr.IP, nil
+		}
+	}
+	return "", fmt.Errorf("pod doesn't have HostIP of family %s", ipFam)
+}
diff --git a/cilium-cli/connectivity/tests/host.go b/cilium-cli/connectivity/tests/host.go
index d6501cb87a..bb743bcf31 100644
--- a/cilium-cli/connectivity/tests/host.go
+++ b/cilium-cli/connectivity/tests/host.go
@@ -144,18 +144,24 @@ func (s *podToHostPort) Run(ctx context.Context, t *check.Test) {
 
 	for _, client := range ct.ClientPods() {
 		for _, echo := range ct.EchoPods() {
-			baseURL := fmt.Sprintf("%s://%s:%d%s", echo.Scheme(), echo.Pod.Status.HostIP, ct.Params().EchoServerHostPort, echo.Path())
-			ep := check.HTTPEndpoint(echo.Name(), baseURL)
-			t.NewAction(s, fmt.Sprintf("curl-%d", i), &client, ep, features.IPFamilyAny).Run(func(a *check.Action) {
-				a.ExecInPod(ctx, a.CurlCommand(ep))
-
-				a.ValidateFlows(ctx, client, a.GetEgressRequirements(check.FlowParameters{
-					// Because the HostPort request is NATed, we might only
-					// observe flows after DNAT has been applied (e.g. by
-					// HostReachableServices),
-					AltDstIP:   echo.Address(features.IPFamilyAny),
-					AltDstPort: echo.Port(),
-				}))
+			t.ForEachIPFamily(func(ipFam features.IPFamily) {
+				hostIP, err := ct.GetPodHostIPByFamily(echo, ipFam)
+				if err != nil {
+					return
+				}
+				baseURL := fmt.Sprintf("%s://%s:%d%s", echo.Scheme(), hostIP, ct.Params().EchoServerHostPort, echo.Path())
+				ep := check.HTTPEndpoint(echo.Name(), baseURL)
+				t.NewAction(s, fmt.Sprintf("curl-%s-%d", ipFam, i), &client, ep, ipFam).Run(func(a *check.Action) {
+					a.ExecInPod(ctx, a.CurlCommand(ep))
+
+					a.ValidateFlows(ctx, client, a.GetEgressRequirements(check.FlowParameters{
+						// Because the HostPort request is NATed, we might only
+						// observe flows after DNAT has been applied (e.g. by
+						// HostReachableServices),
+						AltDstIP:   echo.Address(ipFam),
+						AltDstPort: echo.Port(),
+					}))
+				})
 			})
 
 			i++
diff --git a/cilium-dbg/cmd/encrypt_flush.go b/cilium-dbg/cmd/encrypt_flush.go
index 1abe4fed01..46a44e38f4 100644
--- a/cilium-dbg/cmd/encrypt_flush.go
+++ b/cilium-dbg/cmd/encrypt_flush.go
@@ -190,10 +190,10 @@ func filterXFRMs(policies []netlink.XfrmPolicy, states []netlink.XfrmState,
 
 func filterStaleXFRMs(logger *slog.Logger, policies []netlink.XfrmPolicy, states []netlink.XfrmState) ([]netlink.XfrmPolicy, []netlink.XfrmState) {
 	bpfNodeIDs := map[uint16]bool{}
-	parse := func(key *nodemap.NodeKey, val *nodemap.NodeValue) {
+	parse := func(key *nodemap.NodeKey, val *nodemap.NodeValueV2) {
 		bpfNodeIDs[val.NodeID] = true
 	}
-	nodeMap, err := nodemap.LoadNodeMap(logger)
+	nodeMap, err := nodemap.LoadNodeMapV2(logger)
 	if err != nil {
 		Fatalf("Cannot load node bpf map: %s", err)
 	}
diff --git a/clustermesh-apiserver/clustermesh/users_mgmt.go b/clustermesh-apiserver/clustermesh/users_mgmt.go
index 563487fcc7..3252614ab1 100644
--- a/clustermesh-apiserver/clustermesh/users_mgmt.go
+++ b/clustermesh-apiserver/clustermesh/users_mgmt.go
@@ -105,7 +105,7 @@ func (us *usersManager) Start(cell.HookContext) error {
 		logfields.Path, us.ClusterUsersConfigPath,
 	)
 
-	configWatcher, err := fswatcher.New([]string{us.ClusterUsersConfigPath})
+	configWatcher, err := fswatcher.New(us.logger, []string{us.ClusterUsersConfigPath})
 	if err != nil {
 		us.logger.Error("Unable to setup config watcher", logfields.Error, err)
 		return fmt.Errorf("unable to setup config watcher: %w", err)
diff --git a/contrib/coccinelle/encrypt_agg.cocci b/contrib/coccinelle/encrypt_agg.cocci
index 9b4ed90676..2fb9c9bebc 100644
--- a/contrib/coccinelle/encrypt_agg.cocci
+++ b/contrib/coccinelle/encrypt_agg.cocci
@@ -23,8 +23,9 @@ position p;
 (
   send_trace_notify@f(e1, e2, e3, e4, e5, e6,
   TRACE_REASON_ENCRYPTED,
-- m@p);
-+ 0);
+- m@p,
++ 0,
+  e7);
 |
   \(send_trace_notify4@f\|send_trace_notify6@f\)(e1, e2, e3, e4, e5, e6, e7,
   TRACE_REASON_ENCRYPTED,
diff --git a/contrib/coccinelle/zero_trace_reason.cocci b/contrib/coccinelle/zero_trace_reason.cocci
index b5550a835e..6656ef3321 100644
--- a/contrib/coccinelle/zero_trace_reason.cocci
+++ b/contrib/coccinelle/zero_trace_reason.cocci
@@ -23,7 +23,7 @@ position p;
   send_trace_notify@f(e1, e2, e3, e4, e5, e6,
 - 0@p,
 + TRACE_REASON_UNKNOWN,
-  e7);
+  e7, e8);
 |
   \(send_trace_notify4@f\|send_trace_notify6@f\)(e1, e2, e3, e4, e5, e6, e7,
 - 0@p,
diff --git a/contrib/scripts/kind.sh b/contrib/scripts/kind.sh
index 5b56c09f59..07e398e06a 100755
--- a/contrib/scripts/kind.sh
+++ b/contrib/scripts/kind.sh
@@ -236,6 +236,14 @@ kubeadmConfigPatches:
     apiServer:
       extraArgs:
         "v": "3"
+    controllerManager:
+      extraArgs:
+        authorization-always-allow-paths: /healthz,/readyz,/livez,/metrics
+        bind-address: 0.0.0.0
+    scheduler:
+      extraArgs:
+        authorization-always-allow-paths: /healthz,/readyz,/livez,/metrics
+        bind-address: 0.0.0.0
 EOF
 
 if [ "${secondary_network_flag}" = true ]; then
diff --git a/contrib/testing/kind-common.yaml b/contrib/testing/kind-common.yaml
index 1be34fdebd..1da6455efd 100644
--- a/contrib/testing/kind-common.yaml
+++ b/contrib/testing/kind-common.yaml
@@ -1,5 +1,9 @@
 debug:
   enabled: true
+pprof:
+  enabled: true
+prometheus:
+  enabled: true
 operator:
   nodeSelector:
     kubernetes.io/os: linux
diff --git a/daemon/cmd/cells.go b/daemon/cmd/cells.go
index b28ab3d857..1bb905593c 100644
--- a/daemon/cmd/cells.go
+++ b/daemon/cmd/cells.go
@@ -260,6 +260,7 @@ var (
 
 		// ClusterMesh is the Cilium's multicluster implementation.
 		cell.Config(cmtypes.DefaultClusterInfo),
+		cell.Config(cmtypes.DefaultPolicyConfig),
 		clustermesh.Cell,
 
 		// L2announcer resolves l2announcement policies, services, node labels and devices into a list of IPs+netdevs
diff --git a/daemon/cmd/daemon_main.go b/daemon/cmd/daemon_main.go
index 743bc81c82..738c4ea73c 100644
--- a/daemon/cmd/daemon_main.go
+++ b/daemon/cmd/daemon_main.go
@@ -237,10 +237,6 @@ func InitGlobalFlags(logger *slog.Logger, cmd *cobra.Command, vp *viper.Viper) {
 	flags.StringSlice(option.DebugVerbose, []string{}, "List of enabled verbose debug groups")
 	option.BindEnv(vp, option.DebugVerbose)
 
-	flags.Bool(option.EnableRuntimeDeviceDetection, true, "Enable runtime device detection and datapath reconfiguration (experimental)")
-	option.BindEnv(vp, option.EnableRuntimeDeviceDetection)
-	flags.MarkDeprecated(option.EnableRuntimeDeviceDetection, "Runtime device detection and datapath reconfiguration is now the default and only mode of operation")
-
 	flags.String(option.DatapathMode, defaults.DatapathMode,
 		fmt.Sprintf("Datapath mode name (%s, %s, %s)",
 			datapathOption.DatapathModeVeth, datapathOption.DatapathModeNetkit, datapathOption.DatapathModeNetkitL2))
@@ -620,8 +616,9 @@ func InitGlobalFlags(logger *slog.Logger, cmd *cobra.Command, vp *viper.Viper) {
 	flags.Bool(option.LogSystemLoadConfigName, false, "Enable periodic logging of system load")
 	option.BindEnv(vp, option.LogSystemLoadConfigName)
 
-	flags.String(option.LoopbackIPv4, defaults.LoopbackIPv4, "IPv4 address for service loopback SNAT")
-	option.BindEnv(vp, option.LoopbackIPv4)
+	flags.String(option.ServiceLoopbackIPv4, defaults.ServiceLoopbackIPv4, "IPv4 source address to use for SNAT "+
+		"when a Pod talks to itself over a Service.")
+	option.BindEnv(vp, option.ServiceLoopbackIPv4)
 
 	flags.Bool(option.EnableIPv4Masquerade, true, "Masquerade IPv4 traffic from endpoints leaving the host")
 	option.BindEnv(vp, option.EnableIPv4Masquerade)
@@ -1074,7 +1071,7 @@ func initEnv(logger *slog.Logger, vp *viper.Viper) {
 	logger.Info(fmt.Sprintf("Cilium %s", version.Version))
 
 	if option.Config.LogSystemLoadConfig {
-		loadinfo.StartBackgroundLogger()
+		loadinfo.StartBackgroundLogger(logging.DefaultSlogLogger)
 	}
 
 	if option.Config.PreAllocateMaps {
@@ -1191,7 +1188,7 @@ func initEnv(logger *slog.Logger, vp *viper.Viper) {
 	// useful if the daemon is being round inside a namespace and the
 	// BPF filesystem is mapped into the slave namespace.
 	bpf.CheckOrMountFS(logging.DefaultSlogLogger, option.Config.BPFRoot)
-	cgroups.CheckOrMountCgrpFS(option.Config.CGroupRoot)
+	cgroups.CheckOrMountCgrpFS(logging.DefaultSlogLogger, option.Config.CGroupRoot)
 
 	option.Config.Opts.SetBool(option.Debug, debugDatapath)
 	option.Config.Opts.SetBool(option.DebugLB, debugDatapath)
@@ -1693,7 +1690,7 @@ func startDaemon(d *Daemon, restoredEndpoints *endpointRestoreState, cleaner *da
 	}
 
 	if option.Config.EnableIPMasqAgent {
-		ipmasqAgent, err := ipmasq.NewIPMasqAgent(d.metricsRegistry, option.Config.IPMasqAgentConfigPath)
+		ipmasqAgent, err := ipmasq.NewIPMasqAgent(logging.DefaultSlogLogger, d.metricsRegistry, option.Config.IPMasqAgentConfigPath)
 		if err != nil {
 			return fmt.Errorf("failed to create ipmasq agent: %w", err)
 		}
diff --git a/daemon/cmd/ipam.go b/daemon/cmd/ipam.go
index ac42bc26ee..8ae792cf9e 100644
--- a/daemon/cmd/ipam.go
+++ b/daemon/cmd/ipam.go
@@ -501,12 +501,12 @@ func (d *Daemon) allocateIPs(ctx context.Context, router restoredIPs) error {
 		}
 
 		// Allocate IPv4 service loopback IP
-		loopbackIPv4 := net.ParseIP(option.Config.LoopbackIPv4)
+		loopbackIPv4 := net.ParseIP(option.Config.ServiceLoopbackIPv4)
 		if loopbackIPv4 == nil {
-			return fmt.Errorf("Invalid IPv4 loopback address %s", option.Config.LoopbackIPv4)
+			return fmt.Errorf("Invalid IPv4 loopback address %s", option.Config.ServiceLoopbackIPv4)
 		}
-		node.SetIPv4Loopback(loopbackIPv4)
-		d.logger.Info(fmt.Sprintf("  Loopback IPv4: %s", node.GetIPv4Loopback(d.logger).String()))
+		node.SetServiceLoopbackIPv4(loopbackIPv4)
+		d.logger.Info(fmt.Sprintf("  Loopback IPv4: %s", node.GetServiceLoopbackIPv4(d.logger).String()))
 
 		d.logger.Info("  Local IPv4 addresses:")
 		for _, addr := range addrs {
diff --git a/daemon/healthz/agenthealth.go b/daemon/healthz/agenthealth.go
index 839b88c5ad..366337741d 100644
--- a/daemon/healthz/agenthealth.go
+++ b/daemon/healthz/agenthealth.go
@@ -80,6 +80,12 @@ func registerAgentHealthHTTPService(params agentHealthParams) error {
 				Addr:    addr,
 				Handler: mux,
 			}
+
+			go func() {
+				<-ctx.Done()
+				srv.Shutdown(context.Background()) // does not use job context, as it has already been closed!
+			}()
+
 			params.Logger.Info("Starting healthz status API server", logfields.Address, addr)
 			if err := srv.Serve(ln); errors.Is(err, http.ErrServerClosed) {
 				params.Logger.Info("healthz status API server shutdown", logfields.Address, addr)
diff --git a/daemon/k8s/script_test.go b/daemon/k8s/script_test.go
index 564d7f1f39..5002098f8d 100644
--- a/daemon/k8s/script_test.go
+++ b/daemon/k8s/script_test.go
@@ -19,6 +19,7 @@ import (
 
 	"github.com/cilium/cilium/pkg/hive"
 	"github.com/cilium/cilium/pkg/k8s/client"
+	nodeTypes "github.com/cilium/cilium/pkg/node/types"
 	"github.com/cilium/cilium/pkg/time"
 )
 
@@ -32,6 +33,7 @@ func TestScript(t *testing.T) {
 	}
 	t.Cleanup(func() { time.Now = now })
 	t.Setenv("TZ", "")
+	nodeTypes.SetName("testnode")
 
 	log := hivetest.Logger(t)
 	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
diff --git a/daemon/k8s/testdata/pod.txtar b/daemon/k8s/testdata/pod.txtar
index 6ba9e2a2d2..f37b0522a1 100644
--- a/daemon/k8s/testdata/pod.txtar
+++ b/daemon/k8s/testdata/pod.txtar
@@ -5,18 +5,7 @@
 # api-server.
 #
 
-# Start and wait for reflector to sync up (List & Watch). This is needed
-# as the fake k8s client is dumb and would miss events between the List
-# and Watch calls.
 hive start
-db/initialized
-
-# We can also use the k8s/wait-watchers to wait for a watcher to appear.
-# This is useful when we don't have a StateDB table and thus cannot use
-# 'db/initialized'.
-# This is only here as an example for other tests. You don't need both
-# this and db/initialized!
-k8s/wait-watchers v1.pods
 
 # At the start the table is empty
 db/cmp k8s-pods empty.table 
@@ -80,7 +69,7 @@ default/nginx
     "name": "nginx",
     "namespace": "default",
     "uid": "d96be1eb-51df-4383-87ab-bc29d5b42933",
-    "resourceVersion": "482057",
+    "resourceVersion": "1",
     "labels": {
       "run": "nginx"
     }
@@ -171,7 +160,7 @@ metadata:
     run: nginx
   name: nginx
   namespace: default
-  resourceVersion: "482057"
+  resourceVersion: "1"
   uid: d96be1eb-51df-4383-87ab-bc29d5b42933
 spec:
   containers:
diff --git a/hubble-relay/cmd/serve/serve.go b/hubble-relay/cmd/serve/serve.go
index a493aaa37a..1bb7852aa7 100644
--- a/hubble-relay/cmd/serve/serve.go
+++ b/hubble-relay/cmd/serve/serve.go
@@ -250,7 +250,7 @@ func runServe(vp *viper.Viper) error {
 	}
 
 	if vp.GetBool(keyPprof) {
-		pprof.Enable(vp.GetString(keyPprofAddress), vp.GetInt(keyPprofPort))
+		pprof.Enable(logger, vp.GetString(keyPprofAddress), vp.GetInt(keyPprofPort))
 	}
 	gopsEnabled := vp.GetBool(keyGops)
 	if gopsEnabled {
diff --git a/images/scripts/update-cilium-envoy-image.sh b/images/scripts/update-cilium-envoy-image.sh
index d104058077..94348087a2 100755
--- a/images/scripts/update-cilium-envoy-image.sh
+++ b/images/scripts/update-cilium-envoy-image.sh
@@ -18,13 +18,15 @@ latest_commit_sha="$(curl -s https://api.github.com/repos/"${github_repo}"/commi
 
 repo="cilium-envoy"
 image="quay.io/cilium/cilium-envoy"
+filter="select(.name | test(\".*-.*-.*\"))"
 if [ "${github_branch}" != "main" ] && ! [[ "${github_branch}" =~ ^v1\.[0-9]+$ ]]; then
     image="quay.io/cilium/cilium-envoy-dev"
     repo="cilium-envoy-dev"
+    filter="select(.name)"
 fi
 
 # Filter all tags that are in the format of .*-.*-.* (e.g. v1.33.2-1742995211-ca0b42f0ecdf835224a8ddfc6fe0442368d4d766)
-tags=$(curl -s "https://quay.io/api/v1/repository/cilium/${repo}/tag/?onlyActiveTags=true&filter_tag_name=like:${latest_commit_sha}" | jq -r '.tags[] | select(.name | test(".*-.*-.*"))')
+tags=$(curl -s "https://quay.io/api/v1/repository/cilium/${repo}/tag/?onlyActiveTags=true&filter_tag_name=like:${latest_commit_sha}" | jq -r ".tags[] | ${filter}")
 image_tag=$(echo "${tags}" | jq -r .name)
 image_sha256=$(echo "${tags}" | jq -r .manifest_digest)
 
diff --git a/images/scripts/update-cilium-runtime-image.sh b/images/scripts/update-cilium-runtime-image.sh
index fc85916d59..c300ac28b3 100755
--- a/images/scripts/update-cilium-runtime-image.sh
+++ b/images/scripts/update-cilium-runtime-image.sh
@@ -19,7 +19,7 @@ image=${1}
 image_dir=${2}
 image="${image%%:*}"
 
-image_tag="$(WITHOUT_SUFFIX=1 "${script_dir}/make-image-tag.sh" images/runtime)"
+image_tag="$(WITHOUT_SUFFIX=1 "${script_dir}/make-image-tag.sh" "${image_dir}")"
 
 image_full="${image}:${image_tag}"
 sha256=$("${script_dir}/get-image-digest.sh" "${image_full}" || echo "")
diff --git a/install/kubernetes/cilium/README.md b/install/kubernetes/cilium/README.md
index 2a17220f78..d2f8fb845e 100644
--- a/install/kubernetes/cilium/README.md
+++ b/install/kubernetes/cilium/README.md
@@ -283,6 +283,7 @@ contributors across the globe, there is almost always someone available to help.
 | clustermesh.enableEndpointSliceSynchronization | bool | `false` | Enable the synchronization of Kubernetes EndpointSlices corresponding to the remote endpoints of appropriately-annotated global services through ClusterMesh |
 | clustermesh.enableMCSAPISupport | bool | `false` | Enable Multi-Cluster Services API support |
 | clustermesh.maxConnectedClusters | int | `255` | The maximum number of clusters to support in a ClusterMesh. This value cannot be changed on running clusters, and all clusters in a ClusterMesh must be configured with the same value. Values > 255 will decrease the maximum allocatable cluster-local identities. Supported values are 255 and 511. |
+| clustermesh.policyDefaultLocalCluster | bool | `false` | Control whether policy rules assume by default the local cluster if not explicitly selected |
 | clustermesh.useAPIServer | bool | `false` | Deploy clustermesh-apiserver for clustermesh |
 | cni.binPath | string | `"/opt/cni/bin"` | Configure the path to the CNI binary directory on the host. |
 | cni.chainingMode | string | `nil` | Configure chaining on top of other CNI plugins. Possible values:  - none  - aws-cni  - flannel  - generic-veth  - portmap |
@@ -341,7 +342,6 @@ contributors across the globe, there is almost always someone available to help.
 | enableLBIPAM | bool | `true` | Enable LoadBalancer IP Address Management |
 | enableMasqueradeRouteSource | bool | `false` | Enables masquerading to the source of the route for traffic leaving the node from endpoints. |
 | enableNonDefaultDenyPolicies | bool | `true` | Enable Non-Default-Deny policies |
-| enableRuntimeDeviceDetection | bool | `true` | Enables experimental support for the detection of new and removed datapath devices. When devices change the eBPF datapath is reloaded and services updated. If "devices" is set then only those devices, or devices matching a wildcard will be considered.  This option has been deprecated and is a no-op. |
 | enableXTSocketFallback | bool | `true` | Enables the fallback compatibility solution for when the xt_socket kernel module is missing and it is needed for the datapath L7 redirection to work properly. See documentation for details on when this can be disabled: https://docs.cilium.io/en/stable/operations/system_requirements/#linux-kernel. |
 | encryption.enabled | bool | `false` | Enable transparent network encryption. |
 | encryption.ipsec.encryptedOverlay | bool | `false` | Enable IPsec encrypted overlay |
@@ -446,7 +446,6 @@ contributors across the globe, there is almost always someone available to help.
 | etcd.enabled | bool | `false` | Enable etcd mode for the agent. |
 | etcd.endpoints | list | `["https://CHANGE-ME:2379"]` | List of etcd endpoints |
 | etcd.ssl | bool | `false` | Enable use of TLS/SSL for connectivity to etcd. |
-| externalIPs.enabled | bool | `false` | Enable ExternalIPs service support. |
 | extraArgs | list | `[]` | Additional agent container arguments. |
 | extraConfig | object | `{}` | extraConfig allows you to specify additional configuration parameters to be included in the cilium-config configmap. |
 | extraContainers | list | `[]` | Additional containers added to the cilium DaemonSet. |
@@ -475,7 +474,6 @@ contributors across the globe, there is almost always someone available to help.
 | healthPort | int | `9879` | TCP port for the agent health API. This is not the port for cilium-health. |
 | hostFirewall | object | `{"enabled":false}` | Configure the host firewall. |
 | hostFirewall.enabled | bool | `false` | Enables the enforcement of host policies in the eBPF datapath. |
-| hostPort.enabled | bool | `false` | Enable hostPort service support. |
 | hubble.annotations | object | `{}` | Annotations to be added to all top-level hubble objects (resources under templates/hubble) |
 | hubble.dropEventEmitter | object | `{"enabled":false,"interval":"2m","reasons":["auth_required","policy_denied"]}` | Emit v1.Events related to pods on detection of packet drops.    This feature is alpha, please provide feedback at https://github.com/cilium/cilium/issues/33975. |
 | hubble.dropEventEmitter.interval | string | `"2m"` | - Minimum time between emitting same events. |
diff --git a/install/kubernetes/cilium/templates/cilium-configmap.yaml b/install/kubernetes/cilium/templates/cilium-configmap.yaml
index a96b0aed79..b7544fbb15 100644
--- a/install/kubernetes/cilium/templates/cilium-configmap.yaml
+++ b/install/kubernetes/cilium/templates/cilium-configmap.yaml
@@ -759,17 +759,13 @@ data:
   devices: {{ join " " .Values.devices | quote }}
 {{- end }}
 
-{{- if .Values.enableRuntimeDeviceDetection }}
-  enable-runtime-device-detection: "true"
-{{- end }}
-
 {{- if .Values.forceDeviceDetection }}
   force-device-detection: "true"
 {{- end }}
 
   kube-proxy-replacement: {{ $kubeProxyReplacement | quote }}
 
-{{- if ne $kubeProxyReplacement "disabled" }}
+{{- if eq $kubeProxyReplacement "true" }}
   kube-proxy-replacement-healthz-bind-address: {{ default "" .Values.kubeProxyReplacementHealthzBindAddr | quote}}
 {{- end }}
 
@@ -788,18 +784,8 @@ data:
 {{- end }}
 {{- end }}
 
-{{- if hasKey .Values "hostPort" }}
-{{- if eq $kubeProxyReplacement "partial" }}
-  enable-host-port: {{ .Values.hostPort.enabled | quote }}
-{{- end }}
-{{- end }}
-{{- if hasKey .Values "externalIPs" }}
-{{- if eq $kubeProxyReplacement "partial" }}
-  enable-external-ips: {{ .Values.externalIPs.enabled | quote }}
-{{- end }}
-{{- end }}
 {{- if hasKey .Values "nodePort" }}
-{{- if or (eq $kubeProxyReplacement "partial") (eq $kubeProxyReplacement "false") }}
+{{- if eq $kubeProxyReplacement "false" }}
   enable-node-port: {{ .Values.nodePort.enabled | quote }}
 {{- end }}
 {{- if hasKey .Values.nodePort "range" }}
@@ -1415,6 +1401,7 @@ data:
 {{- end }}
   clustermesh-enable-endpoint-sync: {{ .Values.clustermesh.enableEndpointSliceSynchronization | quote }}
   clustermesh-enable-mcs-api: {{ .Values.clustermesh.enableMCSAPISupport | quote }}
+  policy-default-local-cluster: {{ .Values.clustermesh.policyDefaultLocalCluster | quote }}
 
   nat-map-stats-entries: {{ .Values.nat.mapStatsEntries | quote }}
   nat-map-stats-interval: {{ .Values.nat.mapStatsInterval | quote }}
diff --git a/install/kubernetes/cilium/values.schema.json b/install/kubernetes/cilium/values.schema.json
index 067b96915f..83a64bd39c 100644
--- a/install/kubernetes/cilium/values.schema.json
+++ b/install/kubernetes/cilium/values.schema.json
@@ -1467,6 +1467,9 @@
         "maxConnectedClusters": {
           "type": "integer"
         },
+        "policyDefaultLocalCluster": {
+          "type": "boolean"
+        },
         "useAPIServer": {
           "type": "boolean"
         }
@@ -1737,9 +1740,6 @@
     "enableNonDefaultDenyPolicies": {
       "type": "boolean"
     },
-    "enableRuntimeDeviceDetection": {
-      "type": "boolean"
-    },
     "enableXTSocketFallback": {
       "type": "boolean"
     },
@@ -2433,14 +2433,6 @@
       },
       "type": "object"
     },
-    "externalIPs": {
-      "properties": {
-        "enabled": {
-          "type": "boolean"
-        }
-      },
-      "type": "object"
-    },
     "extraArgs": {
       "items": {},
       "type": "array"
@@ -2564,14 +2556,6 @@
       },
       "type": "object"
     },
-    "hostPort": {
-      "properties": {
-        "enabled": {
-          "type": "boolean"
-        }
-      },
-      "type": "object"
-    },
     "hubble": {
       "properties": {
         "annotations": {
diff --git a/install/kubernetes/cilium/values.yaml b/install/kubernetes/cilium/values.yaml
index 1348129769..067db0011a 100644
--- a/install/kubernetes/cilium/values.yaml
+++ b/install/kubernetes/cilium/values.yaml
@@ -838,13 +838,6 @@ daemon:
 # a non-local route. This should be used only when autodetection is not suitable.
 # devices: ""
 
-# -- Enables experimental support for the detection of new and removed datapath
-# devices. When devices change the eBPF datapath is reloaded and services updated.
-# If "devices" is set then only those devices, or devices matching a wildcard will
-# be considered.
-#
-# This option has been deprecated and is a no-op.
-enableRuntimeDeviceDetection: true
 # -- Forces the auto-detection of devices, even if specific devices are explicitly listed
 forceDeviceDetection: false
 # -- Chains to ignore when installing feeder rules.
@@ -1131,9 +1124,6 @@ eni:
   # -- Filter via AWS EC2 Instance tags (k=v) which will dictate which AWS EC2 Instances
   # are going to be used to create new ENIs
   instanceTagsFilter: []
-externalIPs:
-  # -- Enable ExternalIPs service support.
-  enabled: false
 # fragmentTracking enables IPv4 fragment tracking support in the datapath.
 # fragmentTracking: true
 gke:
@@ -1149,9 +1139,6 @@ healthCheckICMPFailureThreshold: 3
 hostFirewall:
   # -- Enables the enforcement of host policies in the eBPF datapath.
   enabled: false
-hostPort:
-  # -- Enable hostPort service support.
-  enabled: false
 # -- Configure socket LB
 socketLB:
   # -- Enable socket LB
@@ -3263,6 +3250,8 @@ clustermesh:
   enableEndpointSliceSynchronization: false
   # -- Enable Multi-Cluster Services API support
   enableMCSAPISupport: false
+  # -- Control whether policy rules assume by default the local cluster if not explicitly selected
+  policyDefaultLocalCluster: false
   # -- Annotations to be added to all top-level clustermesh objects (resources under templates/clustermesh-apiserver and templates/clustermesh-config)
   annotations: {}
   # -- Clustermesh explicit configuration.
diff --git a/install/kubernetes/cilium/values.yaml.tmpl b/install/kubernetes/cilium/values.yaml.tmpl
index c75a2980d2..491691fd4d 100644
--- a/install/kubernetes/cilium/values.yaml.tmpl
+++ b/install/kubernetes/cilium/values.yaml.tmpl
@@ -841,14 +841,6 @@ daemon:
 # a non-local route. This should be used only when autodetection is not suitable.
 # devices: ""
 
-# -- Enables experimental support for the detection of new and removed datapath
-# devices. When devices change the eBPF datapath is reloaded and services updated.
-# If "devices" is set then only those devices, or devices matching a wildcard will
-# be considered.
-#
-# This option has been deprecated and is a no-op.
-enableRuntimeDeviceDetection: true
-
 # -- Forces the auto-detection of devices, even if specific devices are explicitly listed
 forceDeviceDetection: false
 
@@ -1143,9 +1135,6 @@ eni:
   # -- Filter via AWS EC2 Instance tags (k=v) which will dictate which AWS EC2 Instances
   # are going to be used to create new ENIs
   instanceTagsFilter: []
-externalIPs:
-  # -- Enable ExternalIPs service support.
-  enabled: false
 # fragmentTracking enables IPv4 fragment tracking support in the datapath.
 # fragmentTracking: true
 gke:
@@ -1161,9 +1150,6 @@ healthCheckICMPFailureThreshold: 3
 hostFirewall:
   # -- Enables the enforcement of host policies in the eBPF datapath.
   enabled: false
-hostPort:
-  # -- Enable hostPort service support.
-  enabled: false
 # -- Configure socket LB
 socketLB:
   # -- Enable socket LB
@@ -3285,6 +3271,8 @@ clustermesh:
   enableEndpointSliceSynchronization: false
   # -- Enable Multi-Cluster Services API support
   enableMCSAPISupport: false
+  # -- Control whether policy rules assume by default the local cluster if not explicitly selected
+  policyDefaultLocalCluster: false
 
   # -- Annotations to be added to all top-level clustermesh objects (resources under templates/clustermesh-apiserver and templates/clustermesh-config)
   annotations: {}
diff --git a/operator/cmd/ccnp_event.go b/operator/cmd/ccnp_event.go
index 6d5334bf88..8d6ffcdd6e 100644
--- a/operator/cmd/ccnp_event.go
+++ b/operator/cmd/ccnp_event.go
@@ -32,8 +32,8 @@ func k8sEventMetric(scope, action string) {
 // enableCCNPWatcher is similar to enableCNPWatcher but handles the watch events for
 // clusterwide policies. Since, internally Clusterwide policies are implemented
 // using CiliumNetworkPolicy itself, the entire implementation uses the methods
-// associcated with CiliumNetworkPolicy.
-func enableCCNPWatcher(ctx context.Context, logger *slog.Logger, wg *sync.WaitGroup, clientset k8sClient.Clientset) {
+// associated with CiliumNetworkPolicy.
+func enableCCNPWatcher(ctx context.Context, logger *slog.Logger, wg *sync.WaitGroup, clientset k8sClient.Clientset, clusterName string) {
 	logger.Info("Starting CCNP derivative handler")
 
 	ccnpStore := cache.NewStore(cache.DeletionHandlingMetaNamespaceKeyFunc)
@@ -51,7 +51,7 @@ func enableCCNPWatcher(ctx context.Context, logger *slog.Logger, wg *sync.WaitGr
 					// See https://github.com/cilium/cilium/blob/27fee207f5422c95479422162e9ea0d2f2b6c770/pkg/policy/api/ingress.go#L112-L134
 					cnpCpy := cnp.DeepCopy()
 
-					groups.AddDerivativePolicyIfNeeded(logger, clientset, cnpCpy.CiliumNetworkPolicy, true)
+					groups.AddDerivativePolicyIfNeeded(logger, clientset, clusterName, cnpCpy.CiliumNetworkPolicy, true)
 				}
 			},
 			UpdateFunc: func(oldObj, newObj any) {
@@ -68,7 +68,7 @@ func enableCCNPWatcher(ctx context.Context, logger *slog.Logger, wg *sync.WaitGr
 						newCNPCpy := newCNP.DeepCopy()
 						oldCNPCpy := oldCNP.DeepCopy()
 
-						groups.UpdateDerivativePolicyIfNeeded(logger, clientset, newCNPCpy.CiliumNetworkPolicy, oldCNPCpy.CiliumNetworkPolicy, true)
+						groups.UpdateDerivativePolicyIfNeeded(logger, clientset, clusterName, newCNPCpy.CiliumNetworkPolicy, oldCNPCpy.CiliumNetworkPolicy, true)
 					}
 				}
 			},
@@ -100,7 +100,7 @@ func enableCCNPWatcher(ctx context.Context, logger *slog.Logger, wg *sync.WaitGr
 		controller.ControllerParams{
 			Group: ccnpToGroupsControllerGroup,
 			DoFunc: func(ctx context.Context) error {
-				groups.UpdateCNPInformation(logger, clientset)
+				groups.UpdateCNPInformation(logger, clientset, clusterName)
 				return nil
 			},
 			RunInterval: 5 * time.Minute,
diff --git a/operator/cmd/cnp_event.go b/operator/cmd/cnp_event.go
index b0a7f2c8f3..b353bc75d7 100644
--- a/operator/cmd/cnp_event.go
+++ b/operator/cmd/cnp_event.go
@@ -35,7 +35,7 @@ func init() {
 
 // enableCNPWatcher waits for the CiliumNetworkPolicy CRD availability and then
 // garbage collects stale CiliumNetworkPolicy status field entries.
-func enableCNPWatcher(ctx context.Context, logger *slog.Logger, wg *sync.WaitGroup, clientset k8sClient.Clientset) {
+func enableCNPWatcher(ctx context.Context, logger *slog.Logger, wg *sync.WaitGroup, clientset k8sClient.Clientset, clusterName string) {
 	logger.Info("Starting CNP derivative handler")
 	cnpStore := cache.NewStore(cache.DeletionHandlingMetaNamespaceKeyFunc)
 
@@ -52,7 +52,7 @@ func enableCNPWatcher(ctx context.Context, logger *slog.Logger, wg *sync.WaitGro
 					// See https://github.com/cilium/cilium/blob/27fee207f5422c95479422162e9ea0d2f2b6c770/pkg/policy/api/ingress.go#L112-L134
 					cnpCpy := cnp.DeepCopy()
 
-					groups.AddDerivativePolicyIfNeeded(logger, clientset, cnpCpy.CiliumNetworkPolicy, false)
+					groups.AddDerivativePolicyIfNeeded(logger, clientset, clusterName, cnpCpy.CiliumNetworkPolicy, false)
 				}
 			},
 			UpdateFunc: func(oldObj, newObj any) {
@@ -69,7 +69,7 @@ func enableCNPWatcher(ctx context.Context, logger *slog.Logger, wg *sync.WaitGro
 						newCNPCpy := newCNP.DeepCopy()
 						oldCNPCpy := oldCNP.DeepCopy()
 
-						groups.UpdateDerivativePolicyIfNeeded(logger, clientset, newCNPCpy.CiliumNetworkPolicy, oldCNPCpy.CiliumNetworkPolicy, false)
+						groups.UpdateDerivativePolicyIfNeeded(logger, clientset, clusterName, newCNPCpy.CiliumNetworkPolicy, oldCNPCpy.CiliumNetworkPolicy, false)
 					}
 				}
 			},
@@ -101,7 +101,7 @@ func enableCNPWatcher(ctx context.Context, logger *slog.Logger, wg *sync.WaitGro
 		controller.ControllerParams{
 			Group: cnpToGroupsControllerGroup,
 			DoFunc: func(ctx context.Context) error {
-				groups.UpdateCNPInformation(logger, clientset)
+				groups.UpdateCNPInformation(logger, clientset, clusterName)
 				return nil
 			},
 			RunInterval: 5 * time.Minute,
diff --git a/operator/cmd/root.go b/operator/cmd/root.go
index 8cf50b6382..353fb87a4e 100644
--- a/operator/cmd/root.go
+++ b/operator/cmd/root.go
@@ -134,6 +134,7 @@ var (
 		"Operator Control Plane",
 
 		cell.Config(cmtypes.DefaultClusterInfo),
+		cell.Config(cmtypes.DefaultPolicyConfig),
 		cell.Invoke(cmtypes.ClusterInfo.InitClusterIDMax),
 		cell.Invoke(cmtypes.ClusterInfo.Validate),
 
@@ -508,18 +509,19 @@ var legacyCell = cell.Module(
 	metrics.Metric(NewUnmanagedPodsMetric),
 )
 
-func registerLegacyOnLeader(lc cell.Lifecycle, clientset k8sClient.Clientset, resources operatorK8s.Resources, factory store.Factory, svcResolver *dial.ServiceResolver, cfgMCSAPI cmoperator.MCSAPIConfig, metrics *UnmanagedPodsMetric, logger *slog.Logger) {
+func registerLegacyOnLeader(lc cell.Lifecycle, clientset k8sClient.Clientset, resources operatorK8s.Resources, factory store.Factory, svcResolver *dial.ServiceResolver, cfgMCSAPI cmoperator.MCSAPIConfig, cfgClusterMeshPolicy cmtypes.PolicyConfig, metrics *UnmanagedPodsMetric, logger *slog.Logger) {
 	ctx, cancel := context.WithCancel(context.Background())
 	legacy := &legacyOnLeader{
-		ctx:          ctx,
-		cancel:       cancel,
-		clientset:    clientset,
-		resources:    resources,
-		storeFactory: factory,
-		svcResolver:  svcResolver,
-		cfgMCSAPI:    cfgMCSAPI,
-		metrics:      metrics,
-		logger:       logger,
+		ctx:                  ctx,
+		cancel:               cancel,
+		clientset:            clientset,
+		resources:            resources,
+		storeFactory:         factory,
+		svcResolver:          svcResolver,
+		cfgMCSAPI:            cfgMCSAPI,
+		cfgClusterMeshPolicy: cfgClusterMeshPolicy,
+		metrics:              metrics,
+		logger:               logger,
 	}
 	lc.Append(cell.Hook{
 		OnStart: legacy.onStart,
@@ -528,15 +530,16 @@ func registerLegacyOnLeader(lc cell.Lifecycle, clientset k8sClient.Clientset, re
 }
 
 type legacyOnLeader struct {
-	ctx          context.Context
-	cancel       context.CancelFunc
-	clientset    k8sClient.Clientset
-	wg           sync.WaitGroup
-	resources    operatorK8s.Resources
-	storeFactory store.Factory
-	svcResolver  *dial.ServiceResolver
-	cfgMCSAPI    cmoperator.MCSAPIConfig
-	metrics      *UnmanagedPodsMetric
+	ctx                  context.Context
+	cancel               context.CancelFunc
+	clientset            k8sClient.Clientset
+	wg                   sync.WaitGroup
+	resources            operatorK8s.Resources
+	storeFactory         store.Factory
+	svcResolver          *dial.ServiceResolver
+	cfgMCSAPI            cmoperator.MCSAPIConfig
+	cfgClusterMeshPolicy cmtypes.PolicyConfig
+	metrics              *UnmanagedPodsMetric
 
 	logger *slog.Logger
 }
@@ -739,12 +742,14 @@ func (legacy *legacyOnLeader) onStart(_ cell.HookContext) error {
 		}
 	}
 
+	clusterNamePolicy := cmtypes.LocalClusterNameForPolicies(legacy.cfgClusterMeshPolicy, option.Config.ClusterName)
+
 	if legacy.clientset.IsEnabled() && option.Config.EnableCiliumNetworkPolicy {
-		enableCNPWatcher(legacy.ctx, legacy.logger, &legacy.wg, legacy.clientset)
+		enableCNPWatcher(legacy.ctx, legacy.logger, &legacy.wg, legacy.clientset, clusterNamePolicy)
 	}
 
 	if legacy.clientset.IsEnabled() && option.Config.EnableCiliumClusterwideNetworkPolicy {
-		enableCCNPWatcher(legacy.ctx, legacy.logger, &legacy.wg, legacy.clientset)
+		enableCCNPWatcher(legacy.ctx, legacy.logger, &legacy.wg, legacy.clientset, clusterNamePolicy)
 	}
 
 	if legacy.clientset.IsEnabled() {
diff --git a/pkg/aws/eni/node.go b/pkg/aws/eni/node.go
index c61a02eae0..87c822bab7 100644
--- a/pkg/aws/eni/node.go
+++ b/pkg/aws/eni/node.go
@@ -293,7 +293,7 @@ func isSubnetAtPrefixCapacity(err error) bool {
 	return false
 }
 
-// AllocateIPs performs the ENI allocation oepration
+// AllocateIPs performs the ENI allocation operation
 func (n *Node) AllocateIPs(ctx context.Context, a *ipam.AllocationAction) error {
 	// Check if the interface to allocate on is prefix delegated
 	n.mutex.RLock()
diff --git a/pkg/bgpv1/manager/instance/instance.go b/pkg/bgpv1/manager/instance/instance.go
index 77d342c9ed..db5eeddfce 100644
--- a/pkg/bgpv1/manager/instance/instance.go
+++ b/pkg/bgpv1/manager/instance/instance.go
@@ -66,11 +66,19 @@ func NewServerWithConfig(ctx context.Context, log *slog.Logger, params types.Ser
 //
 // This is used in BGPv2 implementation.
 type BGPInstance struct {
-	Name      string
-	Global    types.BGPGlobal
-	CancelCtx context.CancelFunc
-	Config    *v2.CiliumBGPNodeInstance
-	Router    types.Router
+	Name                string
+	Global              types.BGPGlobal
+	CancelCtx           context.CancelFunc
+	Config              *v2.CiliumBGPNodeInstance
+	Router              types.Router
+	stateNotificationCh chan struct{}
+}
+
+func (i *BGPInstance) NotifyStateChange() {
+	select {
+	case i.stateNotificationCh <- struct{}{}:
+	default:
+	}
 }
 
 // NewBGPInstance will start an underlying BGP instance utilizing types.ServerParameters
@@ -90,10 +98,11 @@ func NewBGPInstance(ctx context.Context, log *slog.Logger, name string, params t
 	}
 
 	return &BGPInstance{
-		Name:      name,
-		Global:    params.Global,
-		CancelCtx: cancel,
-		Config:    nil,
-		Router:    s,
+		Name:                name,
+		Global:              params.Global,
+		CancelCtx:           cancel,
+		Config:              nil,
+		Router:              s,
+		stateNotificationCh: params.StateNotification,
 	}, nil
 }
diff --git a/pkg/bgpv1/manager/reconcilerv2/crd_status_test.go b/pkg/bgpv1/manager/reconcilerv2/crd_status_test.go
index 2980dbfca4..25757a8c60 100644
--- a/pkg/bgpv1/manager/reconcilerv2/crd_status_test.go
+++ b/pkg/bgpv1/manager/reconcilerv2/crd_status_test.go
@@ -28,6 +28,7 @@ import (
 	v2 "github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2"
 	k8s_client "github.com/cilium/cilium/pkg/k8s/client"
 	cilium_client_v2 "github.com/cilium/cilium/pkg/k8s/client/clientset/versioned/typed/cilium.io/v2"
+	nodeTypes "github.com/cilium/cilium/pkg/node/types"
 	"github.com/cilium/cilium/pkg/option"
 )
 
@@ -258,7 +259,7 @@ func TestCRDConditions(t *testing.T) {
 	for _, tt := range tests {
 		t.Run(tt.name, func(t *testing.T) {
 			ctx, cancel := context.WithTimeout(context.Background(), TestTimeout)
-			logger := hivetest.Logger(t)
+			logger := hivetest.Logger(t, hivetest.LogLevel(slog.LevelDebug))
 
 			f, watcherReadyFn := newCRDStatusFixture(ctx, require.New(t), logger)
 
@@ -336,6 +337,7 @@ func TestCRDConditions(t *testing.T) {
 func TestDisableStatusReport(t *testing.T) {
 	ctx := context.TODO()
 	logger := hivetest.Logger(t)
+	nodeTypes.SetName("node0")
 
 	var cs k8s_client.Clientset
 	hive := hive.New(cell.Module("test", "test",
diff --git a/pkg/bgpv1/test/testdata/peering-auth.txtar b/pkg/bgpv1/test/testdata/peering-auth.txtar
index d010abad3b..e867a47305 100644
--- a/pkg/bgpv1/test/testdata/peering-auth.txtar
+++ b/pkg/bgpv1/test/testdata/peering-auth.txtar
@@ -5,10 +5,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-k8s/wait-watchers v1.services v1.secrets
-
 # Configure gobgp server
 gobgp/add-server --router-id=1.2.3.4 65001 fd00::aa:bb:cc:101 1790
 gobgp/add-peer --password=Cilium123 fd00::aa:bb:cc:102 65001
diff --git a/pkg/bgpv1/test/testdata/peering-changes.txtar b/pkg/bgpv1/test/testdata/peering-changes.txtar
index 7bde9c6d06..d79e31ffcc 100644
--- a/pkg/bgpv1/test/testdata/peering-changes.txtar
+++ b/pkg/bgpv1/test/testdata/peering-changes.txtar
@@ -5,10 +5,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-k8s/wait-watchers v1.services
-
 # Add a LoadBalancer service
 k8s/add service-lb.yaml
 
diff --git a/pkg/bgpv1/test/testdata/peering-ipv6.txtar b/pkg/bgpv1/test/testdata/peering-ipv6.txtar
index ee4c3593cc..15fc0d00df 100644
--- a/pkg/bgpv1/test/testdata/peering-ipv6.txtar
+++ b/pkg/bgpv1/test/testdata/peering-ipv6.txtar
@@ -5,9 +5,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-
 # Configure gobgp server
 gobgp/add-server --router-id=1.2.3.4 65001 fd00::aa:bb:cc:111 1790
 gobgp/add-peer fd00::aa:bb:cc:112 65001
diff --git a/pkg/bgpv1/test/testdata/peering-multi-instance.txtar b/pkg/bgpv1/test/testdata/peering-multi-instance.txtar
index 957baac5ef..34eec5b95d 100644
--- a/pkg/bgpv1/test/testdata/peering-multi-instance.txtar
+++ b/pkg/bgpv1/test/testdata/peering-multi-instance.txtar
@@ -5,10 +5,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-k8s/wait-watchers v1.services
-
 # Configure gobgp servers
 gobgp/add-server 65010 10.99.0.101 1790
 gobgp/add-server 65011 10.99.0.102 1790
diff --git a/pkg/bgpv1/test/testdata/pod-cidr.txtar b/pkg/bgpv1/test/testdata/pod-cidr.txtar
index 4ac0551a45..3a6d72a87b 100644
--- a/pkg/bgpv1/test/testdata/pod-cidr.txtar
+++ b/pkg/bgpv1/test/testdata/pod-cidr.txtar
@@ -5,9 +5,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-
 # Configure gobgp server
 gobgp/add-server --router-id=1.2.3.4 65001 fd00::aa:bb:dd:101 1790
 gobgp/add-peer fd00::aa:bb:dd:102 65001
diff --git a/pkg/bgpv1/test/testdata/pod-ip-pool.txtar b/pkg/bgpv1/test/testdata/pod-ip-pool.txtar
index 8061cc0080..173bdeed63 100644
--- a/pkg/bgpv1/test/testdata/pod-ip-pool.txtar
+++ b/pkg/bgpv1/test/testdata/pod-ip-pool.txtar
@@ -5,9 +5,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-
 # Configure GoBGP server
 gobgp/add-server 65010 10.99.2.1 1790
 gobgp/add-peer 10.99.2.2 65001
diff --git a/pkg/bgpv1/test/testdata/svc-adverts.txtar b/pkg/bgpv1/test/testdata/svc-adverts.txtar
index 4dd9cba2ec..6914d6b5a9 100644
--- a/pkg/bgpv1/test/testdata/svc-adverts.txtar
+++ b/pkg/bgpv1/test/testdata/svc-adverts.txtar
@@ -6,10 +6,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-k8s/wait-watchers v1.services
-
 # Configure gobgp server
 gobgp/add-server --router-id=1.2.3.4 65010 fd00::bb:cc:dd:1 1790
 gobgp/add-server --router-id=5.6.7.8 65011 fd00::bb:cc:dd:2 1790
diff --git a/pkg/bgpv1/test/testdata/svc-aggregation.txtar b/pkg/bgpv1/test/testdata/svc-aggregation.txtar
index 8db1be2daf..3c90a08989 100644
--- a/pkg/bgpv1/test/testdata/svc-aggregation.txtar
+++ b/pkg/bgpv1/test/testdata/svc-aggregation.txtar
@@ -6,10 +6,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-k8s/wait-watchers v1.services
-
 # Configure gobgp server
 gobgp/add-server 65010 10.99.4.211 1790
 
diff --git a/pkg/bgpv1/test/testdata/svc-path-attributes.txtar b/pkg/bgpv1/test/testdata/svc-path-attributes.txtar
index 8b3c448393..cc8bfdcb45 100644
--- a/pkg/bgpv1/test/testdata/svc-path-attributes.txtar
+++ b/pkg/bgpv1/test/testdata/svc-path-attributes.txtar
@@ -7,10 +7,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-k8s/wait-watchers v1.services
-
 # Configure gobgp server
 gobgp/add-server --router-id=1.2.3.4 65001 fd00::bb:cc:dd:101 1790
 gobgp/add-peer fd00::bb:cc:dd:102 65001
diff --git a/pkg/bgpv1/test/testdata/svc-sharing.txtar b/pkg/bgpv1/test/testdata/svc-sharing.txtar
index 0e4ac4b76a..03309ea336 100644
--- a/pkg/bgpv1/test/testdata/svc-sharing.txtar
+++ b/pkg/bgpv1/test/testdata/svc-sharing.txtar
@@ -6,10 +6,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-k8s/wait-watchers v1.services
-
 # Configure gobgp server
 gobgp/add-server 65010 10.99.4.201 1790
 
diff --git a/pkg/bgpv1/test/testdata/svc-traffic-policy.txtar b/pkg/bgpv1/test/testdata/svc-traffic-policy.txtar
index 0b3744ddcf..0d9be8341c 100644
--- a/pkg/bgpv1/test/testdata/svc-traffic-policy.txtar
+++ b/pkg/bgpv1/test/testdata/svc-traffic-policy.txtar
@@ -6,10 +6,6 @@
 # Start the hive
 hive start
 
-# Wait for k8s watchers to be initialized
-k8s/wait-watchers cilium.io.v2.ciliumnodes cilium.io.v2.ciliumbgpnodeconfigs cilium.io.v2.ciliumbgppeerconfigs cilium.io.v2.ciliumbgpadvertisements
-k8s/wait-watchers v1.services v1.endpoints
-
 # Configure gobgp server
 gobgp/add-server 65010 10.99.4.101 1790
 
diff --git a/pkg/cgroups/cgroups.go b/pkg/cgroups/cgroups.go
index f3de5a7879..b387049c6c 100644
--- a/pkg/cgroups/cgroups.go
+++ b/pkg/cgroups/cgroups.go
@@ -4,10 +4,10 @@
 package cgroups
 
 import (
+	"log/slog"
 	"sync"
 
 	"github.com/cilium/cilium/pkg/defaults"
-	"github.com/cilium/cilium/pkg/logging"
 	"github.com/cilium/cilium/pkg/logging/logfields"
 )
 
@@ -19,8 +19,6 @@ var (
 	cgrpMountOnce sync.Once
 )
 
-var log = logging.DefaultLogger.WithField(logfields.LogSubsys, "cgroups")
-
 // setCgroupRoot will set the path to mount cgroupv2
 func setCgroupRoot(path string) {
 	cgroupRoot = path
@@ -36,17 +34,22 @@ func GetCgroupRoot() string {
 // location. It is harmless to have multiple cgroupv2 root mounts so unlike
 // BPFFS case we simply mount at the cilium default regardless if the system
 // has another mount created by systemd or otherwise.
-func CheckOrMountCgrpFS(mapRoot string) {
+func CheckOrMountCgrpFS(logger *slog.Logger, mapRoot string) {
 	cgrpMountOnce.Do(func() {
 		if mapRoot == "" {
 			mapRoot = cgroupRoot
 		}
 
 		if err := cgrpCheckOrMountLocation(mapRoot); err != nil {
-			log.WithError(err).
-				Warn("Failed to mount cgroupv2. Any functionality that needs cgroup (e.g.: socket-based LB) will not work.")
+			logger.Warn(
+				"Failed to mount cgroupv2. Any functionality that needs cgroup (e.g.: socket-based LB) will not work.",
+				logfields.Error, err,
+			)
 		} else {
-			log.Infof("Mounted cgroupv2 filesystem at %s", mapRoot)
+			logger.Info(
+				"Mounted cgroupv2 filesystem",
+				logfields.Location, mapRoot,
+			)
 		}
 	})
 }
diff --git a/pkg/cgroups/manager/cell.go b/pkg/cgroups/manager/cell.go
index 1d1a4bb8f2..9147677f52 100644
--- a/pkg/cgroups/manager/cell.go
+++ b/pkg/cgroups/manager/cell.go
@@ -4,9 +4,11 @@
 package manager
 
 import (
+	"log/slog"
+
 	"github.com/cilium/hive/cell"
-	"github.com/sirupsen/logrus"
 
+	"github.com/cilium/cilium/pkg/logging/logfields"
 	"github.com/cilium/cilium/pkg/option"
 )
 
@@ -22,7 +24,7 @@ var Cell = cell.Module(
 type cgroupManagerParams struct {
 	cell.In
 
-	Logger    logrus.FieldLogger
+	Logger    *slog.Logger
 	Lifecycle cell.Lifecycle
 
 	AgentConfig *option.DaemonConfig
@@ -36,8 +38,10 @@ func newCGroupManager(params cgroupManagerParams) CGroupManager {
 	pathProvider, err := getCgroupPathProvider()
 	if err != nil {
 		params.Logger.
-			WithError(err).
-			Info("Failed to setup socket load-balancing tracing with Hubble. See the kubeproxy-free guide for more details.")
+			Info(
+				"Failed to setup socket load-balancing tracing with Hubble. See the kubeproxy-free guide for more details.",
+				logfields.Error, err,
+			)
 
 		return &noopCGroupManager{}
 	}
diff --git a/pkg/cgroups/manager/manager.go b/pkg/cgroups/manager/manager.go
index 8f32d9b17d..6461301e14 100644
--- a/pkg/cgroups/manager/manager.go
+++ b/pkg/cgroups/manager/manager.go
@@ -9,11 +9,10 @@ import (
 	"slices"
 	"strings"
 
-	"github.com/sirupsen/logrus"
-
 	"github.com/cilium/cilium/pkg/cgroups"
 	v1 "github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1"
 	"github.com/cilium/cilium/pkg/lock"
+	"github.com/cilium/cilium/pkg/logging"
 	"github.com/cilium/cilium/pkg/logging/logfields"
 	nodetypes "github.com/cilium/cilium/pkg/node/types"
 )
@@ -51,7 +50,7 @@ type CGroupManager interface {
 // During initialization, the manager checks for a valid cgroup path pathProvider.
 // If it fails to find a pathProvider, it will ignore all the subsequent pod events.
 type cgroupManager struct {
-	logger logrus.FieldLogger
+	logger logging.FieldLogger
 	// Map of pod metadata indexed by their UIDs
 	podMetadataById map[podUID]*podMetadata
 	// Map of container metadata indexed by their cgroup ids
@@ -201,7 +200,7 @@ func (c cgroupImpl) GetCgroupID(cgroupPath string) (uint64, error) {
 	return cgroups.GetCgroupID(cgroupPath)
 }
 
-func newManager(logger logrus.FieldLogger, cg cgroup, pathProvider cgroupPathProvider, channelSize int) *cgroupManager {
+func newManager(logger logging.FieldLogger, cg cgroup, pathProvider cgroupPathProvider, channelSize int) *cgroupManager {
 	return &cgroupManager{
 		logger:                    logger,
 		podMetadataById:           make(map[string]*podMetadata),
@@ -290,11 +289,12 @@ func (m *cgroupManager) updatePodMetadata(pod, oldPod *v1.Pod) {
 		// Example:containerd://e275d1a37782ab30008aa3ae6666cccefe53b3a14a2ab5a8dc459939107c8c0e
 		_, after, found := strings.Cut(cId, "//")
 		if !found || after == "" {
-			m.logger.WithFields(logrus.Fields{
-				logfields.K8sPodName:   pod.Name,
-				logfields.K8sNamespace: pod.Namespace,
-				"container-id":         cId,
-			}).Error("unexpected container ID")
+			m.logger.Error(
+				"unexpected container ID",
+				logfields.K8sPodName, pod.Name,
+				logfields.K8sNamespace, pod.Namespace,
+				logfields.ContainerID, cId,
+			)
 			continue
 		}
 		cId = after
@@ -310,20 +310,24 @@ func (m *cgroupManager) updatePodMetadata(pod, oldPod *v1.Pod) {
 		// Container could've been gone, so don't log any errors.
 		cgrpPath, err := m.pathProvider.getContainerPath(id, cId, pod.Status.QOSClass)
 		if err != nil {
-			m.logger.WithFields(logrus.Fields{
-				logfields.K8sPodName:   pod.Name,
-				logfields.K8sNamespace: pod.Namespace,
-				"container-id":         cId,
-			}).WithError(err).Debugf("failed to get container metadata")
+			m.logger.Debug(
+				"failed to get container metadata",
+				logfields.Error, err,
+				logfields.K8sPodName, pod.Name,
+				logfields.K8sNamespace, pod.Namespace,
+				logfields.ContainerID, cId,
+			)
 			continue
 		}
 		cgrpId, err := m.cgroupsChecker.GetCgroupID(cgrpPath)
 		if err != nil {
-			m.logger.WithFields(logrus.Fields{
-				logfields.K8sPodName:   pod.Name,
-				logfields.K8sNamespace: pod.Namespace,
-				"cgroup-path":          cgrpPath,
-			}).WithError(err).Debugf("failed to get cgroup id")
+			m.logger.Debug(
+				"failed to get cgroup id",
+				logfields.Error, err,
+				logfields.K8sPodName, pod.Name,
+				logfields.K8sNamespace, pod.Namespace,
+				logfields.ContainerID, cId,
+			)
 			continue
 		}
 		m.containerMetadataByCgrpId[cgrpId] = &containerMetadata{
@@ -400,9 +404,10 @@ func (m *cgroupManager) dumpPodMetadata(allMetadataOut chan []*FullPodMetadata)
 	for _, cm := range m.containerMetadataByCgrpId {
 		pm, ok := m.podMetadataById[cm.podId]
 		if !ok {
-			m.logger.WithFields(logrus.Fields{
-				"container-cgroup-id": cm.cgroupId,
-			}).Debugf("Pod metadata not found")
+			m.logger.Debug(
+				"Pod metadata not found",
+				logfields.CGroupID, cm.cgroupId,
+			)
 			continue
 		}
 		fullPm, ok := allMetas[cm.podId]
diff --git a/pkg/cgroups/manager/manager_test.go b/pkg/cgroups/manager/manager_test.go
index 4595ad11ba..0027730ee2 100644
--- a/pkg/cgroups/manager/manager_test.go
+++ b/pkg/cgroups/manager/manager_test.go
@@ -5,10 +5,9 @@ package manager
 
 import (
 	"fmt"
-	"io"
+	"log/slog"
 	"testing"
 
-	"github.com/sirupsen/logrus"
 	"github.com/stretchr/testify/require"
 
 	slimcorev1 "github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1"
@@ -122,8 +121,7 @@ var (
 )
 
 func newCgroupManagerTest(t testing.TB, pMock providerMock, cg cgroup, events chan podEventStatus) CGroupManager {
-	logger := logrus.New()
-	logger.SetOutput(io.Discard)
+	logger := slog.New(slog.DiscardHandler)
 
 	// Unbuffered channel tests to detect any issues on the caller side.
 	tcm := newManager(logger, cg, pMock, 0)
diff --git a/pkg/ciliumenvoyconfig/testdata/anyport.txtar b/pkg/ciliumenvoyconfig/testdata/anyport.txtar
index 956be44845..aa897aad3d 100644
--- a/pkg/ciliumenvoyconfig/testdata/anyport.txtar
+++ b/pkg/ciliumenvoyconfig/testdata/anyport.txtar
@@ -1,8 +1,6 @@
 # Test handling of CiliumEnvoyConfig, without specifying service ports
 
-# Start the hive and wait for tables to be synchronized before adding k8s objects.
 hive start
-db/initialized
 
 # Set up the services and endpoints
 k8s/add service.yaml
diff --git a/pkg/ciliumenvoyconfig/testdata/backendservices.txtar b/pkg/ciliumenvoyconfig/testdata/backendservices.txtar
index a9d9460216..33d9c4a8b8 100644
--- a/pkg/ciliumenvoyconfig/testdata/backendservices.txtar
+++ b/pkg/ciliumenvoyconfig/testdata/backendservices.txtar
@@ -1,8 +1,6 @@
 # Test handling of CiliumEnvoyConfig with backend services.
 
-# Start the hive and wait for tables to be synchronized before adding k8s objects.
 hive start
-db/initialized
 
 # Set up the services and endpoints. Add the test/frontend first and wait for it
 # to reconcile.
diff --git a/pkg/ciliumenvoyconfig/testdata/clusterwide.txtar b/pkg/ciliumenvoyconfig/testdata/clusterwide.txtar
index b183788bc2..1fd4a4ee0d 100644
--- a/pkg/ciliumenvoyconfig/testdata/clusterwide.txtar
+++ b/pkg/ciliumenvoyconfig/testdata/clusterwide.txtar
@@ -1,8 +1,6 @@
 # Test handling of CiliumClusterwideEnvoyConfig
 
-# Start the hive and wait for tables to be synchronized before adding k8s objects.
 hive start
-db/initialized
 
 # Set up the services and endpoints
 k8s/add service.yaml endpointslice.yaml
diff --git a/pkg/ciliumenvoyconfig/testdata/headless.txtar b/pkg/ciliumenvoyconfig/testdata/headless.txtar
index eda2f4bc47..250b5fb378 100644
--- a/pkg/ciliumenvoyconfig/testdata/headless.txtar
+++ b/pkg/ciliumenvoyconfig/testdata/headless.txtar
@@ -1,8 +1,6 @@
 # Test handling of headless services
 
-# Start the hive and wait for tables to be synchronized before adding k8s objects.
 hive start
-db/initialized
 
 # Set up the services and endpoints
 k8s/add service.yaml endpointslice.yaml
diff --git a/pkg/ciliumenvoyconfig/testdata/ingress.txtar b/pkg/ciliumenvoyconfig/testdata/ingress.txtar
index 3fafc6b8ab..9caa54b4f2 100644
--- a/pkg/ciliumenvoyconfig/testdata/ingress.txtar
+++ b/pkg/ciliumenvoyconfig/testdata/ingress.txtar
@@ -6,7 +6,6 @@
 # dumped using e.g. "kubectl get svc/details -o yaml".
 
 hive/start
-db/initialized
 
 # Add the objects
 k8s/add svc-ingress.yaml eps-ingress.yaml cec.yaml
diff --git a/pkg/ciliumenvoyconfig/testdata/labels.txtar b/pkg/ciliumenvoyconfig/testdata/labels.txtar
index 3ce95e702b..244cd52358 100644
--- a/pkg/ciliumenvoyconfig/testdata/labels.txtar
+++ b/pkg/ciliumenvoyconfig/testdata/labels.txtar
@@ -2,7 +2,6 @@
 
 # Start the hive and wait for tables to be synchronized before adding k8s objects.
 hive start
-db/initialized
 
 set-node-labels foo=a
 
diff --git a/pkg/ciliumenvoyconfig/testdata/marshalling.txtar b/pkg/ciliumenvoyconfig/testdata/marshalling.txtar
index e50e424833..3c37793199 100644
--- a/pkg/ciliumenvoyconfig/testdata/marshalling.txtar
+++ b/pkg/ciliumenvoyconfig/testdata/marshalling.txtar
@@ -2,7 +2,6 @@
 
 # Start the hive and wait for tables to be synchronized before adding k8s objects.
 hive start
-db/initialized
 
 # Add the objects and wait for it to be ingested.
 k8s/add service.yaml endpointslice.yaml cec.yaml
diff --git a/pkg/ciliumenvoyconfig/testdata/namespaced.txtar b/pkg/ciliumenvoyconfig/testdata/namespaced.txtar
index 8bfa5aa5ed..76315b487e 100644
--- a/pkg/ciliumenvoyconfig/testdata/namespaced.txtar
+++ b/pkg/ciliumenvoyconfig/testdata/namespaced.txtar
@@ -5,7 +5,6 @@ db/insert node-addresses addrv4.yaml
 
 # Start the hive and wait for tables to be synchronized before adding k8s objects.
 hive start
-db/initialized
 
 # Set up the services and endpoints
 k8s/add service.yaml
diff --git a/pkg/ciliumenvoyconfig/testdata/shared_cluster.txtar b/pkg/ciliumenvoyconfig/testdata/shared_cluster.txtar
index 88b0c0e2b4..0f909beae5 100644
--- a/pkg/ciliumenvoyconfig/testdata/shared_cluster.txtar
+++ b/pkg/ciliumenvoyconfig/testdata/shared_cluster.txtar
@@ -5,7 +5,6 @@ db/insert node-addresses addrv4.yaml
 
 # Start the hive and wait for tables to be synchronized before adding k8s objects.
 hive start
-db/initialized
 
 # Set up the services and endpoints
 k8s/add service.yaml
diff --git a/pkg/clustermesh/testdata/clusterservice.txtar b/pkg/clustermesh/testdata/clusterservice.txtar
index 3af97615b9..28890b9711 100644
--- a/pkg/clustermesh/testdata/clusterservice.txtar
+++ b/pkg/clustermesh/testdata/clusterservice.txtar
@@ -1,7 +1,6 @@
 #! --cluster-id=1 --cluster-name=cluster1
 
 hive/start
-db/initialized
 
 # Create a service to which to add the external backends.
 k8s/add service.yaml endpointslice.yaml
diff --git a/pkg/clustermesh/testdata/service-affinity.txtar b/pkg/clustermesh/testdata/service-affinity.txtar
index 2fbb1a3dc8..c9c8d08b82 100644
--- a/pkg/clustermesh/testdata/service-affinity.txtar
+++ b/pkg/clustermesh/testdata/service-affinity.txtar
@@ -1,7 +1,6 @@
 #! --cluster-id=1 --cluster-name=cluster1
 
 hive/start
-db/initialized
 
 # Create a service with affinity set to local.
 k8s/add service.yaml endpointslice.yaml
diff --git a/pkg/clustermesh/types/option.go b/pkg/clustermesh/types/option.go
index fe746e82b8..5689684344 100644
--- a/pkg/clustermesh/types/option.go
+++ b/pkg/clustermesh/types/option.go
@@ -135,3 +135,33 @@ func (_ QuirksConfig) Flags(flags *pflag.FlagSet) {
 			"policy both within and across clusters")
 	flags.MarkHidden("allow-unsafe-policy-skb-usage")
 }
+
+const PolicyAnyCluster = ""
+
+// PolicyConfig allows the user to configure config related to ClusterMesh and policies
+type PolicyConfig struct {
+	// PolicyDefaultLocalCluster control whether policy rules assume
+	// by default the local cluster if not explicitly selected
+	PolicyDefaultLocalCluster bool
+}
+
+var DefaultPolicyConfig = PolicyConfig{
+	PolicyDefaultLocalCluster: false,
+}
+
+func (p PolicyConfig) Flags(flags *pflag.FlagSet) {
+	flags.Bool(
+		"policy-default-local-cluster", p.PolicyDefaultLocalCluster,
+		"Control whether policy rules assume by default the local cluster if not explicitly selected",
+	)
+}
+
+// LocalClusterNameForPolicies returns what should be considered the local cluster
+// name in network policies
+func LocalClusterNameForPolicies(cfg PolicyConfig, localClusterName string) string {
+	if cfg.PolicyDefaultLocalCluster {
+		return localClusterName
+	} else {
+		return PolicyAnyCluster
+	}
+}
diff --git a/pkg/crypto/certloader/watcher.go b/pkg/crypto/certloader/watcher.go
index 2db33de019..5a303dc4e8 100644
--- a/pkg/crypto/certloader/watcher.go
+++ b/pkg/crypto/certloader/watcher.go
@@ -35,7 +35,7 @@ func NewWatcher(log *slog.Logger, caFiles []string, certFile, privkeyFile string
 	// An error here would be unexpected as we were able to create a
 	// FileReloader having read the files, so the files should exist and be
 	// "watchable".
-	fswatcher, err := newFsWatcher(caFiles, certFile, privkeyFile)
+	fswatcher, err := newFsWatcher(log, caFiles, certFile, privkeyFile)
 	if err != nil {
 		return nil, err
 	}
@@ -59,7 +59,7 @@ func FutureWatcher(log *slog.Logger, caFiles []string, certFile, privkeyFile str
 	if err != nil {
 		return nil, err
 	}
-	fswatcher, err := newFsWatcher(caFiles, certFile, privkeyFile)
+	fswatcher, err := newFsWatcher(log, caFiles, certFile, privkeyFile)
 	if err != nil {
 		return nil, err
 	}
@@ -208,7 +208,7 @@ func (w *Watcher) Stop() {
 // newFsWatcher returns a fswatcher.Watcher watching over the given files.
 // The fswatcher.Watcher supports watching over files which do not exist yet.
 // A create event will be emitted once the file is added.
-func newFsWatcher(caFiles []string, certFile, privkeyFile string) (*fswatcher.Watcher, error) {
+func newFsWatcher(logger *slog.Logger, caFiles []string, certFile, privkeyFile string) (*fswatcher.Watcher, error) {
 	trackFiles := []string{}
 
 	if certFile != "" {
@@ -223,5 +223,5 @@ func newFsWatcher(caFiles []string, certFile, privkeyFile string) (*fswatcher.Wa
 		}
 	}
 
-	return fswatcher.New(trackFiles)
+	return fswatcher.New(logger, trackFiles)
 }
diff --git a/pkg/datapath/config/host_config.go b/pkg/datapath/config/host_config.go
index 02193d3020..d4b5ef1ce5 100644
--- a/pkg/datapath/config/host_config.go
+++ b/pkg/datapath/config/host_config.go
@@ -20,7 +20,7 @@ type BPFHost struct {
 	// MAC address of the interface the bpf program is attached to.
 	InterfaceMAC [8]byte `config:"interface_mac"`
 	// Masquerade address for IPv4 traffic.
-	NATIPv4Masquerade uint32 `config:"nat_ipv4_masquerade"`
+	NATIPv4Masquerade [4]byte `config:"nat_ipv4_masquerade"`
 	// Masquerade address for IPv6 traffic.
 	NATIPv6Masquerade [16]byte `config:"nat_ipv6_masquerade"`
 	// Pull security context from IP cache.
@@ -33,7 +33,7 @@ type BPFHost struct {
 
 func NewBPFHost(node Node) *BPFHost {
 	return &BPFHost{0x5dc, 0xe, 0x0, [8]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
-		0x0,
+		[4]byte{0x0, 0x0, 0x0, 0x0},
 		[16]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
 		false, 0x0, node}
 }
diff --git a/pkg/datapath/config/lxc_config.go b/pkg/datapath/config/lxc_config.go
index 201891ebeb..c2ca2f629f 100644
--- a/pkg/datapath/config/lxc_config.go
+++ b/pkg/datapath/config/lxc_config.go
@@ -15,7 +15,7 @@ type BPFLXC struct {
 	// The endpoint's security ID.
 	EndpointID uint16 `config:"endpoint_id"`
 	// The endpoint's IPv4 address.
-	EndpointIPv4 uint32 `config:"endpoint_ipv4"`
+	EndpointIPv4 [4]byte `config:"endpoint_ipv4"`
 	// The endpoint's IPv6 address.
 	EndpointIPv6 [16]byte `config:"endpoint_ipv6"`
 	// The endpoint's network namespace cookie.
@@ -25,7 +25,7 @@ type BPFLXC struct {
 	// MAC address of the interface the bpf program is attached to.
 	InterfaceMAC [8]byte `config:"interface_mac"`
 	// Masquerade address for IPv4 traffic.
-	NATIPv4Masquerade uint32 `config:"nat_ipv4_masquerade"`
+	NATIPv4Masquerade [4]byte `config:"nat_ipv4_masquerade"`
 	// Masquerade address for IPv6 traffic.
 	NATIPv6Masquerade [16]byte `config:"nat_ipv6_masquerade"`
 	// The log level for policy verdicts in workload endpoints.
@@ -39,10 +39,10 @@ type BPFLXC struct {
 }
 
 func NewBPFLXC(node Node) *BPFLXC {
-	return &BPFLXC{0x5dc, 0x0, 0x0,
+	return &BPFLXC{0x5dc, 0x0, [4]byte{0x0, 0x0, 0x0, 0x0},
 		[16]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
 		0x0, 0x0, [8]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
-		0x0,
+		[4]byte{0x0, 0x0, 0x0, 0x0},
 		[16]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
 		0x0, false, 0x0, node}
 }
diff --git a/pkg/datapath/config/node_config.go b/pkg/datapath/config/node_config.go
index b8302ed50f..3fb84e47c1 100644
--- a/pkg/datapath/config/node_config.go
+++ b/pkg/datapath/config/node_config.go
@@ -11,8 +11,11 @@ package config
 type Node struct {
 	// Internal IPv6 router address assigned to the cilium_host interface.
 	RouterIPv6 [16]byte `config:"router_ipv6"`
+	// IPv4 source address used for SNAT when a Pod talks to itself over a Service.
+	ServiceLoopbackIPv4 [4]byte `config:"service_loopback_ipv4"`
 }
 
 func NewNode() *Node {
-	return &Node{[16]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}}
+	return &Node{[16]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
+		[4]byte{0x0, 0x0, 0x0, 0x0}}
 }
diff --git a/pkg/datapath/config/overlay_config.go b/pkg/datapath/config/overlay_config.go
index e485cab5d8..ea9810f684 100644
--- a/pkg/datapath/config/overlay_config.go
+++ b/pkg/datapath/config/overlay_config.go
@@ -17,7 +17,7 @@ type BPFOverlay struct {
 	// MAC address of the interface the bpf program is attached to.
 	InterfaceMAC [8]byte `config:"interface_mac"`
 	// Masquerade address for IPv4 traffic.
-	NATIPv4Masquerade uint32 `config:"nat_ipv4_masquerade"`
+	NATIPv4Masquerade [4]byte `config:"nat_ipv4_masquerade"`
 	// Masquerade address for IPv6 traffic.
 	NATIPv6Masquerade [16]byte `config:"nat_ipv6_masquerade"`
 	// Pull security context from IP cache.
@@ -27,7 +27,7 @@ type BPFOverlay struct {
 }
 
 func NewBPFOverlay(node Node) *BPFOverlay {
-	return &BPFOverlay{0x5dc, 0x0, [8]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, 0x0,
+	return &BPFOverlay{0x5dc, 0x0, [8]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, [4]byte{0x0, 0x0, 0x0, 0x0},
 		[16]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
 		false, node}
 }
diff --git a/pkg/datapath/config/wireguard_config.go b/pkg/datapath/config/wireguard_config.go
index 769e908fe8..4b0d30103a 100644
--- a/pkg/datapath/config/wireguard_config.go
+++ b/pkg/datapath/config/wireguard_config.go
@@ -17,7 +17,7 @@ type BPFWireguard struct {
 	// MAC address of the interface the bpf program is attached to.
 	InterfaceMAC [8]byte `config:"interface_mac"`
 	// Masquerade address for IPv4 traffic.
-	NATIPv4Masquerade uint32 `config:"nat_ipv4_masquerade"`
+	NATIPv4Masquerade [4]byte `config:"nat_ipv4_masquerade"`
 	// Masquerade address for IPv6 traffic.
 	NATIPv6Masquerade [16]byte `config:"nat_ipv6_masquerade"`
 	// Pull security context from IP cache.
@@ -27,7 +27,7 @@ type BPFWireguard struct {
 }
 
 func NewBPFWireguard(node Node) *BPFWireguard {
-	return &BPFWireguard{0x5dc, 0x0, [8]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, 0x0,
+	return &BPFWireguard{0x5dc, 0x0, [8]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, [4]byte{0x0, 0x0, 0x0, 0x0},
 		[16]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
 		false, node}
 }
diff --git a/pkg/datapath/config/xdp_config.go b/pkg/datapath/config/xdp_config.go
index c9cddc2427..ea1f4b6cb6 100644
--- a/pkg/datapath/config/xdp_config.go
+++ b/pkg/datapath/config/xdp_config.go
@@ -17,7 +17,7 @@ type BPFXDP struct {
 	// MAC address of the interface the bpf program is attached to.
 	InterfaceMAC [8]byte `config:"interface_mac"`
 	// Masquerade address for IPv4 traffic.
-	NATIPv4Masquerade uint32 `config:"nat_ipv4_masquerade"`
+	NATIPv4Masquerade [4]byte `config:"nat_ipv4_masquerade"`
 	// Masquerade address for IPv6 traffic.
 	NATIPv6Masquerade [16]byte `config:"nat_ipv6_masquerade"`
 	// Pull security context from IP cache.
@@ -27,7 +27,7 @@ type BPFXDP struct {
 }
 
 func NewBPFXDP(node Node) *BPFXDP {
-	return &BPFXDP{0x5dc, 0x0, [8]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, 0x0,
+	return &BPFXDP{0x5dc, 0x0, [8]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, [4]byte{0x0, 0x0, 0x0, 0x0},
 		[16]byte{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},
 		false, node}
 }
diff --git a/pkg/datapath/iptables/iptables.go b/pkg/datapath/iptables/iptables.go
index b691b08460..08e2653e0d 100644
--- a/pkg/datapath/iptables/iptables.go
+++ b/pkg/datapath/iptables/iptables.go
@@ -345,7 +345,6 @@ func newIptablesManager(p params) datapath.IptablesManager {
 
 	// init haveIp6tables argument before using it in a reconciliation loop
 	iptMgr.startDone = iptMgr.argsInit.Add()
-	p.Lifecycle.Append(iptMgr)
 
 	p.JobGroup.Add(
 		job.OneShot("iptables-reconciliation-loop", func(ctx context.Context, health cell.Health) error {
@@ -368,6 +367,10 @@ func newIptablesManager(p params) datapath.IptablesManager {
 		}),
 	)
 
+	// Add the manager after the reconciler, otherwise there is a deadlock on shutdown
+	// between closing and draining the channels.
+	p.Lifecycle.Append(iptMgr)
+
 	return iptMgr
 }
 
diff --git a/pkg/datapath/linux/config/config.go b/pkg/datapath/linux/config/config.go
index dc1af296ce..c8f85ebe2f 100644
--- a/pkg/datapath/linux/config/config.go
+++ b/pkg/datapath/linux/config/config.go
@@ -142,10 +142,8 @@ func (h *HeaderfileWriter) WriteNodeConfig(w io.Writer, cfg *datapath.LocalNodeC
 
 	if option.Config.EnableIPv4 {
 		ipv4GW := cfg.CiliumInternalIPv4
-		loopbackIPv4 := cfg.LoopbackIPv4
 		ipv4Range := cfg.AllocCIDRIPv4
 		cDefinesMap["IPV4_GATEWAY"] = fmt.Sprintf("%#x", byteorder.NetIPv4ToHost32(ipv4GW))
-		cDefinesMap["IPV4_LOOPBACK"] = fmt.Sprintf("%#x", byteorder.NetIPv4ToHost32(loopbackIPv4))
 		cDefinesMap["IPV4_MASK"] = fmt.Sprintf("%#x", byteorder.NetIPv4ToHost32(net.IP(ipv4Range.Mask)))
 
 		if option.Config.EnableIPv4FragmentsTracking {
diff --git a/pkg/datapath/linux/config/config_test.go b/pkg/datapath/linux/config/config_test.go
index f15c97e8f4..00bda65bec 100644
--- a/pkg/datapath/linux/config/config_test.go
+++ b/pkg/datapath/linux/config/config_test.go
@@ -36,16 +36,16 @@ import (
 
 var (
 	dummyNodeCfg = datapath.LocalNodeConfiguration{
-		NodeIPv4:           ipv4DummyAddr.AsSlice(),
-		NodeIPv6:           ipv6DummyAddr.AsSlice(),
-		CiliumInternalIPv4: ipv4DummyAddr.AsSlice(),
-		CiliumInternalIPv6: ipv6DummyAddr.AsSlice(),
-		AllocCIDRIPv4:      cidr.MustParseCIDR("10.147.0.0/16"),
-		LoopbackIPv4:       ipv4DummyAddr.AsSlice(),
-		Devices:            []*tables.Device{},
-		NodeAddresses:      []tables.NodeAddress{},
-		HostEndpointID:     1,
-		MaglevConfig:       maglev.DefaultConfig,
+		NodeIPv4:            ipv4DummyAddr.AsSlice(),
+		NodeIPv6:            ipv6DummyAddr.AsSlice(),
+		CiliumInternalIPv4:  ipv4DummyAddr.AsSlice(),
+		CiliumInternalIPv6:  ipv6DummyAddr.AsSlice(),
+		AllocCIDRIPv4:       cidr.MustParseCIDR("10.147.0.0/16"),
+		ServiceLoopbackIPv4: ipv4DummyAddr.AsSlice(),
+		Devices:             []*tables.Device{},
+		NodeAddresses:       []tables.NodeAddress{},
+		HostEndpointID:      1,
+		MaglevConfig:        maglev.DefaultConfig,
 	}
 	dummyDevCfg   testutils.TestEndpoint
 	ipv4DummyAddr = netip.MustParseAddr("192.0.2.3")
diff --git a/pkg/datapath/linux/devices_controller.go b/pkg/datapath/linux/devices_controller.go
index 5e84c6f206..4abcce61b4 100644
--- a/pkg/datapath/linux/devices_controller.go
+++ b/pkg/datapath/linux/devices_controller.go
@@ -14,8 +14,6 @@ import (
 	"slices"
 	"strings"
 
-	"github.com/cilium/ebpf"
-	"github.com/cilium/ebpf/asm"
 	"github.com/cilium/hive/cell"
 	"github.com/cilium/statedb"
 	"github.com/spf13/pflag"
@@ -26,7 +24,6 @@ import (
 	"golang.org/x/sys/unix"
 	"k8s.io/apimachinery/pkg/util/sets"
 
-	"github.com/cilium/cilium/pkg/datapath/linux/probes"
 	"github.com/cilium/cilium/pkg/datapath/linux/safenetlink"
 	"github.com/cilium/cilium/pkg/datapath/tables"
 	"github.com/cilium/cilium/pkg/defaults"
@@ -131,7 +128,6 @@ type devicesController struct {
 	initialized          chan struct{}
 	filter               tables.DeviceFilter
 	enforceAutoDetection bool
-	l3DevSupported       bool
 
 	// deadLinkIndexes tracks the set of links that have been deleted. This is needed
 	// to avoid processing route or address updates after a link delete as they may
@@ -161,9 +157,6 @@ func (dc *devicesController) Start(startCtx cell.HookContext) error {
 		if err != nil {
 			return err
 		}
-
-		// Only probe for L3 device support when netlink isn't mocked by tests.
-		dc.l3DevSupported = probes.HaveProgramHelper(dc.log, ebpf.SchedCLS, asm.FnSkbChangeHead) == nil
 	}
 
 	var ctx context.Context
@@ -664,12 +657,6 @@ func (dc *devicesController) isSelectedDevice(d *tables.Device, txn statedb.Writ
 		return false, fmt.Sprintf("bridged or bonded to ifindex %d", d.MasterIndex)
 	}
 
-	// Ignore L3 devices if we cannot support them.
-	hasMacAddr := len(d.HardwareAddr) != 0
-	if !dc.l3DevSupported && !hasMacAddr {
-		return false, "L3 device, kernel too old, >= 5.8 required"
-	}
-
 	// Never consider devices with any of the excluded devices.
 	for _, p := range defaults.ExcludedDevicePrefixes {
 		if strings.HasPrefix(d.Name, p) {
diff --git a/pkg/datapath/linux/ipsec/ipsec_linux.go b/pkg/datapath/linux/ipsec/ipsec_linux.go
index 4f5980ce27..6fb76bd0fe 100644
--- a/pkg/datapath/linux/ipsec/ipsec_linux.go
+++ b/pkg/datapath/linux/ipsec/ipsec_linux.go
@@ -1227,7 +1227,7 @@ func StartKeyfileWatcher(log *slog.Logger, group job.Group, keyfilePath string,
 		return nil
 	}
 
-	watcher, err := fswatcher.New([]string{keyfilePath})
+	watcher, err := fswatcher.New(log, []string{keyfilePath})
 	if err != nil {
 		return err
 	}
diff --git a/pkg/datapath/linux/requirements.go b/pkg/datapath/linux/requirements.go
index 468a5972eb..47663a9b1d 100644
--- a/pkg/datapath/linux/requirements.go
+++ b/pkg/datapath/linux/requirements.go
@@ -103,6 +103,10 @@ func CheckRequirements(log *slog.Logger) error {
 			return errors.New("Require support for bpf_csum_level() (Linux 5.8.0 or newer)")
 		}
 
+		if probes.HaveProgramHelper(log, ebpf.SchedCLS, asm.FnSkbChangeHead) != nil {
+			return errors.New("Require support for bpf_skb_change_head() (Linux 5.8.0 or newer)")
+		}
+
 		if probes.HaveProgramHelper(log, ebpf.SchedCLS, asm.FnRedirectNeigh) != nil {
 			return errors.New("Require support for bpf_redirect_neigh() (Linux 5.10.0 or newer)")
 		}
diff --git a/pkg/datapath/linux/routing/routing.go b/pkg/datapath/linux/routing/routing.go
index a1e442327e..c4d710af9c 100644
--- a/pkg/datapath/linux/routing/routing.go
+++ b/pkg/datapath/linux/routing/routing.go
@@ -87,14 +87,15 @@ func (info *RoutingInfo) Configure(ip net.IP, mtu int, compat bool, host bool) e
 		}
 	}
 
-	var egressPriority, tableID int
+	var egressPriority, ifaceNum, tableID int
 	if compat {
 		egressPriority = linux_defaults.RulePriorityEgress
-		tableID = ifindex
+		ifaceNum = ifindex
 	} else {
 		egressPriority = linux_defaults.RulePriorityEgressv2
-		tableID = computeTableIDFromIfaceNumber(info.InterfaceNumber)
+		ifaceNum = info.InterfaceNumber
 	}
+	tableID = computeTableIDFromIfaceNumber(compat, ifaceNum)
 
 	// The condition here should mirror the condition in Delete.
 	if info.Masquerade && info.IpamMode == ipamOption.IPAMENI {
@@ -136,12 +137,13 @@ func (info *RoutingInfo) ReconcileGatewayRoutes(mtu int, compat bool, rx statedb
 		return set, fmt.Errorf("unable to find ifindex for interface MAC: %w", err)
 	}
 
-	var tableID int
+	var ifaceNum, tableID int
 	if compat {
-		tableID = ifindex
+		ifaceNum = ifindex
 	} else {
-		tableID = computeTableIDFromIfaceNumber(info.InterfaceNumber)
+		ifaceNum = info.InterfaceNumber
 	}
+	tableID = computeTableIDFromIfaceNumber(compat, ifaceNum)
 
 	// Get the desired routes.
 	gwRoutes := info.gatewayRoutes(ifindex, tableID)
@@ -249,7 +251,8 @@ func (info *RoutingInfo) installRoutes(ifindex, tableID int) error {
 // to perform a narrower search on the rule because we know it references the
 // main routing table. Due to multiple routing CIDRs, there might be more than
 // one egress rule. Deletion of any rule only proceeds if the rule matches
-// the IP & priority. If more than one rule matches, then deletion is skipped.
+// the IP & priority. In order to avoid leaving stale rules behind, all the
+// matching rules are removed.
 func Delete(logger *slog.Logger, ip netip.Addr, compat bool) error {
 	if !ip.Is4() && !ip.Is6() && !ip.IsValid() {
 		logger.Warn(
@@ -260,12 +263,12 @@ func Delete(logger *slog.Logger, ip netip.Addr, compat bool) error {
 	}
 
 	ipWithMask := netipx.AddrIPNet(ip)
-	var deleteRuleFn func(*slog.Logger, route.Rule) error
 
+	var family int
 	if ip.Is4() {
-		deleteRuleFn = deleteRuleIPv4
+		family = netlink.FAMILY_V4
 	} else {
-		deleteRuleFn = deleteRuleIPv6
+		family = netlink.FAMILY_V6
 	}
 
 	// Ingress rules
@@ -275,7 +278,13 @@ func Delete(logger *slog.Logger, ip netip.Addr, compat bool) error {
 		Table:    route.MainTable,
 	}
 
-	if err := deleteRuleFn(logger, ingress); err != nil {
+	if err := deleteRulesFiltered(
+		logger, ingress, family,
+		deleteRuleFilter{
+			fn:  func(r netlink.Rule) bool { return true }, // no further check needed, delete all rules found
+			msg: "",
+		},
+	); err != nil {
 		return fmt.Errorf("unable to delete ingress rule from main table with ip %s: %w", ipWithMask.String(), err)
 	}
 	logger.Debug("Deleted ingress rule",
@@ -289,6 +298,11 @@ func Delete(logger *slog.Logger, ip netip.Addr, compat bool) error {
 	}
 
 	// Egress rules
+	withENIRouteTableID := deleteRuleFilter{
+		fn:  func(r netlink.Rule) bool { return r.Table >= computeTableIDFromIfaceNumber(compat, 0) },
+		msg: "rule does not refer to a per-ENI routing table ID",
+	}
+
 	// The condition here should mirror the conditions in Configure.
 	info := node.GetRouterInfo()
 	if info != nil && option.Config.EnableIPv4Masquerade && option.Config.IPAM == ipamOption.IPAMENI {
@@ -307,7 +321,7 @@ func Delete(logger *slog.Logger, ip netip.Addr, compat bool) error {
 				From:     ipWithMask,
 				To:       normalizeRuleToCIDR(cidr),
 			}
-			if err := deleteRuleIPv4(logger, egress); err != nil {
+			if err := deleteRulesFiltered(logger, egress, netlink.FAMILY_V4, withENIRouteTableID); err != nil {
 				return fmt.Errorf("unable to delete egress rule with ip %s: %w", ipWithMask.String(), err)
 			}
 			logger.Debug("Deleted egress rule",
@@ -321,7 +335,7 @@ func Delete(logger *slog.Logger, ip netip.Addr, compat bool) error {
 				From:     ipWithMask,
 				To:       normalizeRuleToCIDR(cidr),
 			}
-			if err := deleteRuleIPv6(logger, egress); err != nil {
+			if err := deleteRulesFiltered(logger, egress, netlink.FAMILY_V6, withENIRouteTableID); err != nil {
 				return fmt.Errorf("unable to delete egress rule with ip %s: %w", ipWithMask.String(), err)
 			}
 			logger.Debug("Deleted egress rule",
@@ -334,7 +348,8 @@ func Delete(logger *slog.Logger, ip netip.Addr, compat bool) error {
 			Priority: priority,
 			From:     ipWithMask,
 		}
-		if err := deleteRuleFn(logger, egress); err != nil {
+		if err := deleteRulesFiltered(
+			logger, egress, family, withENIRouteTableID); err != nil {
 			return fmt.Errorf("unable to delete egress rule with ip %s: %w", ipWithMask.String(), err)
 		}
 		logger.Debug("Deleted egress rule",
@@ -363,39 +378,37 @@ func Delete(logger *slog.Logger, ip netip.Addr, compat bool) error {
 	return nil
 }
 
-func deleteRuleIPv4(logger *slog.Logger, r route.Rule) error {
-	return deleteRule(logger, r, netlink.FAMILY_V4)
+type deleteRuleFilter struct {
+	fn  func(r netlink.Rule) bool
+	msg string
 }
 
-func deleteRuleIPv6(logger *slog.Logger, r route.Rule) error {
-	return deleteRule(logger, r, netlink.FAMILY_V6)
-}
-
-func deleteRule(logger *slog.Logger, r route.Rule, family int) error {
-	rules, err := route.ListRules(family, &r)
+func deleteRulesFiltered(logger *slog.Logger, template route.Rule, family int, filters ...deleteRuleFilter) error {
+	rules, err := route.ListRules(family, &template)
 	if err != nil {
 		return err
 	}
 
-	length := len(rules)
-	switch {
-	case length > 1:
-		logger.Warn(
-			"Found too many rules matching, skipping deletion",
-			logfields.Candidates, rules,
-			logfields.Rule, r,
-		)
-		return errors.New("unexpected number of rules found to delete")
-	case length == 1:
-		return route.DeleteRule(family, r)
+	if len(rules) == 0 {
+		logger.Warn("No rule matching found", logfields.Rule, template)
+		return errors.New("no rule found to delete")
 	}
 
-	logger.Warn(
-		"No rule matching found",
-		logfields.Rule, r,
-	)
-
-	return errors.New("no rule found to delete")
+	var errs []error
+next:
+	for _, rule := range rules {
+		for _, filter := range filters {
+			if !filter.fn(rule) {
+				logger.Info("Skipping deletion of matching rule",
+					logfields.Rule, rule,
+					logfields.Message, filter.msg,
+				)
+				continue next
+			}
+		}
+		errs = append(errs, netlink.RuleDel(&rule))
+	}
+	return errors.Join(errs...)
 }
 
 // retrieveIfIndexFromMAC finds the corresponding device index (ifindex) for a
@@ -440,7 +453,10 @@ func retrieveIfIndexFromMAC(mac mac.MAC, mtu int) (int, error) {
 
 // computeTableIDFromIfaceNumber returns a computed per-ENI route table ID for the given
 // ENI interface number.
-func computeTableIDFromIfaceNumber(num int) int {
+func computeTableIDFromIfaceNumber(compat bool, num int) int {
+	if compat {
+		return num
+	}
 	return linux_defaults.RouteTableInterfacesOffset + num
 }
 
diff --git a/pkg/datapath/linux/routing/routing_test.go b/pkg/datapath/linux/routing/routing_test.go
index aee0dd9320..fb94531aae 100644
--- a/pkg/datapath/linux/routing/routing_test.go
+++ b/pkg/datapath/linux/routing/routing_test.go
@@ -97,7 +97,7 @@ func TestDelete(t *testing.T) {
 		wantErr bool
 	}{
 		{
-			name: "valid IP addr matching rules",
+			name: "valid IP addr matching a single rule",
 			preRun: func() netip.Addr {
 				runConfigure(t, fakeRoutingInfo, fakeIP, 1500)
 				return fakeIP
@@ -105,7 +105,7 @@ func TestDelete(t *testing.T) {
 			wantErr: false,
 		},
 		{
-			name: "IP addr doesn't match rules",
+			name: "IP addr doesn't match any rule",
 			preRun: func() netip.Addr {
 				ip := netip.MustParseAddr("192.168.2.233")
 
@@ -115,7 +115,7 @@ func TestDelete(t *testing.T) {
 			wantErr: true,
 		},
 		{
-			name: "IP addr matches more than number expected",
+			name: "IP addr matches multiple rules",
 			preRun: func() netip.Addr {
 				ip := netip.MustParseAddr("192.168.2.233")
 
@@ -130,7 +130,7 @@ func TestDelete(t *testing.T) {
 				require.NotEmpty(t, rules)
 
 				// Insert almost duplicate rule; the reason for this is to
-				// trigger an error while trying to delete the ingress rule. We
+				// trigger the deletion of all the matching rules. We
 				// are setting the Src because ingress rules don't have
 				// one (only Dst), thus we set Src to create a near-duplicate.
 				r := rules[0]
@@ -139,30 +139,31 @@ func TestDelete(t *testing.T) {
 
 				return ip
 			},
-			wantErr: true,
+			wantErr: false,
 		},
 		{
-			name: "fails to delete rules due to masquerade misconfiguration",
+			name: "delete rules with dest CIDR after masquerade is disabled",
 			preRun: func() netip.Addr {
 				runConfigure(t, fakeRoutingInfo, fakeIP, 1500)
-				// inconsistency with fakeRoutingInfo.Masquerade should lead to failure
 				option.Config.EnableIPv4Masquerade = false
 				return fakeIP
 			},
-			wantErr: true,
+			wantErr: false,
 		},
 	}
 	for _, tt := range tests {
-		t.Log("Test: " + tt.name)
-		ns := netns.NewNetNS(t)
-		ns.Do(func() error {
-			ifaceCleanup := createDummyDevice(t, masterMAC)
-			defer ifaceCleanup()
-
-			ip := tt.preRun()
-			err := Delete(hivetest.Logger(t), ip, false)
-			require.Equal(t, tt.wantErr, (err != nil))
-			return nil
+		t.Run(tt.name, func(t *testing.T) {
+			ns := netns.NewNetNS(t)
+			ns.Do(func() error {
+				ifaceCleanup := createDummyDevice(t, masterMAC)
+				defer ifaceCleanup()
+
+				ip := tt.preRun()
+				err := Delete(hivetest.Logger(t), ip, false)
+				require.Equalf(t, tt.wantErr, (err != nil), "got error: %v", err)
+
+				return nil
+			})
 		})
 	}
 }
diff --git a/pkg/datapath/linux/testdata/device-controller-tables.txtar b/pkg/datapath/linux/testdata/device-controller-tables.txtar
index aa6820491e..6351360ea7 100644
--- a/pkg/datapath/linux/testdata/device-controller-tables.txtar
+++ b/pkg/datapath/linux/testdata/device-controller-tables.txtar
@@ -7,9 +7,8 @@
 # - Because of the above, we can not rely on assignment of particular interface indexes to the newly created interfaces.
 #   Therefore we always omit the Index / DeviceIndex columns in all tables.
 
-# Start the hive and wait for tables to be synchronized.
+# Start the hive
 hive start
-db/initialized
 
 # Start with clean state.
 db/cmp devices --grep dummy devices_empty.table
@@ -154,4 +153,4 @@ IPAddr          HardwareAddr        Type   State   Flags   FlagsExt
 192.168.1.254   00:00:5e:00:53:02   1      0x80    0x10    0x0
 -- neighbors_dummy1_253.table --
 IPAddr          HardwareAddr        Type   State   Flags   FlagsExt
-192.168.1.253   00:00:5e:00:53:01   1      0x80    0x10    0x0
\ No newline at end of file
+192.168.1.253   00:00:5e:00:53:01   1      0x80    0x10    0x0
diff --git a/pkg/datapath/linux/testdata/device-detection-force.txtar b/pkg/datapath/linux/testdata/device-detection-force.txtar
index 6d2243ac07..533d31620a 100644
--- a/pkg/datapath/linux/testdata/device-detection-force.txtar
+++ b/pkg/datapath/linux/testdata/device-detection-force.txtar
@@ -2,9 +2,8 @@
 
 # Tests the behavior of device detection when forced detection is enabled.
 
-# Start the hive and wait for tables to be synchronized.
+# Start the hive
 hive start
-db/initialized
 
 # Add dummy0 interface - matches devices wildcard, should be selected.
 exec ip link add dummy0 type dummy
diff --git a/pkg/datapath/linux/testdata/device-detection-wildcard.txtar b/pkg/datapath/linux/testdata/device-detection-wildcard.txtar
index b84d2a7f6d..8632864989 100644
--- a/pkg/datapath/linux/testdata/device-detection-wildcard.txtar
+++ b/pkg/datapath/linux/testdata/device-detection-wildcard.txtar
@@ -3,9 +3,8 @@
 # Test that if the user specifies a device wildcard, then all devices not matching the wildcard
 # will be marked as non-selected.
 
-# Start the hive and wait for tables to be synchronized.
+# Start the hive
 hive start
-db/initialized
 
 # Add dummy0 interface - matches devices wildcard.
 exec ip link add dummy0 type dummy
diff --git a/pkg/datapath/linux/testdata/device-detection.txtar b/pkg/datapath/linux/testdata/device-detection.txtar
index 95d9c1f04d..e0e36a30fb 100644
--- a/pkg/datapath/linux/testdata/device-detection.txtar
+++ b/pkg/datapath/linux/testdata/device-detection.txtar
@@ -1,8 +1,7 @@
 # Tests the behavior of device detection without providing explicit --devices option.
 
-# Start the hive and wait for tables to be synchronized.
+# Start the hive
 hive start
-db/initialized
 
 # Create veth0 + veth1 interfaces without default route - should not be selected.
 exec ip link add veth0 type veth peer name veth1
diff --git a/pkg/datapath/loader/config.go b/pkg/datapath/loader/config.go
index 61fcb397e6..6342e30511 100644
--- a/pkg/datapath/loader/config.go
+++ b/pkg/datapath/loader/config.go
@@ -6,14 +6,17 @@ package loader
 import (
 	"github.com/cilium/cilium/pkg/datapath/config"
 	datapath "github.com/cilium/cilium/pkg/datapath/types"
-	"github.com/cilium/cilium/pkg/option"
 )
 
 func nodeConfig(lnc *datapath.LocalNodeConfiguration) config.Node {
 	node := *config.NewNode()
 
-	if option.Config.EnableIPv6 && lnc.CiliumInternalIPv6 != nil {
-		node.RouterIPv6 = ([16]byte)(lnc.CiliumInternalIPv6)
+	if lnc.ServiceLoopbackIPv4 != nil {
+		node.ServiceLoopbackIPv4 = [4]byte(lnc.ServiceLoopbackIPv4.To4())
+	}
+
+	if lnc.CiliumInternalIPv6 != nil {
+		node.RouterIPv6 = ([16]byte)(lnc.CiliumInternalIPv6.To16())
 	}
 
 	return node
diff --git a/pkg/datapath/loader/loader.go b/pkg/datapath/loader/loader.go
index 62f453cd82..fa7e58b04a 100644
--- a/pkg/datapath/loader/loader.go
+++ b/pkg/datapath/loader/loader.go
@@ -21,7 +21,6 @@ import (
 	"go4.org/netipx"
 
 	"github.com/cilium/cilium/pkg/bpf"
-	"github.com/cilium/cilium/pkg/byteorder"
 	"github.com/cilium/cilium/pkg/datapath/config"
 	"github.com/cilium/cilium/pkg/datapath/linux/linux_defaults"
 	"github.com/cilium/cilium/pkg/datapath/linux/route"
@@ -198,7 +197,7 @@ func netdevRewrites(ep datapath.EndpointConfiguration, lnc *datapath.LocalNodeCo
 		ipv4, ipv6 := bpfMasqAddrs(link.Attrs().Name, lnc)
 
 		if option.Config.EnableIPv4Masquerade && ipv4.IsValid() {
-			cfg.NATIPv4Masquerade = byteorder.NetIPv4ToHost32(ipv4.AsSlice())
+			cfg.NATIPv4Masquerade = ipv4.As4()
 		}
 		if option.Config.EnableIPv6Masquerade && ipv6.IsValid() {
 			cfg.NATIPv6Masquerade = ipv6.As16()
@@ -557,12 +556,12 @@ func attachNetworkDevices(logger *slog.Logger, ep datapath.Endpoint, lnc *datapa
 func endpointRewrites(ep datapath.EndpointConfiguration, lnc *datapath.LocalNodeConfiguration) (*config.BPFLXC, map[string]string) {
 	cfg := config.NewBPFLXC(nodeConfig(lnc))
 
+	if ep.IPv4Address().IsValid() {
+		cfg.EndpointIPv4 = ep.IPv4Address().As4()
+	}
 	if ep.IPv6Address().IsValid() {
 		cfg.EndpointIPv6 = ep.IPv6Address().As16()
 	}
-	if ipv4 := ep.IPv4Address().AsSlice(); ipv4 != nil {
-		cfg.EndpointIPv4 = byteorder.NetIPv4ToHost32(net.IP(ipv4))
-	}
 
 	// Netkit devices can be L2-less, meaning they operate with a zero MAC
 	// address. Unlike other L2-less devices, the ethernet header length remains
diff --git a/pkg/datapath/loader/util_test.go b/pkg/datapath/loader/util_test.go
index af3a9e0129..57a1729d55 100644
--- a/pkg/datapath/loader/util_test.go
+++ b/pkg/datapath/loader/util_test.go
@@ -19,12 +19,12 @@ import (
 
 var (
 	localNodeConfig = datapath.LocalNodeConfiguration{
-		NodeIPv4:           templateIPv4[:],
-		CiliumInternalIPv4: templateIPv4[:],
-		AllocCIDRIPv4:      cidr.MustParseCIDR("10.147.0.0/16"),
-		LoopbackIPv4:       templateIPv4[:],
-		HostEndpointID:     1,
-		EnableIPv4:         true,
+		NodeIPv4:            templateIPv4[:],
+		CiliumInternalIPv4:  templateIPv4[:],
+		AllocCIDRIPv4:       cidr.MustParseCIDR("10.147.0.0/16"),
+		ServiceLoopbackIPv4: templateIPv4[:],
+		HostEndpointID:      1,
+		EnableIPv4:          true,
 	}
 )
 
diff --git a/pkg/datapath/orchestrator/localnodeconfig.go b/pkg/datapath/orchestrator/localnodeconfig.go
index 94cdd16a44..8d87bca830 100644
--- a/pkg/datapath/orchestrator/localnodeconfig.go
+++ b/pkg/datapath/orchestrator/localnodeconfig.go
@@ -85,7 +85,7 @@ func newLocalNodeConfig(
 		AllocCIDRIPv6:                localNode.IPv6AllocCIDR,
 		NativeRoutingCIDRIPv4:        datapath.RemoteSNATDstAddrExclusionCIDRv4(localNode),
 		NativeRoutingCIDRIPv6:        datapath.RemoteSNATDstAddrExclusionCIDRv6(localNode),
-		LoopbackIPv4:                 node.GetIPv4Loopback(logger),
+		ServiceLoopbackIPv4:          node.GetServiceLoopbackIPv4(logger),
 		Devices:                      nativeDevices,
 		NodeAddresses:                statedb.Collect(nodeAddrsIter),
 		DirectRoutingDevice:          directRoutingDevice,
diff --git a/pkg/datapath/orchestrator/orchestrator.go b/pkg/datapath/orchestrator/orchestrator.go
index 147e0ad539..04430502a8 100644
--- a/pkg/datapath/orchestrator/orchestrator.go
+++ b/pkg/datapath/orchestrator/orchestrator.go
@@ -163,7 +163,7 @@ func (o *orchestrator) reconciler(ctx context.Context, health cell.Health) error
 		stream.Filter(o.params.LocalNodeStore,
 			func(n node.LocalNode) bool {
 				if agentConfig.EnableIPv4 {
-					loopback := n.IPv4Loopback != nil
+					loopback := n.ServiceLoopbackIPv4 != nil
 					ipv4GW := n.GetCiliumInternalIP(false) != nil
 					ipv4Range := n.IPv4AllocCIDR != nil
 					if !ipv4GW || !ipv4Range || !loopback {
diff --git a/pkg/datapath/types/node.go b/pkg/datapath/types/node.go
index 5f35cb4702..0ae0fd8620 100644
--- a/pkg/datapath/types/node.go
+++ b/pkg/datapath/types/node.go
@@ -64,9 +64,11 @@ type LocalNodeConfiguration struct {
 	// NativeRoutingCIDRIPv6 is the v4 CIDR in which pod IPs are routable.
 	NativeRoutingCIDRIPv6 *cidr.CIDR
 
-	// LoopbackIPv4 is the IPv4 loopback address.
+	// LoopbackIPv4 is the source address used for SNAT when a Pod talks to itself
+	// over a Service.
+	//
 	// Immutable at runtime.
-	LoopbackIPv4 net.IP
+	ServiceLoopbackIPv4 net.IP
 
 	// Devices is the native network devices selected for datapath use.
 	// Mutable at runtime.
diff --git a/pkg/datapath/types/zz_generated.deepequal.go b/pkg/datapath/types/zz_generated.deepequal.go
index 560fbdc288..7fb4b85def 100644
--- a/pkg/datapath/types/zz_generated.deepequal.go
+++ b/pkg/datapath/types/zz_generated.deepequal.go
@@ -115,8 +115,8 @@ func (in *LocalNodeConfiguration) DeepEqual(other *LocalNodeConfiguration) bool
 		}
 	}
 
-	if ((in.LoopbackIPv4 != nil) && (other.LoopbackIPv4 != nil)) || ((in.LoopbackIPv4 == nil) != (other.LoopbackIPv4 == nil)) {
-		in, other := &in.LoopbackIPv4, &other.LoopbackIPv4
+	if ((in.ServiceLoopbackIPv4 != nil) && (other.ServiceLoopbackIPv4 != nil)) || ((in.ServiceLoopbackIPv4 == nil) != (other.ServiceLoopbackIPv4 == nil)) {
+		in, other := &in.ServiceLoopbackIPv4, &other.ServiceLoopbackIPv4
 		if other == nil {
 			return false
 		}
diff --git a/pkg/defaults/defaults.go b/pkg/defaults/defaults.go
index 42d4b84262..b58689365c 100644
--- a/pkg/defaults/defaults.go
+++ b/pkg/defaults/defaults.go
@@ -322,8 +322,8 @@ const (
 	// connection tracking garbage collection
 	ConntrackGCStartingInterval = 5 * time.Minute
 
-	// LoopbackIPv4 is the default address for service loopback
-	LoopbackIPv4 = "169.254.42.1"
+	// ServiceLoopbackIPv4 is the default address for service loopback
+	ServiceLoopbackIPv4 = "169.254.42.1"
 
 	// EnableEndpointRoutes is the value for option.EnableEndpointRoutes.
 	// It is disabled by default for backwards compatibility.
diff --git a/pkg/dynamicconfig/script_test.go b/pkg/dynamicconfig/script_test.go
index 19af6617d0..81491d0ea6 100644
--- a/pkg/dynamicconfig/script_test.go
+++ b/pkg/dynamicconfig/script_test.go
@@ -22,6 +22,7 @@ import (
 	"github.com/cilium/cilium/pkg/k8s/client"
 	"github.com/cilium/cilium/pkg/k8s/testutils"
 	"github.com/cilium/cilium/pkg/k8s/version"
+	nodeTypes "github.com/cilium/cilium/pkg/node/types"
 	"github.com/cilium/cilium/pkg/time"
 )
 
@@ -31,6 +32,8 @@ func TestScript(t *testing.T) {
 	// Catch any leaked goroutines.
 	t.Cleanup(func() { goleak.VerifyNone(t) })
 
+	nodeTypes.SetName("testnode")
+
 	version.Force(testutils.DefaultVersion)
 	setup := func(t testing.TB, args []string) *script.Engine {
 		h := hive.New(
diff --git a/pkg/dynamicconfig/testdata/configmap.txtar b/pkg/dynamicconfig/testdata/configmap.txtar
index 8bf1f88353..fa0fc893a9 100644
--- a/pkg/dynamicconfig/testdata/configmap.txtar
+++ b/pkg/dynamicconfig/testdata/configmap.txtar
@@ -1,7 +1,6 @@
 # Test the reflection of the cilium-config ConfigMap
 
 hive/start
-db/initialized
 
 # Add cilium-config with keys foo and baz
 k8s/add cilium-config.yaml
diff --git a/pkg/dynamicconfig/testdata/node-config.txtar b/pkg/dynamicconfig/testdata/node-config.txtar
index 1a3b9f3461..6e35dbc128 100644
--- a/pkg/dynamicconfig/testdata/node-config.txtar
+++ b/pkg/dynamicconfig/testdata/node-config.txtar
@@ -2,7 +2,6 @@
 
 # Test the reflection of the CiliumNodeConfig
 hive/start
-db/initialized
 
 # Add cilium-config with keys foo and baz
 k8s/add cilium-node-config.yaml
@@ -38,20 +37,20 @@ k8s/delete cilium-node-config.yaml
 ####
 
 -- configs1.table --
-Key   Source          Priority   Value
-baz   cnc-foo         1          quux
-foo   cnc-foo         1          bar
+Key   Source      Priority   Value
+baz   foo         1          quux
+foo   foo         1          bar
 
 -- configs2.table --
-Key   Source          Priority   Value
-foo   cnc-foo         1          bar
+Key   Source      Priority   Value
+foo   foo         1          bar
 
 -- cilium-node-config.yaml --
 apiVersion: cilium.io/v2
 kind: CiliumNodeConfig
 metadata:
   namespace: kube-system
-  name: cnc-foo
+  name: foo
 spec:
   nodeSelector:
     matchLabels:
diff --git a/pkg/dynamicconfig/testdata/node.txtar b/pkg/dynamicconfig/testdata/node.txtar
index 6d7295be7d..793a454755 100644
--- a/pkg/dynamicconfig/testdata/node.txtar
+++ b/pkg/dynamicconfig/testdata/node.txtar
@@ -1,9 +1,8 @@
-#! --config-sources=[{"kind":"node","namespace":"kube-system","name":"foo"}]
+#! --config-sources=[{"kind":"node","name":"testnode"}]
 
 # Test the reflection of the configs as annotations in the Node
 # object.
 hive/start
-db/initialized
 
 # Add node with keys foo and baz as annotations
 k8s/add node.yaml
@@ -42,12 +41,12 @@ k8s/delete cilium-node-config.yaml
 
 -- configs1.table --
 Key   Source          Priority   Value
-baz   node-foo        1          quux
-foo   node-foo        1          bar
+baz   testnode        1          quux
+foo   testnode        1          bar
 
 -- configs2.table --
 Key   Source          Priority   Value
-foo   node-foo        1          bar
+foo   testnode        1          bar
 
 -- node.yaml --
 apiVersion: v1
@@ -59,6 +58,6 @@ metadata:
 
   labels:
     # Labels not relevant
-  name: node-foo
+  name: testnode
 spec:
   # Not relevant for this test
diff --git a/pkg/egressgateway/manager.go b/pkg/egressgateway/manager.go
index e0ece85c95..945348596c 100644
--- a/pkg/egressgateway/manager.go
+++ b/pkg/egressgateway/manager.go
@@ -11,12 +11,10 @@ import (
 	"log/slog"
 	"net/netip"
 	"slices"
-	"strings"
 	"sync"
 	"sync/atomic"
 
 	"github.com/cilium/hive/cell"
-	"github.com/sirupsen/logrus"
 	"github.com/spf13/pflag"
 	"k8s.io/apimachinery/pkg/util/sets"
 	"k8s.io/client-go/util/workqueue"
@@ -32,7 +30,6 @@ import (
 	k8sTypes "github.com/cilium/cilium/pkg/k8s/types"
 	"github.com/cilium/cilium/pkg/labels"
 	"github.com/cilium/cilium/pkg/lock"
-	"github.com/cilium/cilium/pkg/logging"
 	"github.com/cilium/cilium/pkg/logging/logfields"
 	"github.com/cilium/cilium/pkg/maps/egressmap"
 	nodeTypes "github.com/cilium/cilium/pkg/node/types"
@@ -42,7 +39,6 @@ import (
 )
 
 var (
-	log = logging.DefaultLogger.WithField(logfields.LogSubsys, "egressgateway")
 	// GatewayNotFoundIPv4 is a special IP value used as gatewayIP in the BPF policy
 	// map to indicate no gateway was found for the given policy
 	GatewayNotFoundIPv4 = netip.IPv4Unspecified()
@@ -238,8 +234,7 @@ func newEgressGatewayManager(p Params) (*Manager, error) {
 		Name:        "egress_gateway_reconciliation",
 		MinInterval: p.Config.EgressGatewayReconciliationTriggerInterval,
 		TriggerFunc: func(reasons []string) {
-			reason := strings.Join(reasons, ", ")
-			log.WithField(logfields.Reason, reason).Debug("reconciliation triggered")
+			manager.logger.Debug("reconciliation triggered", logfields.Reasons, reasons)
 
 			manager.Lock()
 			defer manager.Unlock()
@@ -394,11 +389,14 @@ func (manager *Manager) handlePolicyEvent(event resource.Event[*Policy]) {
 // onAddEgressPolicy parses the given policy config, and updates internal state
 // with the config fields.
 func (manager *Manager) onAddEgressPolicy(policy *Policy) error {
-	logger := log.WithField(logfields.CiliumEgressGatewayPolicyName, policy.Name)
 
 	config, err := ParseCEGP(policy)
 	if err != nil {
-		logger.WithError(err).Warn("Failed to parse CiliumEgressGatewayPolicy")
+		manager.logger.Warn(
+			"Failed to parse CiliumEgressGatewayPolicy",
+			logfields.Error, err,
+			logfields.CiliumEgressGatewayPolicyName, policy.Name,
+		)
 		return err
 	}
 
@@ -406,9 +404,15 @@ func (manager *Manager) onAddEgressPolicy(policy *Policy) error {
 	defer manager.Unlock()
 
 	if _, ok := manager.policyConfigs[config.id]; !ok {
-		logger.Debug("Added CiliumEgressGatewayPolicy")
+		manager.logger.Debug(
+			"Added CiliumEgressGatewayPolicy",
+			logfields.CiliumEgressGatewayPolicyName, policy.Name,
+		)
 	} else {
-		logger.Debug("Updated CiliumEgressGatewayPolicy")
+		manager.logger.Debug(
+			"Updated CiliumEgressGatewayPolicy",
+			logfields.CiliumEgressGatewayPolicyName, policy.Name,
+		)
 	}
 
 	config.updateMatchedEndpointIDs(manager.epDataStore, manager.nodesAddresses2Labels)
@@ -428,13 +432,17 @@ func (manager *Manager) onDeleteEgressPolicy(policy *Policy) {
 	manager.Lock()
 	defer manager.Unlock()
 
-	logger := log.WithField(logfields.CiliumEgressGatewayPolicyName, configID.Name)
-
 	if manager.policyConfigs[configID] == nil {
-		logger.Warn("Can't delete CiliumEgressGatewayPolicy: policy not found")
+		manager.logger.Warn(
+			"Can't delete CiliumEgressGatewayPolicy: policy not found",
+			logfields.CiliumEgressGatewayPolicyName, policy.Name,
+		)
 	}
 
-	logger.Debug("Deleted CiliumEgressGatewayPolicy")
+	manager.logger.Debug(
+		"Deleted CiliumEgressGatewayPolicy",
+		logfields.CiliumEgressGatewayPolicyName, policy.Name,
+	)
 
 	delete(manager.policyConfigs, configID)
 
@@ -450,33 +458,45 @@ func (manager *Manager) addEndpoint(endpoint *k8sTypes.CiliumEndpoint) error {
 	manager.Lock()
 	defer manager.Unlock()
 
-	logger := log.WithFields(logrus.Fields{
-		logfields.K8sEndpointName: endpoint.Name,
-		logfields.K8sNamespace:    endpoint.Namespace,
-		logfields.K8sUID:          endpoint.UID,
-	})
+	logger := manager.logger.With(
+		logfields.K8sEndpointName, endpoint.Name,
+		logfields.K8sNamespace, endpoint.Namespace,
+		logfields.K8sUID, endpoint.UID,
+	)
 
 	if endpoint.Identity == nil {
-		logger.Warning("Endpoint is missing identity metadata, skipping update to egress policy.")
+		logger.Warn(
+			"Endpoint is missing identity metadata, skipping update to egress policy.",
+		)
 		return nil
 	}
 
 	if identityLabels, err = manager.getIdentityLabels(uint32(endpoint.Identity.ID)); err != nil {
-		logger.WithError(err).
-			Warning("Failed to get identity labels for endpoint")
+		logger.Warn(
+			"Failed to get identity labels for endpoint",
+			logfields.Error, err,
+		)
 		return err
 	}
 
 	if epData, err = getEndpointMetadata(endpoint, identityLabels); err != nil {
-		logger.WithError(err).
-			Error("Failed to get valid endpoint metadata, skipping update to egress policy.")
+		logger.Error(
+			"Failed to get valid endpoint metadata, skipping update to egress policy.",
+			logfields.Error, err,
+		)
 		return nil
 	}
 
 	if _, ok := manager.epDataStore[epData.id]; ok {
-		logger.Debug("Updated CiliumEndpoint")
+		logger.Debug(
+			"Updated CiliumEndpoint",
+			logfields.Error, err,
+		)
 	} else {
-		logger.Debug("Added CiliumEndpoint")
+		logger.Debug(
+			"Added CiliumEndpoint",
+			logfields.Error, err,
+		)
 	}
 
 	manager.epDataStore[epData.id] = epData
@@ -491,13 +511,12 @@ func (manager *Manager) deleteEndpoint(endpoint *k8sTypes.CiliumEndpoint) {
 	manager.Lock()
 	defer manager.Unlock()
 
-	logger := log.WithFields(logrus.Fields{
-		logfields.K8sEndpointName: endpoint.Name,
-		logfields.K8sNamespace:    endpoint.Namespace,
-		logfields.K8sUID:          endpoint.UID,
-	})
-
-	logger.Debug("Deleted CiliumEndpoint")
+	manager.logger.Debug(
+		"Deleted CiliumEndpoint",
+		logfields.K8sEndpointName, endpoint.Name,
+		logfields.K8sNamespace, endpoint.Namespace,
+		logfields.K8sUID, endpoint.UID,
+	)
 	delete(manager.epDataStore, endpoint.UID)
 
 	manager.setEventBitmap(eventDeleteEndpoint)
@@ -634,17 +653,22 @@ func (manager *Manager) updateEgressRules4() {
 			return
 		}
 
-		logger := log.WithFields(logrus.Fields{
-			logfields.SourceIP:        endpointIP,
-			logfields.DestinationCIDR: dstCIDR.String(),
-			logfields.EgressIP:        gwc.egressIP4,
-			logfields.GatewayIP:       gatewayIP,
-		})
-
 		if err := manager.policyMap4.Update(endpointIP, dstCIDR, gwc.egressIP4, gatewayIP); err != nil {
-			logger.WithError(err).Error("Error applying IPv4 egress gateway policy")
+			manager.logger.Error(
+				"Error applying IPv4 egress gateway policy",
+				logfields.Error, err,
+				logfields.SourceIP, endpointIP,
+				logfields.DestinationCIDR, dstCIDR,
+				logfields.EgressIP, gwc.egressIP4,
+				logfields.GatewayIP, gatewayIP,
+			)
 		} else {
-			logger.Debug("IPv4 egress gateway policy applied")
+			manager.logger.Debug("IPv4 egress gateway policy applied",
+				logfields.SourceIP, endpointIP,
+				logfields.DestinationCIDR, dstCIDR,
+				logfields.EgressIP, gwc.egressIP4,
+				logfields.GatewayIP, gatewayIP,
+			)
 		}
 	}
 
@@ -654,15 +678,19 @@ func (manager *Manager) updateEgressRules4() {
 
 	// Remove all the entries marked as stale.
 	for policyKey := range stale {
-		logger := log.WithFields(logrus.Fields{
-			logfields.SourceIP:        policyKey.GetSourceIP(),
-			logfields.DestinationCIDR: policyKey.GetDestCIDR().String(),
-		})
-
 		if err := manager.policyMap4.Delete(policyKey.GetSourceIP(), policyKey.GetDestCIDR()); err != nil {
-			logger.WithError(err).Error("Error removing IPv4 egress gateway policy")
+			manager.logger.Error(
+				"Error removing IPv4 egress gateway policy",
+				logfields.Error, err,
+				logfields.SourceIP, policyKey.GetSourceIP(),
+				logfields.DestinationCIDR, policyKey.GetDestCIDR(),
+			)
 		} else {
-			logger.Debug("IPv4 egress gateway policy removed")
+			manager.logger.Debug(
+				"IPv4 egress gateway policy removed",
+				logfields.SourceIP, policyKey.GetSourceIP(),
+				logfields.DestinationCIDR, policyKey.GetDestCIDR(),
+			)
 		}
 	}
 }
@@ -704,17 +732,22 @@ func (manager *Manager) updateEgressRules6() {
 			return
 		}
 
-		logger := log.WithFields(logrus.Fields{
-			logfields.SourceIP:        endpointIP,
-			logfields.DestinationCIDR: dstCIDR.String(),
-			logfields.EgressIP:        gwc.egressIP6,
-			logfields.GatewayIP:       gatewayIP,
-		})
-
 		if err := manager.policyMap6.Update(endpointIP, dstCIDR, gwc.egressIP6, gatewayIP); err != nil {
-			logger.WithError(err).Error("Error applying IPv6 egress gateway policy")
+			manager.logger.Error(
+				"Error applying IPv6 egress gateway policy",
+				logfields.Error, err,
+				logfields.SourceIP, endpointIP,
+				logfields.DestinationCIDR, dstCIDR,
+				logfields.EgressIP, gwc.egressIP6,
+				logfields.GatewayIP, gatewayIP,
+			)
 		} else {
-			logger.Debug("IPv6 egress gateway policy applied")
+			manager.logger.Debug("IPv6 egress gateway policy applied",
+				logfields.SourceIP, endpointIP,
+				logfields.DestinationCIDR, dstCIDR,
+				logfields.EgressIP, gwc.egressIP6,
+				logfields.GatewayIP, gatewayIP,
+			)
 		}
 	}
 
@@ -723,15 +756,19 @@ func (manager *Manager) updateEgressRules6() {
 	}
 
 	for policyKey := range stale {
-		logger := log.WithFields(logrus.Fields{
-			logfields.SourceIP:        policyKey.GetSourceIP(),
-			logfields.DestinationCIDR: policyKey.GetDestCIDR().String(),
-		})
-
 		if err := manager.policyMap6.Delete(policyKey.GetSourceIP(), policyKey.GetDestCIDR()); err != nil {
-			logger.WithError(err).Error("Error removing IPv6 egress gateway policy")
+			manager.logger.Error(
+				"Error removing IPv6 egress gateway policy",
+				logfields.Error, err,
+				logfields.SourceIP, policyKey.GetSourceIP(),
+				logfields.DestinationCIDR, policyKey.GetDestCIDR(),
+			)
 		} else {
-			logger.Debug("IPv6 egress gateway policy removed")
+			manager.logger.Debug(
+				"IPv6 egress gateway policy removed",
+				logfields.SourceIP, policyKey.GetSourceIP(),
+				logfields.DestinationCIDR, policyKey.GetDestCIDR(),
+			)
 		}
 	}
 }
@@ -766,9 +803,12 @@ func (manager *Manager) reconcileLocked() {
 		// Therefore, for the sake of resiliency, it is acceptable for EGW to continue reconciling gatewayConfigs
 		// even if the rp_filter setting are failing.
 		if err := manager.relaxRPFilter(); err != nil {
-			log.WithError(err).Error("Error relaxing rp_filter for gateway interfaces. "+
-				"Selected egress gateway interfaces require rp_filter settings to use loose mode (rp_filter=2) for gateway forwarding to work correctly. ",
-				"This may cause connectivity issues for egress gateway traffic being forwarded through this node for Pods running on the same host. ")
+			manager.logger.Error(
+				"Error relaxing rp_filter for gateway interfaces. "+
+					"Selected egress gateway interfaces require rp_filter settings to use loose mode (rp_filter=2) for gateway forwarding to work correctly. "+
+					"This may cause connectivity issues for egress gateway traffic being forwarded through this node for Pods running on the same host. ",
+				logfields.Error, err,
+			)
 		}
 	}
 
diff --git a/pkg/endpoint/bpf.go b/pkg/endpoint/bpf.go
index af02efd191..3fd60b89cf 100644
--- a/pkg/endpoint/bpf.go
+++ b/pkg/endpoint/bpf.go
@@ -450,6 +450,7 @@ func (e *Endpoint) regenerateBPF(regenContext *regenerationContext) (revnum uint
 	dir := datapathRegenCtxt.currentDir
 	if datapathRegenCtxt.regenerationLevel >= regeneration.RegenerateWithDatapath {
 		if err := e.writeHeaderfile(datapathRegenCtxt.nextDir); err != nil {
+			e.unlock()
 			return 0, fmt.Errorf("write endpoint header file: %w", err)
 		}
 		dir = datapathRegenCtxt.nextDir
@@ -469,12 +470,14 @@ func (e *Endpoint) regenerateBPF(regenContext *regenerationContext) (revnum uint
 		// Ingress endpoint needs entries in the endpoints map so that the return traffic,
 		// ARP, and IPv6 ND are delivered to the host stack in all datapath configurations.
 		if e.isProperty(PropertyAtHostNS) {
+			mapSyncSuccess := false
 			stats.mapSync.Start()
+			defer func() { stats.mapSync.End(mapSyncSuccess) }()
 			err = lxcmap.WriteEndpoint(datapathRegenCtxt.epInfoCache)
-			stats.mapSync.End(err == nil)
 			if err != nil {
 				return 0, fmt.Errorf("Exposing endpoint in endpoints BPF map failed: %w", err)
 			}
+			mapSyncSuccess = true
 		}
 
 		// Allow another builder to start while we wait for the proxy
@@ -482,35 +485,41 @@ func (e *Endpoint) regenerateBPF(regenContext *regenerationContext) (revnum uint
 			regenContext.DoneFunc()
 		}
 
+		proxyWaitForAckSuccess := false
 		stats.proxyWaitForAck.Start()
+		defer func() { stats.proxyWaitForAck.End(proxyWaitForAckSuccess) }()
 		err = e.waitForProxyCompletions(datapathRegenCtxt.proxyWaitGroup)
-		stats.proxyWaitForAck.End(err == nil)
 		if err != nil {
 			return 0, fmt.Errorf("Error while updating network policy: %w", err)
 		}
+		proxyWaitForAckSuccess = true
 
 		return e.nextPolicyRevision, nil
 	}
 
 	// Wait for connection tracking cleaning to complete
+	waitingForCTCleanSuccess := false
 	stats.waitingForCTClean.Start()
+	defer func() { stats.waitingForCTClean.End(waitingForCTCleanSuccess) }()
 	<-datapathRegenCtxt.ctCleaned
-	stats.waitingForCTClean.End(true)
+	waitingForCTCleanSuccess = true
 
 	err = e.realizeBPFState(regenContext)
 	if err != nil {
 		return datapathRegenCtxt.epInfoCache.revision, err
 	}
 
+	mapSyncSuccess := false
+	stats.mapSync.Start()
+	defer func() { stats.mapSync.End(mapSyncSuccess) }()
 	if !datapathRegenCtxt.epInfoCache.IsHost() || option.Config.EnableHostFirewall {
 		// Hook the endpoint into the endpoint and endpoint to policy tables then expose it
-		stats.mapSync.Start()
 		err = lxcmap.WriteEndpoint(datapathRegenCtxt.epInfoCache)
-		stats.mapSync.End(err == nil)
 		if err != nil {
 			return 0, fmt.Errorf("Exposing new BPF failed: %w", err)
 		}
 	}
+	mapSyncSuccess = true // Assuming success if WriteEndpoint doesn't error
 
 	// Signal that BPF program has been generated.
 	// The endpoint has at least L3/L4 connectivity at this point.
@@ -521,12 +530,14 @@ func (e *Endpoint) regenerateBPF(regenContext *regenerationContext) (revnum uint
 		regenContext.DoneFunc()
 	}
 
+	proxyWaitForAckSuccess := false
 	stats.proxyWaitForAck.Start()
+	defer func() { stats.proxyWaitForAck.End(proxyWaitForAckSuccess) }()
 	err = e.waitForProxyCompletions(datapathRegenCtxt.proxyWaitGroup)
-	stats.proxyWaitForAck.End(err == nil)
 	if err != nil {
 		return 0, fmt.Errorf("error while configuring proxy redirects: %w", err)
 	}
+	proxyWaitForAckSuccess = true
 
 	stats.waitingForLock.Start()
 	err = e.lockAlive()
@@ -539,10 +550,14 @@ func (e *Endpoint) regenerateBPF(regenContext *regenerationContext) (revnum uint
 	e.ctCleaned = true
 
 	if !datapathRegenCtxt.policyMapSyncDone {
+		mapSyncSuccess := false
+		stats.mapSync.Start()
+		defer func() { stats.mapSync.End(mapSyncSuccess) }()
 		err = e.policyMapSync(datapathRegenCtxt.policyMapDump, stats)
 		if err != nil {
 			return 0, fmt.Errorf("unable to regenerate policy because PolicyMap synchronization failed: %w", err)
 		}
+		mapSyncSuccess = true
 		datapathRegenCtxt.policyMapSyncDone = true
 	}
 
@@ -567,7 +582,7 @@ func (e *Endpoint) regenerateBPF(regenContext *regenerationContext) (revnum uint
 // Sync is done against 'policyMapDump' if non-empty, otherwise it is done against e.realizedPolicy
 // e.mutex must be held!
 func (e *Endpoint) policyMapSync(policyMapDump policy.MapStateMap, stats *regenerationStatistics) (err error) {
-	stats.mapSync.Start()
+	// stats.mapSync is started by the caller, regenerateBPF
 	// Nothing to do if the desired policy is already fully realized.
 	if e.realizedPolicy != e.desiredPolicy {
 		if len(policyMapDump) > 0 {
@@ -576,7 +591,6 @@ func (e *Endpoint) policyMapSync(policyMapDump policy.MapStateMap, stats *regene
 			err = e.syncPolicyMap()
 		}
 	}
-	stats.mapSync.End(err == nil)
 	return err
 }
 
@@ -603,6 +617,7 @@ func (e *Endpoint) realizeBPFState(regenContext *regenerationContext) (err error
 		}
 
 		// Compile and install BPF programs for this endpoint
+		// stats.datapathRealization is passed directly to ReloadDatapath
 		templateHash, err := e.orchestrator.ReloadDatapath(datapathRegenCtxt.completionCtx, datapathRegenCtxt.epInfoCache, &stats.datapathRealization)
 		if err != nil {
 			if !errors.Is(err, context.Canceled) {
@@ -666,12 +681,14 @@ func (e *Endpoint) runPreCompilationSteps(regenContext *regenerationContext) (pr
 		datapathRegenCtxt.policyResult.endpointPolicy == nil ||
 		datapathRegenCtxt.policyResult.identityRevision < identityRevision ||
 		datapathRegenCtxt.policyResult.policyRevision < policyRevision {
+		policyCalculationSuccess := false
 		stats.policyCalculation.Start()
+		defer func() { stats.policyCalculation.End(policyCalculationSuccess) }()
 		err := e.regeneratePolicy(stats, datapathRegenCtxt)
-		stats.policyCalculation.End(err == nil)
 		if err != nil {
 			return fmt.Errorf("unable to regenerate policy for '%s': %w", e.StringID(), err)
 		}
+		policyCalculationSuccess = true
 	}
 
 	// Once the policy has been calculated, we can update the standalone dns proxy as well.
@@ -802,20 +819,28 @@ func (e *Endpoint) runPreCompilationSteps(regenContext *regenerationContext) (pr
 		// Sync policy map before bpf compilation if the bpf policymap is empty.
 		// This allows for upgrades and downgrades from versions using a different policy map
 		if len(datapathRegenCtxt.policyMapDump) == 0 {
+			mapSyncSuccess := false
+			stats.mapSync.Start()
+			defer func() { stats.mapSync.End(mapSyncSuccess) }()
 			err = e.policyMapSync(nil, stats)
 			if err != nil {
 				return fmt.Errorf("policymap synchronization failed: %w", err)
 			}
+			mapSyncSuccess = true
 		}
 		datapathRegenCtxt.policyMapSyncDone = true
 	}
 
 	// sync policy map for fake endpoints, bpf compilation will be skipped for them.
 	if e.isProperty(PropertyFakeEndpoint) {
+		mapSyncSuccess := false
+		stats.mapSync.Start()
+		defer func() { stats.mapSync.End(mapSyncSuccess) }()
 		err = e.policyMapSync(nil, stats)
 		if err != nil {
 			return fmt.Errorf("fake ep policymap synchronization failed: %w", err)
 		}
+		mapSyncSuccess = true
 	}
 
 	if e.isProperty(PropertySkipBPFRegeneration) {
@@ -1182,12 +1207,14 @@ func (e *Endpoint) applyPolicyMapChangesLocked(regenContext *regenerationContext
 				"applyPolicyMapChanges: Updating Envoy NetworkPolicy",
 				logfields.SelectorCacheVersion, e.desiredPolicy.VersionHandle,
 			)
+			proxyPolicyCalculationSuccess := false
 			stats.proxyPolicyCalculation.Start()
+			defer func() { stats.proxyPolicyCalculation.End(proxyPolicyCalculationSuccess) }()
 			var rf revert.RevertFunc
 			err, rf = e.proxy.UpdateNetworkPolicy(e, &e.desiredPolicy.L4Policy, e.desiredPolicy.IngressPolicyEnabled, e.desiredPolicy.EgressPolicyEnabled, proxyWaitGroup)
-			stats.proxyPolicyCalculation.End(err == nil)
 			if err == nil {
 				datapathRegenCtxt.revertStack.Push(rf)
+				proxyPolicyCalculationSuccess = true
 			}
 		} else if hasEnvoyRedirect {
 			// Wait for a possible ongoing update to be done if there were no current changes.
diff --git a/pkg/endpoint/endpoint.go b/pkg/endpoint/endpoint.go
index 8351ee12c3..3b768944dd 100644
--- a/pkg/endpoint/endpoint.go
+++ b/pkg/endpoint/endpoint.go
@@ -1631,6 +1631,10 @@ func (e *Endpoint) GetPolicyVersionHandle() *versioned.VersionHandle {
 	return nil
 }
 
+func (e *Endpoint) GetListenerProxyPort(listener string) uint16 {
+	return e.proxy.GetListenerProxyPort(listener)
+}
+
 // getProxyStatistics gets the ProxyStatistics for the flows with the
 // given characteristics, or adds a new one and returns it.
 func (e *Endpoint) getProxyStatistics(key string, l7Protocol string, port uint16, ingress bool, redirectPort uint16) *models.ProxyStatistics {
@@ -2537,8 +2541,7 @@ func (e *Endpoint) Delete(conf DeleteConfig) []error {
 
 		// This is a best-effort attempt to cleanup. We expect there to be one
 		// ingress rule and multiple egress rules. If we find more rules than
-		// expected, then the rules will be left as-is because there was
-		// likely manual intervention.
+		// expected, we delete all rules referring to a per-ENI routing table ID.
 		if e.IPv4.IsValid() {
 			if err := linuxrouting.Delete(e.getLogger(), e.IPv4, option.Config.EgressMultiHomeIPRuleCompat); err != nil {
 				errs = append(errs, fmt.Errorf("unable to delete endpoint routing rules: %w", err))
diff --git a/pkg/endpoint/policy.go b/pkg/endpoint/policy.go
index 3cfa0ff804..d0d27f0558 100644
--- a/pkg/endpoint/policy.go
+++ b/pkg/endpoint/policy.go
@@ -245,9 +245,11 @@ func (e *Endpoint) regeneratePolicy(stats *regenerationStatistics, datapathRegen
 	}
 	// Ingress endpoint needs no redirects
 	if !e.isProperty(PropertySkipBPFPolicy) {
+		proxyConfigurationSuccess := true // Assuming success unless addNewRedirects errors, though it doesn't return error
 		stats.proxyConfiguration.Start()
 		desiredRedirects, rf = e.addNewRedirects(selectorPolicy, datapathRegenCtxt.proxyWaitGroup)
-		stats.proxyConfiguration.End(true)
+		// TODO: Determine actual success of addNewRedirects if possible, for now assuming true
+		stats.proxyConfiguration.End(proxyConfigurationSuccess)
 		datapathRegenCtxt.revertStack.Push(rf)
 
 		// Add a finalize function to clear out stale redirects. This will be called after
@@ -267,9 +269,10 @@ func (e *Endpoint) regeneratePolicy(stats *regenerationStatistics, datapathRegen
 	e.runlock()
 
 	// DistillPolicy converts a SelectorPolicy in to an EndpointPolicy
+	endpointPolicyCalculationSuccess := true // Assuming success, DistillPolicy doesn't return error
 	stats.endpointPolicyCalculation.Start()
 	result.endpointPolicy = selectorPolicy.DistillPolicy(logging.DefaultSlogLogger, e, desiredRedirects)
-	stats.endpointPolicyCalculation.End(true)
+	stats.endpointPolicyCalculation.End(endpointPolicyCalculationSuccess)
 
 	datapathRegenCtxt.policyResult = result
 	return nil
@@ -379,6 +382,8 @@ func (e *Endpoint) regenerate(ctx *regenerationContext) (retErr error) {
 	ctx.Stats = regenerationStatistics{}
 	stats := &ctx.Stats
 	stats.totalTime.Start()
+	// placeholder for actual success value
+	defer func() { stats.totalTime.End(retErr == nil) }()
 	debugLogsEnabled := e.getLogger().Enabled(context.Background(), slog.LevelDebug)
 
 	if debugLogsEnabled {
@@ -392,6 +397,8 @@ func (e *Endpoint) regenerate(ctx *regenerationContext) (retErr error) {
 	defer func() {
 		// This has to be within a func(), not deferred directly, so that the
 		// value of retErr is passed in from when regenerate returns.
+		// The success flag for totalTime is set above.
+		ctx.Stats.success = retErr == nil
 		e.updateRegenerationStatistics(ctx, retErr)
 	}()
 
@@ -431,7 +438,6 @@ func (e *Endpoint) regenerate(ctx *regenerationContext) (retErr error) {
 
 	e.unlock()
 
-	stats.prepareBuild.Start()
 	origDir := e.StateDirectoryPath()
 	ctx.datapathRegenerationContext.currentDir = origDir
 
@@ -443,18 +449,19 @@ func (e *Endpoint) regenerate(ctx *regenerationContext) (retErr error) {
 
 	// Remove an eventual existing temporary directory that has been left
 	// over to make sure we can start the build from scratch
+	prepareBuildSuccess := false
+	stats.prepareBuild.Start()
+	defer func() { stats.prepareBuild.End(prepareBuildSuccess) }()
 	if err := e.removeDirectory(tmpDir); err != nil && !os.IsNotExist(err) {
-		stats.prepareBuild.End(false)
 		return fmt.Errorf("unable to remove old temporary directory: %w", err)
 	}
 
 	// Create temporary endpoint directory if it does not exist yet
 	if err := os.MkdirAll(tmpDir, 0777); err != nil {
-		stats.prepareBuild.End(false)
 		return fmt.Errorf("Failed to create endpoint directory: %w", err)
 	}
 
-	stats.prepareBuild.End(true)
+	prepareBuildSuccess = true
 
 	defer func() {
 		if err := e.lockAlive(); err != nil {
@@ -567,12 +574,10 @@ func (e *Endpoint) updateRealizedState(stats *regenerationStatistics, origDir st
 }
 
 func (e *Endpoint) updateRegenerationStatistics(ctx *regenerationContext, err error) {
-	success := err == nil
+	// success field for stats.totalTime is set in the defer func in regenerate()
+	// success field for ctx.Stats is set in the defer func in regenerate()
 	stats := &ctx.Stats
 
-	stats.totalTime.End(success)
-	stats.success = success
-
 	e.mutex.RLock()
 	stats.endpointID = e.ID
 	stats.policyStatus = e.policyStatus()
@@ -859,9 +864,11 @@ func (e *Endpoint) ComputeInitialPolicy(regenContext *regenerationContext) (erro
 	defer e.buildMutex.Unlock()
 
 	// Compute Endpoint's policy
+	var err error
+	policyCalculationSuccess := false
 	stats.policyCalculation.Start()
-	err := e.regeneratePolicy(stats, datapathRegenCtxt)
-	stats.policyCalculation.End(err == nil)
+	defer func() { stats.policyCalculation.End(policyCalculationSuccess) }()
+	err = e.regeneratePolicy(stats, datapathRegenCtxt)
 	if err != nil {
 		if !errors.Is(err, context.Canceled) {
 			e.getLogger().Warn(
@@ -872,6 +879,7 @@ func (e *Endpoint) ComputeInitialPolicy(regenContext *regenerationContext) (erro
 		// Do not error out so that the policy regeneration is tried again.
 		return nil, func() {}
 	}
+	policyCalculationSuccess = true
 
 	err = e.lockAlive()
 	if err != nil {
@@ -905,10 +913,11 @@ func (e *Endpoint) ComputeInitialPolicy(regenContext *regenerationContext) (erro
 			logfields.SelectorCacheVersion, e.desiredPolicy.VersionHandle,
 		)
 
+		proxyPolicyCalculationSuccess := false
 		stats.proxyPolicyCalculation.Start()
+		defer func() { stats.proxyPolicyCalculation.End(proxyPolicyCalculationSuccess) }()
 		// Initial NetworkPolicy is not reverted
 		err, _ = e.proxy.UpdateNetworkPolicy(e, &e.desiredPolicy.L4Policy, e.desiredPolicy.IngressPolicyEnabled, e.desiredPolicy.EgressPolicyEnabled, nil)
-		stats.proxyPolicyCalculation.End(err == nil)
 		if err != nil {
 			e.getLogger().Warn(
 				"Initial Envoy NetworkPolicy failed",
@@ -917,6 +926,7 @@ func (e *Endpoint) ComputeInitialPolicy(regenContext *regenerationContext) (erro
 			// Do not error out so that the policy regeneration is tried again.
 			return nil, release
 		}
+		proxyPolicyCalculationSuccess = true
 	}
 
 	// Signal computation of the initial Envoy policy if not done yet
@@ -942,7 +952,7 @@ func (e *Endpoint) startRegenerationFailureHandler() {
 				e.getLogger().Debug("received signal that regeneration failed")
 			case <-ctx.Done():
 				e.getLogger().Debug("exiting retrying regeneration goroutine due to endpoint being deleted")
-				return nil
+				return controller.NewExitReason("endpoint being deleted")
 			}
 
 			regenMetadata := &regeneration.ExternalRegenerationMetadata{
@@ -952,7 +962,10 @@ func (e *Endpoint) startRegenerationFailureHandler() {
 				// of the failure, simply that something failed.
 				RegenerationLevel: regeneration.RegenerateWithDatapath,
 			}
-			regen, _ := e.SetRegenerateStateIfAlive(regenMetadata)
+			regen, err := e.SetRegenerateStateIfAlive(regenMetadata)
+			if err != nil {
+				return controller.NewExitReason("endpoint being deleted")
+			}
 			if !regen {
 				// We don't need to regenerate because the endpoint is d
 				// disconnecting / is disconnected, or another regeneration has
diff --git a/pkg/endpoint/proxy.go b/pkg/endpoint/proxy.go
index 7aa59723d6..5959267d71 100644
--- a/pkg/endpoint/proxy.go
+++ b/pkg/endpoint/proxy.go
@@ -22,6 +22,7 @@ type EndpointProxy interface {
 	UpdateNetworkPolicy(ep endpoint.EndpointUpdater, policy *policy.L4Policy, ingressPolicyEnforced, egressPolicyEnforced bool, wg *completion.WaitGroup) (error, func() error)
 	UseCurrentNetworkPolicy(ep endpoint.EndpointUpdater, policy *policy.L4Policy, wg *completion.WaitGroup)
 	RemoveNetworkPolicy(ep endpoint.EndpointInfoSource)
+	GetListenerProxyPort(listener string) uint16
 }
 
 func (e *Endpoint) removeNetworkPolicy() {
@@ -61,3 +62,8 @@ func (f *FakeEndpointProxy) RemoveNetworkPolicy(ep endpoint.EndpointInfoSource)
 
 func (f *FakeEndpointProxy) UpdateSDP(rules map[identity.NumericIdentity]policy.SelectorPolicy) {
 }
+
+// GetListenerProxyPort does nothing.
+func (f *FakeEndpointProxy) GetListenerProxyPort(listener string) uint16 {
+	return 0
+}
diff --git a/pkg/endpoint/redirect_test.go b/pkg/endpoint/redirect_test.go
index dd72557aca..c37cc867b6 100644
--- a/pkg/endpoint/redirect_test.go
+++ b/pkg/endpoint/redirect_test.go
@@ -124,6 +124,11 @@ func (r *RedirectSuiteProxy) RemoveNetworkPolicy(ep endpoint.EndpointInfoSource)
 func (r *RedirectSuiteProxy) UpdateSDP(rules map[identity.NumericIdentity]policy.SelectorPolicy) {
 }
 
+// GetListenerProxyPort does nothing.
+func (r *RedirectSuiteProxy) GetListenerProxyPort(listener string) uint16 {
+	return 0
+}
+
 // DummyIdentityAllocatorOwner implements
 // pkg/identity/cache/IdentityAllocatorOwner. It is used for unit testing.
 type DummyIdentityAllocatorOwner struct{}
diff --git a/pkg/envoy/xds_server.go b/pkg/envoy/xds_server.go
index d927501383..7b8f8f2023 100644
--- a/pkg/envoy/xds_server.go
+++ b/pkg/envoy/xds_server.go
@@ -1291,7 +1291,7 @@ func namespacedNametoSyncedSDSSecretName(namespacedName types.NamespacedName, po
 	return fmt.Sprintf("%s/%s-%s", policySecretsNamespace, namespacedName.Namespace, namespacedName.Name)
 }
 
-func (s *xdsServer) getPortNetworkPolicyRule(version *versioned.VersionHandle, sel policy.CachedSelector, l7Rules *policy.PerSelectorPolicy, useFullTLSContext, useSDS bool, policySecretsNamespace string) (*cilium.PortNetworkPolicyRule, bool) {
+func (s *xdsServer) getPortNetworkPolicyRule(ep endpoint.EndpointUpdater, version *versioned.VersionHandle, sel policy.CachedSelector, l7Rules *policy.PerSelectorPolicy, useFullTLSContext, useSDS bool, policySecretsNamespace string) (*cilium.PortNetworkPolicyRule, bool) {
 	wildcard := sel.IsWildcard()
 	r := &cilium.PortNetworkPolicyRule{}
 
@@ -1318,6 +1318,13 @@ func (s *xdsServer) getPortNetworkPolicyRule(version *versioned.VersionHandle, s
 		return r, false
 	}
 
+	// Pass redirect port as proxy ID if the rule has an explicit listener reference.
+	// This makes this rule to be ignored on any listener that does not have a matching
+	// proxy ID.
+	if l7Rules.Listener != "" {
+		r.ProxyId = uint32(ep.GetListenerProxyPort(l7Rules.Listener))
+	}
+
 	// If secret synchronization is disabled, policySecretsNamespace will be the empty string.
 	//
 	// In that case, useFullTLSContext is used to retain an old, buggy behavior where Secrets may contain a `ca.crt` field as well,
@@ -1521,7 +1528,7 @@ func (s *xdsServer) getDirectionNetworkPolicy(ep endpoint.EndpointUpdater, l4Pol
 			}
 		} else {
 			for sel, l7 := range l4.PerSelectorPolicies {
-				rule, cs := s.getPortNetworkPolicyRule(version, sel, l7, useFullTLSContext, useSDS, policySecretsNamespace)
+				rule, cs := s.getPortNetworkPolicyRule(ep, version, sel, l7, useFullTLSContext, useSDS, policySecretsNamespace)
 				if rule != nil {
 					if !cs {
 						canShortCircuit = false
@@ -1532,11 +1539,12 @@ func (s *xdsServer) getDirectionNetworkPolicy(ep endpoint.EndpointUpdater, l4Pol
 						logfields.Version, version,
 						logfields.TrafficDirection, dir,
 						logfields.Port, port,
+						logfields.ProxyPort, rule.ProxyId,
 						logfields.PolicyID, rule.RemotePolicies,
 						logfields.ServerNames, rule.ServerNames,
 					)
 
-					if len(rule.RemotePolicies) == 0 && rule.L7 == nil && rule.DownstreamTlsContext == nil && rule.UpstreamTlsContext == nil && len(rule.ServerNames) == 0 {
+					if len(rule.RemotePolicies) == 0 && rule.L7 == nil && rule.DownstreamTlsContext == nil && rule.UpstreamTlsContext == nil && len(rule.ServerNames) == 0 && rule.ProxyId == 0 {
 						// Got an allow-all rule, which can short-circuit all of
 						// the other rules.
 						allowAll = true
diff --git a/pkg/envoy/xds_server_test.go b/pkg/envoy/xds_server_test.go
index d743ef2a18..de69971f66 100644
--- a/pkg/envoy/xds_server_test.go
+++ b/pkg/envoy/xds_server_test.go
@@ -533,15 +533,15 @@ func TestGetPortNetworkPolicyRule(t *testing.T) {
 	xds := testXdsServer(t)
 
 	version := versioned.Latest()
-	obtained, canShortCircuit := xds.getPortNetworkPolicyRule(version, cachedSelector1, L7Rules12, false, false, "")
+	obtained, canShortCircuit := xds.getPortNetworkPolicyRule(ep, version, cachedSelector1, L7Rules12, false, false, "")
 	require.Equal(t, ExpectedPortNetworkPolicyRule12, obtained)
 	require.True(t, canShortCircuit)
 
-	obtained, canShortCircuit = xds.getPortNetworkPolicyRule(version, cachedSelector1, L7Rules12HeaderMatch, false, false, "")
+	obtained, canShortCircuit = xds.getPortNetworkPolicyRule(ep, version, cachedSelector1, L7Rules12HeaderMatch, false, false, "")
 	require.Equal(t, ExpectedPortNetworkPolicyRule122HeaderMatch, obtained)
 	require.False(t, canShortCircuit)
 
-	obtained, canShortCircuit = xds.getPortNetworkPolicyRule(version, cachedSelector2, L7Rules1, false, false, "")
+	obtained, canShortCircuit = xds.getPortNetworkPolicyRule(ep, version, cachedSelector2, L7Rules1, false, false, "")
 	require.Equal(t, ExpectedPortNetworkPolicyRule1, obtained)
 	require.True(t, canShortCircuit)
 }
diff --git a/pkg/fswatcher/fswatcher.go b/pkg/fswatcher/fswatcher.go
index c6a901eba8..e5f9280b90 100644
--- a/pkg/fswatcher/fswatcher.go
+++ b/pkg/fswatcher/fswatcher.go
@@ -6,19 +6,16 @@ package fswatcher
 import (
 	"errors"
 	"fmt"
+	"log/slog"
 	"os"
 	"path/filepath"
 
 	"github.com/fsnotify/fsnotify"
-	"github.com/sirupsen/logrus"
 
 	"github.com/cilium/cilium/pkg/counter"
-	"github.com/cilium/cilium/pkg/logging"
 	"github.com/cilium/cilium/pkg/logging/logfields"
 )
 
-var log = logging.DefaultLogger.WithField(logfields.LogSubsys, "fswatcher")
-
 // Event currently wraps fsnotify.Event
 type Event fsnotify.Event
 
@@ -49,6 +46,8 @@ type Event fsnotify.Event
 // itself does not emit an event. Only if the target of the symlink observes
 // an event is the symlink re-evaluated.
 type Watcher struct {
+	logger *slog.Logger
+
 	watcher *fsnotify.Watcher
 
 	// Internally, we distinguish between
@@ -70,13 +69,14 @@ type Watcher struct {
 
 // New creates a new Watcher which watches all trackedFile paths (they do not
 // need to exist yet).
-func New(trackedFiles []string) (*Watcher, error) {
+func New(defaultLogger *slog.Logger, trackedFiles []string) (*Watcher, error) {
 	watcher, err := fsnotify.NewWatcher()
 	if err != nil {
 		return nil, err
 	}
 
 	w := &Watcher{
+		logger:               defaultLogger.With(logfields.LogSubsys, "fswatcher"),
 		watcher:              watcher,
 		watchedPathCount:     counter.Counter[string]{},
 		trackedToWatchedPath: map[string]string{},
@@ -209,11 +209,11 @@ func (w *Watcher) loop() {
 	for {
 		select {
 		case event := <-w.watcher.Events:
-			scopedLog := log.WithFields(logrus.Fields{
-				logfields.Path: event.Name,
-				"operation":    event.Op,
-			})
-			scopedLog.Debug("Received fsnotify event")
+			w.logger.Debug(
+				"Received fsnotify event",
+				logfields.Path, event.Name,
+				logfields.Operation, event.Op,
+			)
 
 			eventPath := event.Name
 			removed := event.Has(fsnotify.Remove)
@@ -332,12 +332,18 @@ func (w *Watcher) loop() {
 				}
 			}
 		case err := <-w.watcher.Errors:
-			log.WithError(err).Debug("Received fsnotify error while watching")
+			w.logger.Debug(
+				"Received fsnotify error while watching",
+				logfields.Error, err,
+			)
 			w.sendError(err)
 		case <-w.stop:
 			err := w.watcher.Close()
 			if err != nil {
-				log.WithError(err).Warn("Received fsnotify error on close")
+				w.logger.Warn(
+					"Received fsnotify error on close",
+					logfields.Error, err,
+				)
 			}
 			close(w.Events)
 			close(w.Errors)
diff --git a/pkg/fswatcher/fswatcher_test.go b/pkg/fswatcher/fswatcher_test.go
index 35fc58d96f..c14f602e0c 100644
--- a/pkg/fswatcher/fswatcher_test.go
+++ b/pkg/fswatcher/fswatcher_test.go
@@ -8,10 +8,12 @@ import (
 	"path/filepath"
 	"testing"
 
+	"github.com/cilium/hive/hivetest"
 	"github.com/stretchr/testify/require"
 )
 
 func TestWatcher(t *testing.T) {
+	logger := hivetest.Logger(t)
 	tmp := t.TempDir()
 
 	regularFile := filepath.Join(tmp, "file")
@@ -22,7 +24,7 @@ func TestWatcher(t *testing.T) {
 	indirectSymlink := filepath.Join(tmp, "foo", "symlink", "nested")
 	targetFile := filepath.Join(tmp, "target")
 
-	w, err := New([]string{
+	w, err := New(logger, []string{
 		regularFile,
 		regularSymlink,
 		nestedFile,
diff --git a/pkg/gops/cell.go b/pkg/gops/cell.go
index 6461e9e156..f378161dc6 100644
--- a/pkg/gops/cell.go
+++ b/pkg/gops/cell.go
@@ -5,10 +5,10 @@ package gops
 
 import (
 	"fmt"
+	"log/slog"
 
 	"github.com/cilium/hive/cell"
 	gopsAgent "github.com/google/gops/agent"
-	"github.com/sirupsen/logrus"
 	"github.com/spf13/pflag"
 
 	"github.com/cilium/cilium/pkg/logging/logfields"
@@ -37,16 +37,15 @@ func (def GopsConfig) Flags(flags *pflag.FlagSet) {
 	flags.Bool(option.EnableGops, def.EnableGops, "Enable gops server")
 }
 
-func registerGopsHooks(lc cell.Lifecycle, log logrus.FieldLogger, cfg GopsConfig) {
+func registerGopsHooks(lc cell.Lifecycle, log *slog.Logger, cfg GopsConfig) {
 	if !cfg.EnableGops {
 		return
 	}
 	addr := fmt.Sprintf("127.0.0.1:%d", cfg.GopsPort)
-	addrField := logrus.Fields{"address": addr, logfields.LogSubsys: "gops"}
-	log = log.WithFields(addrField)
+	scopedLog := log.With(logfields.Address, addr)
 	lc.Append(cell.Hook{
 		OnStart: func(cell.HookContext) error {
-			log.Info("Started gops server")
+			scopedLog.Info("Started gops server")
 			return gopsAgent.Listen(gopsAgent.Options{
 				Addr:                   addr,
 				ReuseSocketAddrAndPort: true,
@@ -54,7 +53,7 @@ func registerGopsHooks(lc cell.Lifecycle, log logrus.FieldLogger, cfg GopsConfig
 		},
 		OnStop: func(cell.HookContext) error {
 			gopsAgent.Close()
-			log.Info("Stopped gops server")
+			scopedLog.Info("Stopped gops server")
 			return nil
 		},
 	})
diff --git a/pkg/health/client/client.go b/pkg/health/client/client.go
index e59c1b959e..37fdfa263d 100644
--- a/pkg/health/client/client.go
+++ b/pkg/health/client/client.go
@@ -164,23 +164,6 @@ func GetPathConnectivityStatusType(cp *models.PathStatus) ConnectivityStatusType
 	return status
 }
 
-func SummarizePathConnectivityStatus(cps []*models.PathStatus) ConnectivityStatusType {
-	status := ConnStatusReachable
-	for _, cp := range cps {
-		switch GetPathConnectivityStatusType(cp) {
-		case ConnStatusUnreachable:
-			// If any status is unreachable, return it immediately.
-			return ConnStatusUnreachable
-		case ConnStatusUnknown:
-			// If the status is unknown, prepare to return it. It's
-			// going to be returned if there is no unreachable
-			// status in next iterations.
-			status = ConnStatusUnknown
-		}
-	}
-	return status
-}
-
 // Returns a map of ConnectivityStatusType --> # of paths with ConnectivityStatusType
 func SummarizePathConnectivityStatusType(cps []*models.PathStatus) map[ConnectivityStatusType]int {
 	status := make(map[ConnectivityStatusType]int)
diff --git a/pkg/health/server/prober.go b/pkg/health/server/prober.go
index 804a0eb5f1..901f4f16c7 100644
--- a/pkg/health/server/prober.go
+++ b/pkg/health/server/prober.go
@@ -330,6 +330,9 @@ func icmpPing(logger *slog.Logger, node string, ip string, ctx context.Context,
 }
 
 func per(nodes int, duration time.Duration) rate.Limit {
+	if nodes == 0 {
+		nodes = 1
+	}
 	return rate.Every(duration / time.Duration(nodes))
 }
 
diff --git a/pkg/health/server/server.go b/pkg/health/server/server.go
index edbb3ca34e..464d94b96d 100644
--- a/pkg/health/server/server.go
+++ b/pkg/health/server/server.go
@@ -179,36 +179,15 @@ func (s *Server) collectNodeConnectivityMetrics(report *healthReport) {
 			continue
 		}
 
-		targetClusterName, targetNodeName := getClusterNodeName(n.Name)
 		nodePathPrimaryAddress := healthClientPkg.GetHostPrimaryAddress(n)
 		nodePathSecondaryAddress := healthClientPkg.GetHostSecondaryAddresses(n)
 
 		endpointPathStatus := n.HealthEndpoint
 
-		isEndpointReachable := healthClientPkg.SummarizePathConnectivityStatus(healthClientPkg.GetAllEndpointAddresses(n)) == healthClientPkg.ConnStatusReachable
-		isNodeReachable := healthClientPkg.SummarizePathConnectivityStatus(healthClientPkg.GetAllHostAddresses(n)) == healthClientPkg.ConnStatusReachable
-
 		isHealthEndpointReachable := healthClientPkg.SummarizePathConnectivityStatusType(healthClientPkg.GetAllEndpointAddresses(n))
 		isHealthNodeReachable := healthClientPkg.SummarizePathConnectivityStatusType(healthClientPkg.GetAllHostAddresses(n))
 
-		location := metrics.LabelLocationLocalNode
-		if targetClusterName != localClusterName {
-			location = metrics.LabelLocationRemoteInterCluster
-		} else if targetNodeName != localNodeName {
-			location = metrics.LabelLocationRemoteIntraCluster
-		}
-
 		// Update idempotent metrics here (to prevent overwriting with nil values).
-		// Aggregated status for endpoint connectivity
-		metrics.NodeConnectivityStatus.WithLabelValues(
-			localClusterName, localNodeName, targetClusterName, targetNodeName, location, metrics.LabelPeerEndpoint).
-			Set(metrics.BoolToFloat64(isEndpointReachable))
-
-		// Aggregated status for node connectivity
-		metrics.NodeConnectivityStatus.WithLabelValues(
-			localClusterName, localNodeName, targetClusterName, targetNodeName, location, metrics.LabelPeerNode).
-			Set(metrics.BoolToFloat64(isNodeReachable))
-
 		// Aggregate health connectivity statuses
 		for connectivityStatusType, value := range isHealthEndpointReachable {
 			endpointStatuses[connectivityStatusType] += value
@@ -234,50 +213,42 @@ func (s *Server) collectNodeConnectivityMetrics(report *healthReport) {
 
 		// HTTP endpoint primary
 		collectConnectivityMetric(s.logger, endpointPathStatus.PrimaryAddress.HTTP, localClusterName, localNodeName,
-			targetClusterName, targetNodeName, endpointPathStatus.PrimaryAddress.IP,
-			location, metrics.LabelPeerEndpoint, metrics.LabelTrafficHTTP, metrics.LabelAddressTypePrimary)
+			metrics.LabelPeerEndpoint, metrics.LabelTrafficHTTP, metrics.LabelAddressTypePrimary)
 
 		// HTTP endpoint secondary
 		for _, secondary := range endpointPathStatus.SecondaryAddresses {
 			collectConnectivityMetric(s.logger, secondary.HTTP, localClusterName, localNodeName,
-				targetClusterName, targetNodeName, secondary.IP,
-				location, metrics.LabelPeerEndpoint, metrics.LabelTrafficHTTP, metrics.LabelAddressTypeSecondary)
+				metrics.LabelPeerEndpoint, metrics.LabelTrafficHTTP, metrics.LabelAddressTypeSecondary)
 		}
 
 		// HTTP node primary
 		collectConnectivityMetric(s.logger, nodePathPrimaryAddress.HTTP, localClusterName, localNodeName,
-			targetClusterName, targetNodeName, nodePathPrimaryAddress.IP,
-			location, metrics.LabelPeerNode, metrics.LabelTrafficHTTP, metrics.LabelAddressTypePrimary)
+			metrics.LabelPeerNode, metrics.LabelTrafficHTTP, metrics.LabelAddressTypePrimary)
 
 		// HTTP node secondary
 		for _, secondary := range nodePathSecondaryAddress {
 			collectConnectivityMetric(s.logger, secondary.HTTP, localClusterName, localNodeName,
-				targetClusterName, targetNodeName, secondary.IP,
-				location, metrics.LabelPeerNode, metrics.LabelTrafficHTTP, metrics.LabelAddressTypeSecondary)
+				metrics.LabelPeerNode, metrics.LabelTrafficHTTP, metrics.LabelAddressTypeSecondary)
 		}
 
 		// ICMP endpoint primary
 		collectConnectivityMetric(s.logger, endpointPathStatus.PrimaryAddress.Icmp, localClusterName, localNodeName,
-			targetClusterName, targetNodeName, endpointPathStatus.PrimaryAddress.IP,
-			location, metrics.LabelPeerEndpoint, metrics.LabelTrafficICMP, metrics.LabelAddressTypePrimary)
+			metrics.LabelPeerEndpoint, metrics.LabelTrafficICMP, metrics.LabelAddressTypePrimary)
 
 		// ICMP endpoint secondary
 		for _, secondary := range endpointPathStatus.SecondaryAddresses {
 			collectConnectivityMetric(s.logger, secondary.Icmp, localClusterName, localNodeName,
-				targetClusterName, targetNodeName, secondary.IP,
-				location, metrics.LabelPeerEndpoint, metrics.LabelTrafficICMP, metrics.LabelAddressTypeSecondary)
+				metrics.LabelPeerEndpoint, metrics.LabelTrafficICMP, metrics.LabelAddressTypeSecondary)
 		}
 
 		// ICMP node primary
 		collectConnectivityMetric(s.logger, nodePathPrimaryAddress.Icmp, localClusterName, localNodeName,
-			targetClusterName, targetNodeName, nodePathPrimaryAddress.IP,
-			location, metrics.LabelPeerNode, metrics.LabelTrafficICMP, metrics.LabelAddressTypePrimary)
+			metrics.LabelPeerNode, metrics.LabelTrafficICMP, metrics.LabelAddressTypePrimary)
 
 		// ICMP node secondary
 		for _, secondary := range nodePathSecondaryAddress {
 			collectConnectivityMetric(s.logger, secondary.Icmp, localClusterName, localNodeName,
-				targetClusterName, targetNodeName, secondary.IP,
-				location, metrics.LabelPeerNode, metrics.LabelTrafficICMP, metrics.LabelAddressTypeSecondary)
+				metrics.LabelPeerNode, metrics.LabelTrafficICMP, metrics.LabelAddressTypeSecondary)
 		}
 	}
 
@@ -309,29 +280,12 @@ func (s *Server) collectNodeConnectivityMetrics(report *healthReport) {
 }
 
 func collectConnectivityMetric(logger *slog.Logger, status *healthModels.ConnectivityStatus, labels ...string) {
-	// collect deprecated node_connectivity_latency_seconds
-	var metricValue float64 = -1
 	if status != nil {
-		metricValue = float64(status.Latency) / float64(time.Second)
-	}
-	metrics.NodeConnectivityLatency.WithLabelValues(labels...).Set(metricValue)
-
-	// collect node_health_connectivity_latency_seconds
-	if status != nil {
-		// node_health_connectivity_latency_seconds copies a subset of the labels
-		// of the deprecated metric for use when observing metrics
-		if len(labels) < 7 {
-			logger.Warn("node_health_connectivity_latency_seconds metric is missing labels, could not be collected")
-			return
-		}
-		healthLabels := make([]string, 5)
-		copy(healthLabels, labels[0:2])
-		copy(healthLabels[2:], labels[6:])
 		if status.Status == "" {
-			metricValue = float64(status.Latency) / float64(time.Second)
-			metrics.NodeHealthConnectivityLatency.WithLabelValues(healthLabels...).Observe(metricValue)
+			metricValue := float64(status.Latency) / float64(time.Second)
+			metrics.NodeHealthConnectivityLatency.WithLabelValues(labels...).Observe(metricValue)
 		} else {
-			metrics.NodeHealthConnectivityLatency.WithLabelValues(healthLabels...).Observe(probe.HttpTimeout.Seconds())
+			metrics.NodeHealthConnectivityLatency.WithLabelValues(labels...).Observe(probe.HttpTimeout.Seconds())
 		}
 	}
 }
diff --git a/pkg/health/server/server_old_test.go b/pkg/health/server/server_old_test.go
deleted file mode 100644
index e3a8cdc111..0000000000
--- a/pkg/health/server/server_old_test.go
+++ /dev/null
@@ -1,338 +0,0 @@
-// SPDX-License-Identifier: Apache-2.0
-// Copyright Authors of Cilium
-
-package server
-
-import (
-	"strings"
-	"testing"
-
-	"github.com/prometheus/client_golang/prometheus"
-	"github.com/prometheus/client_golang/prometheus/testutil"
-	"github.com/stretchr/testify/require"
-
-	healthModels "github.com/cilium/cilium/api/v1/health/models"
-	"github.com/cilium/cilium/pkg/metrics"
-	"github.com/cilium/cilium/pkg/metrics/metric"
-)
-
-// Tests for deprecated metrics cilium_node_connectivity_latency_seconds and cilium_node_connectivity_status
-
-var sampleSingleClusterConnectivityOld = &healthReport{
-	nodes: []*healthModels.NodeStatus{
-		{
-			HealthEndpoint: &healthModels.EndpointStatus{
-				PrimaryAddress: &healthModels.PathStatus{
-					HTTP: &healthModels.ConnectivityStatus{
-						Latency: 212100,
-					},
-					Icmp: &healthModels.ConnectivityStatus{
-						Latency: 672600,
-					},
-					IP: "10.244.3.219",
-				},
-				SecondaryAddresses: []*healthModels.PathStatus{
-					{
-						HTTP: &healthModels.ConnectivityStatus{
-							Latency: 212101,
-						},
-						Icmp: &healthModels.ConnectivityStatus{
-							Latency: 672601,
-						},
-						IP: "10.244.3.220",
-					},
-				},
-			},
-			Host: &healthModels.HostStatus{
-				PrimaryAddress: &healthModels.PathStatus{
-					HTTP: &healthModels.ConnectivityStatus{
-						Latency: 165362,
-					},
-					Icmp: &healthModels.ConnectivityStatus{
-						Latency: 704179,
-					},
-					IP: "172.18.0.3",
-				},
-				SecondaryAddresses: []*healthModels.PathStatus{
-					{
-						HTTP: &healthModels.ConnectivityStatus{
-							Latency: 212102,
-						},
-						Icmp: &healthModels.ConnectivityStatus{
-							Latency: 672602,
-						},
-						IP: "172.18.0.4",
-					},
-				},
-			},
-			Name: "kind-worker",
-		},
-	},
-}
-
-var sampleClustermeshConnectivityOld = &healthReport{
-	nodes: []*healthModels.NodeStatus{
-		{
-			HealthEndpoint: &healthModels.EndpointStatus{
-				PrimaryAddress: &healthModels.PathStatus{
-					HTTP: &healthModels.ConnectivityStatus{
-						Latency: 312100,
-					},
-					Icmp: &healthModels.ConnectivityStatus{
-						Latency: 772600,
-					},
-					IP: "10.244.3.219",
-				},
-				SecondaryAddresses: []*healthModels.PathStatus{
-					{
-						HTTP: &healthModels.ConnectivityStatus{
-							Latency: 312101,
-						},
-						Icmp: &healthModels.ConnectivityStatus{
-							Latency: 772601,
-						},
-						IP: "10.244.3.220",
-					},
-				},
-			},
-			Host: &healthModels.HostStatus{
-				PrimaryAddress: &healthModels.PathStatus{
-					HTTP: &healthModels.ConnectivityStatus{
-						Latency: 165362,
-					},
-					Icmp: &healthModels.ConnectivityStatus{
-						Latency: 704179,
-					},
-					IP: "172.18.0.1",
-				},
-				SecondaryAddresses: []*healthModels.PathStatus{
-					{
-						HTTP: &healthModels.ConnectivityStatus{
-							Latency: 312105,
-						},
-						Icmp: &healthModels.ConnectivityStatus{
-							Latency: 772606,
-						},
-						IP: "172.18.0.2",
-					},
-				},
-			},
-			Name: "kind-cilium-mesh-1/kind-cilium-mesh-1-worker",
-		},
-		{
-			HealthEndpoint: &healthModels.EndpointStatus{
-				PrimaryAddress: &healthModels.PathStatus{
-					HTTP: &healthModels.ConnectivityStatus{
-						Latency: 274815,
-					},
-					Icmp: &healthModels.ConnectivityStatus{
-						Latency: 583711,
-					},
-					IP: "10.1.2.143",
-				},
-				SecondaryAddresses: []*healthModels.PathStatus{
-					{
-						HTTP: &healthModels.ConnectivityStatus{
-							Latency: 212101,
-						},
-						Icmp: &healthModels.ConnectivityStatus{
-							Latency: 672601,
-						},
-						IP: "10.1.2.144",
-					},
-				},
-			},
-			Host: &healthModels.HostStatus{
-				PrimaryAddress: &healthModels.PathStatus{
-					HTTP: &healthModels.ConnectivityStatus{
-						Latency: 166101,
-					},
-					Icmp: &healthModels.ConnectivityStatus{
-						Latency: 635688,
-					},
-					IP: "172.18.0.3",
-				},
-				SecondaryAddresses: []*healthModels.PathStatus{
-					{
-						HTTP: &healthModels.ConnectivityStatus{
-							Latency: 212103,
-						},
-						Icmp: &healthModels.ConnectivityStatus{
-							Latency: 672603,
-						},
-						IP: "172.18.0.4",
-					},
-				},
-			},
-			Name: "kind-cilium-mesh-2/kind-cilium-mesh-2-worker",
-		},
-	},
-}
-
-var expectedSingleClusterMetricOld = map[string]string{
-	"cilium_node_connectivity_latency_seconds": `
-# HELP cilium_node_connectivity_latency_seconds The last observed latency between the current Cilium agent and other Cilium nodes in seconds
-# TYPE cilium_node_connectivity_latency_seconds gauge
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="http",source_cluster="default",source_node_name="kind-worker",target_cluster="default",target_node_ip="10.244.3.219",target_node_name="kind-worker",target_node_type="local_node",type="endpoint"} 0.0002121
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="http",source_cluster="default",source_node_name="kind-worker",target_cluster="default",target_node_ip="172.18.0.3",target_node_name="kind-worker",target_node_type="local_node",type="node"} 0.000165362
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="icmp",source_cluster="default",source_node_name="kind-worker",target_cluster="default",target_node_ip="10.244.3.219",target_node_name="kind-worker",target_node_type="local_node",type="endpoint"} 0.0006726
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="icmp",source_cluster="default",source_node_name="kind-worker",target_cluster="default",target_node_ip="172.18.0.3",target_node_name="kind-worker",target_node_type="local_node",type="node"} 0.000704179
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="http",source_cluster="default",source_node_name="kind-worker",target_cluster="default",target_node_ip="10.244.3.220",target_node_name="kind-worker",target_node_type="local_node",type="endpoint"} 0.000212101
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="http",source_cluster="default",source_node_name="kind-worker",target_cluster="default",target_node_ip="172.18.0.4",target_node_name="kind-worker",target_node_type="local_node",type="node"} 0.000212102
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="icmp",source_cluster="default",source_node_name="kind-worker",target_cluster="default",target_node_ip="10.244.3.220",target_node_name="kind-worker",target_node_type="local_node",type="endpoint"} 0.000672601
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="icmp",source_cluster="default",source_node_name="kind-worker",target_cluster="default",target_node_ip="172.18.0.4",target_node_name="kind-worker",target_node_type="local_node",type="node"} 0.000672602
-`,
-	"cilium_node_connectivity_status": `
-# HELP cilium_node_connectivity_status The last observed status of both ICMP and HTTP connectivity between the current Cilium agent and other Cilium nodes
-# TYPE cilium_node_connectivity_status gauge
-cilium_node_connectivity_status{source_cluster="default",source_node_name="kind-worker",target_cluster="default",target_node_name="kind-worker",target_node_type="local_node",type="endpoint"} 1
-cilium_node_connectivity_status{source_cluster="default",source_node_name="kind-worker",target_cluster="default",target_node_name="kind-worker",target_node_type="local_node",type="node"} 1
-`,
-}
-
-var expectedClustermeshMetricOld = map[string]string{
-	"cilium_node_connectivity_latency_seconds": `
-# HELP cilium_node_connectivity_latency_seconds The last observed latency between the current Cilium agent and other Cilium nodes in seconds
-# TYPE cilium_node_connectivity_latency_seconds gauge
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="http",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-1",target_node_ip="10.244.3.219",target_node_name="kind-cilium-mesh-1-worker",target_node_type="local_node",type="endpoint"} 0.0003121
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="http",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-1",target_node_ip="172.18.0.1",target_node_name="kind-cilium-mesh-1-worker",target_node_type="local_node",type="node"} 0.000165362
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="http",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-2",target_node_ip="10.1.2.143",target_node_name="kind-cilium-mesh-2-worker",target_node_type="remote_inter_cluster",type="endpoint"} 0.000274815
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="http",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-2",target_node_ip="172.18.0.3",target_node_name="kind-cilium-mesh-2-worker",target_node_type="remote_inter_cluster",type="node"} 0.000166101
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="icmp",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-1",target_node_ip="10.244.3.219",target_node_name="kind-cilium-mesh-1-worker",target_node_type="local_node",type="endpoint"} 0.0007726
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="icmp",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-1",target_node_ip="172.18.0.1",target_node_name="kind-cilium-mesh-1-worker",target_node_type="local_node",type="node"} 0.000704179
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="icmp",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-2",target_node_ip="10.1.2.143",target_node_name="kind-cilium-mesh-2-worker",target_node_type="remote_inter_cluster",type="endpoint"} 0.000583711
-cilium_node_connectivity_latency_seconds{address_type="primary",protocol="icmp",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-2",target_node_ip="172.18.0.3",target_node_name="kind-cilium-mesh-2-worker",target_node_type="remote_inter_cluster",type="node"} 0.000635688
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="http",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-1",target_node_ip="10.244.3.220",target_node_name="kind-cilium-mesh-1-worker",target_node_type="local_node",type="endpoint"} 0.000312101
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="http",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-1",target_node_ip="172.18.0.2",target_node_name="kind-cilium-mesh-1-worker",target_node_type="local_node",type="node"} 0.000312105
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="http",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-2",target_node_ip="10.1.2.144",target_node_name="kind-cilium-mesh-2-worker",target_node_type="remote_inter_cluster",type="endpoint"} 0.000212101
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="http",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-2",target_node_ip="172.18.0.4",target_node_name="kind-cilium-mesh-2-worker",target_node_type="remote_inter_cluster",type="node"} 0.000212103
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="icmp",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-1",target_node_ip="10.244.3.220",target_node_name="kind-cilium-mesh-1-worker",target_node_type="local_node",type="endpoint"} 0.000772601
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="icmp",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-1",target_node_ip="172.18.0.2",target_node_name="kind-cilium-mesh-1-worker",target_node_type="local_node",type="node"} 0.000772606
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="icmp",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-2",target_node_ip="10.1.2.144",target_node_name="kind-cilium-mesh-2-worker",target_node_type="remote_inter_cluster",type="endpoint"} 0.000672601
-cilium_node_connectivity_latency_seconds{address_type="secondary",protocol="icmp",source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-2",target_node_ip="172.18.0.4",target_node_name="kind-cilium-mesh-2-worker",target_node_type="remote_inter_cluster",type="node"} 0.000672603
-`,
-	"cilium_node_connectivity_status": `
-# HELP cilium_node_connectivity_status The last observed status of both ICMP and HTTP connectivity between the current Cilium agent and other Cilium nodes
-# TYPE cilium_node_connectivity_status gauge
-cilium_node_connectivity_status{source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-1",target_node_name="kind-cilium-mesh-1-worker",target_node_type="local_node",type="endpoint"} 1
-cilium_node_connectivity_status{source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-1",target_node_name="kind-cilium-mesh-1-worker",target_node_type="local_node",type="node"} 1
-cilium_node_connectivity_status{source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-2",target_node_name="kind-cilium-mesh-2-worker",target_node_type="remote_inter_cluster",type="endpoint"} 1
-cilium_node_connectivity_status{source_cluster="kind-cilium-mesh-1",source_node_name="kind-cilium-mesh-1-worker",target_cluster="kind-cilium-mesh-2",target_node_name="kind-cilium-mesh-2-worker",target_node_type="remote_inter_cluster",type="node"} 1
-`,
-}
-
-func Test_server_getClusterNodeNameOld(t *testing.T) {
-	tests := []struct {
-		name                string
-		fullName            string
-		expectedClusterName string
-		expectedNodeName    string
-	}{
-		{
-			name:                "no cluster name",
-			fullName:            "k8s1",
-			expectedClusterName: "default",
-			expectedNodeName:    "k8s1",
-		},
-		{
-			name:                "simple full name",
-			fullName:            "kind-kind/worker",
-			expectedClusterName: "kind-kind",
-			expectedNodeName:    "worker",
-		},
-		{
-			name:                "cluster name is having slash",
-			fullName:            "arn:aws:eks:us-west-2:012345678910:cluster/cluster/name",
-			expectedClusterName: "arn:aws:eks:us-west-2:012345678910:cluster/cluster",
-			expectedNodeName:    "name",
-		},
-	}
-
-	for _, tt := range tests {
-		t.Run(tt.name, func(t *testing.T) {
-			clusterName, nodeName := getClusterNodeName(tt.fullName)
-			require.Equal(t, tt.expectedClusterName, clusterName)
-			require.Equal(t, tt.expectedNodeName, nodeName)
-		})
-	}
-}
-
-func Test_server_collectNodeConnectivityMetricsOld(t *testing.T) {
-	tests := []struct {
-		name           string
-		localStatus    *healthModels.SelfStatus
-		connectivity   *healthReport
-		metric         func() metric.WithMetadata
-		expectedMetric string
-		expectedCount  int
-	}{
-		{
-			name: "single cluster for cilium_node_connectivity_status",
-			localStatus: &healthModels.SelfStatus{
-				Name: "kind-worker",
-			},
-			connectivity:   sampleSingleClusterConnectivityOld,
-			metric:         func() metric.WithMetadata { return metrics.NodeConnectivityStatus },
-			expectedCount:  2,
-			expectedMetric: expectedSingleClusterMetricOld["cilium_node_connectivity_status"],
-		},
-		{
-			name: "single cluster for cilium_node_connectivity_latency_seconds",
-			localStatus: &healthModels.SelfStatus{
-				Name: "kind-worker",
-			},
-			connectivity:   sampleSingleClusterConnectivityOld,
-			metric:         func() metric.WithMetadata { return metrics.NodeConnectivityLatency },
-			expectedCount:  8,
-			expectedMetric: expectedSingleClusterMetricOld["cilium_node_connectivity_latency_seconds"],
-		},
-		{
-			name: "cluster mesh for cilium_node_connectivity_status",
-			localStatus: &healthModels.SelfStatus{
-				Name: "kind-cilium-mesh-1/kind-cilium-mesh-1-worker",
-			},
-			connectivity:   sampleClustermeshConnectivityOld,
-			metric:         func() metric.WithMetadata { return metrics.NodeConnectivityStatus },
-			expectedCount:  4,
-			expectedMetric: expectedClustermeshMetricOld["cilium_node_connectivity_status"],
-		},
-		{
-			name: "cluster mesh for cilium_node_connectivity_latency_seconds",
-			localStatus: &healthModels.SelfStatus{
-				Name: "kind-cilium-mesh-1/kind-cilium-mesh-1-worker",
-			},
-			connectivity:   sampleClustermeshConnectivityOld,
-			metric:         func() metric.WithMetadata { return metrics.NodeConnectivityLatency },
-			expectedCount:  16,
-			expectedMetric: expectedClustermeshMetricOld["cilium_node_connectivity_latency_seconds"],
-		},
-	}
-
-	for _, tt := range tests {
-		t.Run(tt.name, func(t *testing.T) {
-			metrics.NewLegacyMetrics()
-			tt.metric().SetEnabled(true)
-			collector := tt.metric().(prometheus.Collector)
-			s := &Server{
-				connectivity: tt.connectivity,
-				localStatus:  tt.localStatus,
-				nodesSeen:    make(map[string]struct{}),
-			}
-			s.collectNodeConnectivityMetrics(tt.connectivity)
-
-			// perform static checks such as prometheus naming convention, number of labels matching, etc
-			lintProblems, err := testutil.CollectAndLint(collector)
-			require.NoError(t, err)
-			require.Empty(t, lintProblems)
-
-			// check the number of metrics
-			count := testutil.CollectAndCount(collector)
-			require.Equal(t, tt.expectedCount, count)
-
-			// compare the metric output
-			err = testutil.CollectAndCompare(collector, strings.NewReader(tt.expectedMetric))
-			require.NoError(t, err)
-		})
-	}
-
-}
diff --git a/pkg/ipmasq/ipmasq.go b/pkg/ipmasq/ipmasq.go
index 15db12fff4..1603f9e417 100644
--- a/pkg/ipmasq/ipmasq.go
+++ b/pkg/ipmasq/ipmasq.go
@@ -6,6 +6,7 @@ package ipmasq
 import (
 	"encoding/json"
 	"fmt"
+	"log/slog"
 	"maps"
 	"net/netip"
 	"os"
@@ -15,15 +16,12 @@ import (
 	"github.com/fsnotify/fsnotify"
 	"k8s.io/apimachinery/pkg/util/yaml"
 
-	"github.com/cilium/cilium/pkg/logging"
 	"github.com/cilium/cilium/pkg/logging/logfields"
 	"github.com/cilium/cilium/pkg/maps/ipmasq"
 	"github.com/cilium/cilium/pkg/metrics"
 )
 
 var (
-	log = logging.DefaultLogger.WithField(logfields.LogSubsys, "ipmasq")
-
 	// The following reserved by RFCs IP addr ranges are used by
 	// https://github.com/kubernetes-sigs/ip-masq-agent
 	defaultNonMasqCIDRs = map[string]netip.Prefix{
@@ -89,6 +87,7 @@ type IPMasqMap interface {
 
 // IPMasqAgent represents a state of the ip-masq-agent
 type IPMasqAgent struct {
+	logger                 *slog.Logger
 	configPath             string
 	masqLinkLocalIPv4      bool
 	masqLinkLocalIPv6      bool
@@ -100,11 +99,11 @@ type IPMasqAgent struct {
 	handlerFinished        chan struct{}
 }
 
-func NewIPMasqAgent(reg *metrics.Registry, configPath string) (*IPMasqAgent, error) {
-	return newIPMasqAgent(configPath, &ipmasq.IPMasqBPFMap{MetricsRegistry: reg})
+func NewIPMasqAgent(logger *slog.Logger, reg *metrics.Registry, configPath string) (*IPMasqAgent, error) {
+	return newIPMasqAgent(logger, configPath, &ipmasq.IPMasqBPFMap{MetricsRegistry: reg})
 }
 
-func newIPMasqAgent(configPath string, ipMasqMap IPMasqMap) (*IPMasqAgent, error) {
+func newIPMasqAgent(logger *slog.Logger, configPath string, ipMasqMap IPMasqMap) (*IPMasqAgent, error) {
 	watcher, err := fsnotify.NewWatcher()
 	if err != nil {
 		return nil, fmt.Errorf("Failed to create fsnotify watcher: %w", err)
@@ -119,6 +118,7 @@ func newIPMasqAgent(configPath string, ipMasqMap IPMasqMap) (*IPMasqAgent, error
 	}
 
 	a := &IPMasqAgent{
+		logger:                 logger,
 		configPath:             configPath,
 		nonMasqCIDRsFromConfig: map[string]netip.Prefix{},
 		nonMasqCIDRsInMap:      map[string]netip.Prefix{},
@@ -133,10 +133,10 @@ func newIPMasqAgent(configPath string, ipMasqMap IPMasqMap) (*IPMasqAgent, error
 // updates the BPF map accordingly.
 func (a *IPMasqAgent) Start() {
 	if err := a.restore(); err != nil {
-		log.WithError(err).Warn("Failed to restore")
+		a.logger.Warn("Failed to restore", logfields.Error, err)
 	}
 	if err := a.Update(); err != nil {
-		log.WithError(err).Warn("Failed to update")
+		a.logger.Warn("Failed to update", logfields.Error, err)
 	}
 
 	a.stop = make(chan struct{})
@@ -146,7 +146,7 @@ func (a *IPMasqAgent) Start() {
 		for {
 			select {
 			case event := <-a.watcher.Events:
-				log.Debugf("Received fsnotify event: %+v", event)
+				a.logger.Debug("Received fsnotify event", logfields.Event, event)
 
 				switch {
 				case event.Has(fsnotify.Create),
@@ -155,15 +155,15 @@ func (a *IPMasqAgent) Start() {
 					event.Has(fsnotify.Remove),
 					event.Has(fsnotify.Rename):
 					if err := a.Update(); err != nil {
-						log.WithError(err).Warn("Failed to update")
+						a.logger.Warn("Failed to update", logfields.Error, err)
 					}
 				default:
-					log.Warnf("Watcher received unknown event: %s. Ignoring.", event)
+					a.logger.Warn("Watcher received unknown event. Ignoring...", logfields.Event, event)
 				}
 			case err := <-a.watcher.Errors:
-				log.WithError(err).Warn("Watcher received an error")
+				a.logger.Warn("Watcher received an error", logfields.Error, err)
 			case <-a.stop:
-				log.Info("Stopping ip-masq-agent")
+				a.logger.Info("Stopping ip-masq-agent")
 				close(a.handlerFinished)
 				return
 			}
@@ -200,7 +200,7 @@ func (a *IPMasqAgent) Update() error {
 
 	for cidrStr, cidr := range a.nonMasqCIDRsFromConfig {
 		if _, ok := a.nonMasqCIDRsInMap[cidrStr]; !ok {
-			log.WithField(logfields.CIDR, cidrStr).Info("Adding CIDR")
+			a.logger.Info("Adding CIDR", logfields.CIDR, cidrStr)
 			a.ipMasqMap.Update(cidr)
 			a.nonMasqCIDRsInMap[cidrStr] = cidr
 		}
@@ -208,7 +208,7 @@ func (a *IPMasqAgent) Update() error {
 
 	for cidrStr, cidr := range a.nonMasqCIDRsInMap {
 		if _, ok := a.nonMasqCIDRsFromConfig[cidrStr]; !ok {
-			log.WithField(logfields.CIDR, cidrStr).Info("Removing CIDR")
+			a.logger.Info("Removing CIDR", logfields.CIDR, cidrStr)
 			a.ipMasqMap.Delete(cidr)
 			delete(a.nonMasqCIDRsInMap, cidrStr)
 		}
@@ -225,7 +225,7 @@ func (a *IPMasqAgent) readConfig() (bool, error) {
 	raw, err := os.ReadFile(a.configPath)
 	if err != nil {
 		if os.IsNotExist(err) {
-			log.WithField(logfields.Path, a.configPath).Info("Config file not found")
+			a.logger.Info("Config file not found", logfields.Path, a.configPath)
 			a.nonMasqCIDRsFromConfig = map[string]netip.Prefix{}
 			a.masqLinkLocalIPv4 = false
 			a.masqLinkLocalIPv6 = false
diff --git a/pkg/ipmasq/ipmasq_test.go b/pkg/ipmasq/ipmasq_test.go
index 54f139a0fd..f4cae44e81 100644
--- a/pkg/ipmasq/ipmasq_test.go
+++ b/pkg/ipmasq/ipmasq_test.go
@@ -10,6 +10,7 @@ import (
 	"testing"
 	"time"
 
+	"github.com/cilium/hive/hivetest"
 	"github.com/stretchr/testify/assert"
 	"github.com/stretchr/testify/require"
 
@@ -133,6 +134,7 @@ type IPMasqTestSuite struct {
 }
 
 func setUpTest(tb testing.TB) *IPMasqTestSuite {
+	logger := hivetest.Logger(tb)
 	i := &IPMasqTestSuite{}
 	i.ipMasqMap = &ipMasqMapMock{
 		cidrsIPv4: map[string]netip.Prefix{},
@@ -143,7 +145,7 @@ func setUpTest(tb testing.TB) *IPMasqTestSuite {
 	require.NoError(tb, err)
 	i.configFilePath = configFile.Name()
 
-	agent, err := newIPMasqAgent(i.configFilePath, i.ipMasqMap)
+	agent, err := newIPMasqAgent(logger, i.configFilePath, i.ipMasqMap)
 	require.NoError(tb, err)
 	i.ipMasqAgent = agent
 
@@ -359,6 +361,7 @@ func TestUpdate(t *testing.T) {
 }
 
 func TestRestoreIPv4(t *testing.T) {
+	logger := hivetest.Logger(t)
 	var err error
 
 	i := setUpTest(t)
@@ -374,7 +377,7 @@ func TestRestoreIPv4(t *testing.T) {
 	i.ipMasqMap.cidrsIPv4[cidr.String()] = cidr
 	i.writeConfig(t, "nonMasqueradeCIDRs:\n- 4.4.0.0/16")
 
-	i.ipMasqAgent, err = newIPMasqAgent(i.configFilePath, i.ipMasqMap)
+	i.ipMasqAgent, err = newIPMasqAgent(logger, i.configFilePath, i.ipMasqMap)
 	require.NoError(t, err)
 	i.ipMasqAgent.Start()
 	time.Sleep(300 * time.Millisecond)
@@ -396,7 +399,7 @@ func TestRestoreIPv4(t *testing.T) {
 	}
 	i.ipMasqAgent.ipMasqMap = i.ipMasqMap
 	i.writeConfig(t, "nonMasqueradeCIDRs:\n- 3.3.0.0/16\nmasqLinkLocal: true")
-	i.ipMasqAgent, err = newIPMasqAgent(i.configFilePath, i.ipMasqMap)
+	i.ipMasqAgent, err = newIPMasqAgent(logger, i.configFilePath, i.ipMasqMap)
 	require.NoError(t, err)
 	i.ipMasqAgent.Start()
 
@@ -407,6 +410,7 @@ func TestRestoreIPv4(t *testing.T) {
 }
 
 func TestRestoreIPv6(t *testing.T) {
+	logger := hivetest.Logger(t)
 	var err error
 
 	i := setUpTest(t)
@@ -422,7 +426,7 @@ func TestRestoreIPv6(t *testing.T) {
 	i.ipMasqMap.cidrsIPv6[cidr.String()] = cidr
 	i.writeConfig(t, "nonMasqueradeCIDRs:\n- 4:4::/32")
 
-	i.ipMasqAgent, err = newIPMasqAgent(i.configFilePath, i.ipMasqMap)
+	i.ipMasqAgent, err = newIPMasqAgent(logger, i.configFilePath, i.ipMasqMap)
 	require.NoError(t, err)
 	i.ipMasqAgent.Start()
 	time.Sleep(300 * time.Millisecond)
@@ -444,7 +448,7 @@ func TestRestoreIPv6(t *testing.T) {
 	}
 	i.ipMasqAgent.ipMasqMap = i.ipMasqMap
 	i.writeConfig(t, "nonMasqueradeCIDRs:\n- 3:3::/96\nmasqLinkLocalIPv6: true")
-	i.ipMasqAgent, err = newIPMasqAgent(i.configFilePath, i.ipMasqMap)
+	i.ipMasqAgent, err = newIPMasqAgent(logger, i.configFilePath, i.ipMasqMap)
 	require.NoError(t, err)
 	i.ipMasqAgent.Start()
 
@@ -455,6 +459,7 @@ func TestRestoreIPv6(t *testing.T) {
 }
 
 func TestRestore(t *testing.T) {
+	logger := hivetest.Logger(t)
 	var err error
 
 	i := setUpTest(t)
@@ -474,7 +479,7 @@ func TestRestore(t *testing.T) {
 	i.ipMasqMap.cidrsIPv4[cidr.String()] = cidr
 	i.writeConfig(t, "nonMasqueradeCIDRs:\n- 4.4.0.0/16\n- 4:4::/32")
 
-	i.ipMasqAgent, err = newIPMasqAgent(i.configFilePath, i.ipMasqMap)
+	i.ipMasqAgent, err = newIPMasqAgent(logger, i.configFilePath, i.ipMasqMap)
 	require.NoError(t, err)
 	i.ipMasqAgent.Start()
 	time.Sleep(300 * time.Millisecond)
@@ -501,7 +506,7 @@ func TestRestore(t *testing.T) {
 	}
 	i.ipMasqAgent.ipMasqMap = i.ipMasqMap
 	i.writeConfig(t, "nonMasqueradeCIDRs:\n- 3.3.0.0/16\n- 3:3:3:3::/96\nmasqLinkLocal: true\nmasqLinkLocalIPv6: true")
-	i.ipMasqAgent, err = newIPMasqAgent(i.configFilePath, i.ipMasqMap)
+	i.ipMasqAgent, err = newIPMasqAgent(logger, i.configFilePath, i.ipMasqMap)
 	require.NoError(t, err)
 	i.ipMasqAgent.Start()
 
diff --git a/pkg/k8s/apis/cilium.io/utils/utils.go b/pkg/k8s/apis/cilium.io/utils/utils.go
index 611eedcb73..0b89ce1ba7 100644
--- a/pkg/k8s/apis/cilium.io/utils/utils.go
+++ b/pkg/k8s/apis/cilium.io/utils/utils.go
@@ -8,6 +8,7 @@ import (
 
 	"k8s.io/apimachinery/pkg/types"
 
+	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	k8sConst "github.com/cilium/cilium/pkg/k8s/apis/cilium.io"
 	slim_metav1 "github.com/cilium/cilium/pkg/k8s/slim/k8s/apis/meta/v1"
 	"github.com/cilium/cilium/pkg/labels"
@@ -30,6 +31,13 @@ const (
 	// for any source type.
 	podAnyNamespaceLabelsPrefix = labels.LabelSourceAnyKeyPrefix + k8sConst.PodNamespaceMetaLabelsPrefix
 
+	// clusterPrefixLbl is the prefix use in the label selector for cluster name.
+	clusterPrefixLbl = labels.LabelSourceK8sKeyPrefix + k8sConst.PolicyLabelCluster
+
+	// clusterAnyPrefixLbl is the prefix use in the label selector for cluster name
+	// for any source type.
+	clusterAnyPrefixLbl = labels.LabelSourceAnyKeyPrefix + k8sConst.PolicyLabelCluster
+
 	// podInitLbl is the label used in a label selector to match on
 	// initializing pods.
 	podInitLbl = labels.LabelSourceReservedKeyPrefix + labels.IDNameInit
@@ -62,10 +70,12 @@ func GetPolicyLabels(ns, name string, uid types.UID, derivedFrom string) labels.
 }
 
 // getEndpointSelector converts the provided labelSelector into an EndpointSelector,
-// adding the relevant matches for namespaces based on the provided options.
+// adding the relevant matches for namespaces and clusters based on the provided options.
 // If no namespace is provided then it is assumed that the selector is global to the cluster
 // this is when translating selectors for CiliumClusterwideNetworkPolicy.
-func getEndpointSelector(namespace string, labelSelector *slim_metav1.LabelSelector, addK8sPrefix, matchesInit bool) api.EndpointSelector {
+// If a clusterName is provided then is is assumed that the selector is scoped to the local
+// cluster by default in a ClusterMesh environment.
+func getEndpointSelector(clusterName, namespace string, labelSelector *slim_metav1.LabelSelector, addK8sPrefix, matchesInit bool) api.EndpointSelector {
 	es := api.NewESFromK8sLabelSelector("", labelSelector)
 
 	// The k8s prefix must not be added to reserved labels.
@@ -96,17 +106,24 @@ func getEndpointSelector(namespace string, labelSelector *slim_metav1.LabelSelec
 		}
 	}
 
+	// Similarly to namespace, the user can explicitly specify the cluster in the
+	// FromEndpoints selector. If omitted, we limit the
+	// scope to the cluster the policy lives in.
+	if clusterName != cmtypes.PolicyAnyCluster && !es.HasKey(clusterPrefixLbl) && !es.HasKey(clusterAnyPrefixLbl) {
+		es.AddMatch(clusterPrefixLbl, clusterName)
+	}
+
 	return es
 }
 
-func parseToCiliumIngressCommonRule(namespace string, es api.EndpointSelector, ing api.IngressCommonRule) api.IngressCommonRule {
+func parseToCiliumIngressCommonRule(clusterName, namespace string, es api.EndpointSelector, ing api.IngressCommonRule) api.IngressCommonRule {
 	matchesInit := matchesPodInit(es)
 	var retRule api.IngressCommonRule
 
 	if ing.FromEndpoints != nil {
 		retRule.FromEndpoints = make([]api.EndpointSelector, len(ing.FromEndpoints))
 		for j, ep := range ing.FromEndpoints {
-			retRule.FromEndpoints[j] = getEndpointSelector(namespace, ep.LabelSelector, true, matchesInit)
+			retRule.FromEndpoints[j] = getEndpointSelector(clusterName, namespace, ep.LabelSelector, true, matchesInit)
 		}
 	}
 
@@ -132,7 +149,7 @@ func parseToCiliumIngressCommonRule(namespace string, es api.EndpointSelector, i
 	if ing.FromRequires != nil {
 		retRule.FromRequires = make([]api.EndpointSelector, len(ing.FromRequires))
 		for j, ep := range ing.FromRequires {
-			retRule.FromRequires[j] = getEndpointSelector(namespace, ep.LabelSelector, false, matchesInit)
+			retRule.FromRequires[j] = getEndpointSelector(clusterName, namespace, ep.LabelSelector, false, matchesInit)
 		}
 	}
 
@@ -149,7 +166,7 @@ func parseToCiliumIngressCommonRule(namespace string, es api.EndpointSelector, i
 	return retRule
 }
 
-func parseToCiliumIngressRule(namespace string, es api.EndpointSelector, inRules []api.IngressRule) []api.IngressRule {
+func parseToCiliumIngressRule(clusterName, namespace string, es api.EndpointSelector, inRules []api.IngressRule) []api.IngressRule {
 	var retRules []api.IngressRule
 
 	if inRules != nil {
@@ -163,7 +180,7 @@ func parseToCiliumIngressRule(namespace string, es api.EndpointSelector, inRules
 				retRules[i].ICMPs = make(api.ICMPRules, len(ing.ICMPs))
 				copy(retRules[i].ICMPs, ing.ICMPs)
 			}
-			retRules[i].IngressCommonRule = parseToCiliumIngressCommonRule(namespace, es, ing.IngressCommonRule)
+			retRules[i].IngressCommonRule = parseToCiliumIngressCommonRule(clusterName, namespace, es, ing.IngressCommonRule)
 			retRules[i].Authentication = ing.Authentication.DeepCopy()
 			retRules[i].SetAggregatedSelectors()
 		}
@@ -171,7 +188,7 @@ func parseToCiliumIngressRule(namespace string, es api.EndpointSelector, inRules
 	return retRules
 }
 
-func parseToCiliumIngressDenyRule(namespace string, es api.EndpointSelector, inRules []api.IngressDenyRule) []api.IngressDenyRule {
+func parseToCiliumIngressDenyRule(clusterName, namespace string, es api.EndpointSelector, inRules []api.IngressDenyRule) []api.IngressDenyRule {
 	var retRules []api.IngressDenyRule
 
 	if inRules != nil {
@@ -185,20 +202,20 @@ func parseToCiliumIngressDenyRule(namespace string, es api.EndpointSelector, inR
 				retRules[i].ICMPs = make(api.ICMPRules, len(ing.ICMPs))
 				copy(retRules[i].ICMPs, ing.ICMPs)
 			}
-			retRules[i].IngressCommonRule = parseToCiliumIngressCommonRule(namespace, es, ing.IngressCommonRule)
+			retRules[i].IngressCommonRule = parseToCiliumIngressCommonRule(clusterName, namespace, es, ing.IngressCommonRule)
 			retRules[i].SetAggregatedSelectors()
 		}
 	}
 	return retRules
 }
 
-func parseToCiliumEgressCommonRule(namespace string, es api.EndpointSelector, egr api.EgressCommonRule) api.EgressCommonRule {
+func parseToCiliumEgressCommonRule(clusterName, namespace string, es api.EndpointSelector, egr api.EgressCommonRule) api.EgressCommonRule {
 	matchesInit := matchesPodInit(es)
 	var retRule api.EgressCommonRule
 	if egr.ToEndpoints != nil {
 		retRule.ToEndpoints = make([]api.EndpointSelector, len(egr.ToEndpoints))
 		for j, ep := range egr.ToEndpoints {
-			endpointSelector := getEndpointSelector(namespace, ep.LabelSelector, true, matchesInit)
+			endpointSelector := getEndpointSelector(clusterName, namespace, ep.LabelSelector, true, matchesInit)
 			endpointSelector.Generated = ep.Generated
 			retRule.ToEndpoints[j] = endpointSelector
 		}
@@ -217,7 +234,7 @@ func parseToCiliumEgressCommonRule(namespace string, es api.EndpointSelector, eg
 	if egr.ToRequires != nil {
 		retRule.ToRequires = make([]api.EndpointSelector, len(egr.ToRequires))
 		for j, ep := range egr.ToRequires {
-			retRule.ToRequires[j] = getEndpointSelector(namespace, ep.LabelSelector, false, matchesInit)
+			retRule.ToRequires[j] = getEndpointSelector(clusterName, namespace, ep.LabelSelector, false, matchesInit)
 		}
 	}
 
@@ -248,7 +265,7 @@ func parseToCiliumEgressCommonRule(namespace string, es api.EndpointSelector, eg
 	return retRule
 }
 
-func parseToCiliumEgressRule(namespace string, es api.EndpointSelector, inRules []api.EgressRule) []api.EgressRule {
+func parseToCiliumEgressRule(clusterName, namespace string, es api.EndpointSelector, inRules []api.EgressRule) []api.EgressRule {
 	var retRules []api.EgressRule
 
 	if inRules != nil {
@@ -269,7 +286,7 @@ func parseToCiliumEgressRule(namespace string, es api.EndpointSelector, inRules
 				copy(retRules[i].ToFQDNs, egr.ToFQDNs)
 			}
 
-			retRules[i].EgressCommonRule = parseToCiliumEgressCommonRule(namespace, es, egr.EgressCommonRule)
+			retRules[i].EgressCommonRule = parseToCiliumEgressCommonRule(clusterName, namespace, es, egr.EgressCommonRule)
 			retRules[i].Authentication = egr.Authentication
 			retRules[i].SetAggregatedSelectors()
 		}
@@ -277,7 +294,7 @@ func parseToCiliumEgressRule(namespace string, es api.EndpointSelector, inRules
 	return retRules
 }
 
-func parseToCiliumEgressDenyRule(namespace string, es api.EndpointSelector, inRules []api.EgressDenyRule) []api.EgressDenyRule {
+func parseToCiliumEgressDenyRule(clusterName, namespace string, es api.EndpointSelector, inRules []api.EgressDenyRule) []api.EgressDenyRule {
 	var retRules []api.EgressDenyRule
 
 	if inRules != nil {
@@ -293,7 +310,7 @@ func parseToCiliumEgressDenyRule(namespace string, es api.EndpointSelector, inRu
 				copy(retRules[i].ICMPs, egr.ICMPs)
 			}
 
-			retRules[i].EgressCommonRule = parseToCiliumEgressCommonRule(namespace, es, egr.EgressCommonRule)
+			retRules[i].EgressCommonRule = parseToCiliumEgressCommonRule(clusterName, namespace, es, egr.EgressCommonRule)
 			retRules[i].SetAggregatedSelectors()
 		}
 	}
@@ -318,8 +335,9 @@ func namespacesAreValid(namespace string, userNamespaces []string) bool {
 // ParseToCiliumRule returns an api.Rule with all the labels parsed into cilium
 // labels. If the namespace provided is empty then the rule is cluster scoped, this
 // might happen in case of CiliumClusterwideNetworkPolicy which enforces a policy on the cluster
-// instead of the particular namespace.
-func ParseToCiliumRule(logger *slog.Logger, namespace, name string, uid types.UID, r *api.Rule) *api.Rule {
+// instead of the particular namespace. If the clusterName is provided then the
+// policy is scoped to the local cluster in a ClusterMesh environment.
+func ParseToCiliumRule(logger *slog.Logger, clusterName, namespace, name string, uid types.UID, r *api.Rule) *api.Rule {
 	retRule := &api.Rule{}
 	if r.EndpointSelector.LabelSelector != nil {
 		retRule.EndpointSelector = api.NewESFromK8sLabelSelector("", r.EndpointSelector.LabelSelector)
@@ -349,10 +367,10 @@ func ParseToCiliumRule(logger *slog.Logger, namespace, name string, uid types.UI
 		retRule.NodeSelector = api.NewESFromK8sLabelSelector("", r.NodeSelector.LabelSelector)
 	}
 
-	retRule.Ingress = parseToCiliumIngressRule(namespace, r.EndpointSelector, r.Ingress)
-	retRule.IngressDeny = parseToCiliumIngressDenyRule(namespace, r.EndpointSelector, r.IngressDeny)
-	retRule.Egress = parseToCiliumEgressRule(namespace, r.EndpointSelector, r.Egress)
-	retRule.EgressDeny = parseToCiliumEgressDenyRule(namespace, r.EndpointSelector, r.EgressDeny)
+	retRule.Ingress = parseToCiliumIngressRule(clusterName, namespace, r.EndpointSelector, r.Ingress)
+	retRule.IngressDeny = parseToCiliumIngressDenyRule(clusterName, namespace, r.EndpointSelector, r.IngressDeny)
+	retRule.Egress = parseToCiliumEgressRule(clusterName, namespace, r.EndpointSelector, r.Egress)
+	retRule.EgressDeny = parseToCiliumEgressDenyRule(clusterName, namespace, r.EndpointSelector, r.EgressDeny)
 
 	retRule.Labels = ParseToCiliumLabels(namespace, name, uid, r.Labels)
 
diff --git a/pkg/k8s/apis/cilium.io/utils/utils_test.go b/pkg/k8s/apis/cilium.io/utils/utils_test.go
index c1837cfc53..234e35fc30 100644
--- a/pkg/k8s/apis/cilium.io/utils/utils_test.go
+++ b/pkg/k8s/apis/cilium.io/utils/utils_test.go
@@ -31,6 +31,7 @@ func Test_ParseToCiliumRule(t *testing.T) {
 	uuid := types.UID("11bba160-ddca-11e8-b697-0800273b04ff")
 	type args struct {
 		namespace      string
+		clusterName    string
 		rule           *api.Rule
 		uid            types.UID
 		overrideConfig func()
@@ -347,6 +348,99 @@ func Test_ParseToCiliumRule(t *testing.T) {
 				},
 			),
 		},
+		{
+			name: "set-cluster-by-default",
+			args: args{
+				clusterName: "cluster1",
+				namespace:   slim_metav1.NamespaceDefault,
+				uid:         uuid,
+				rule: &api.Rule{
+					EndpointSelector: api.NewESFromMatchRequirements(
+						map[string]string{
+							role: "backend",
+						},
+						nil,
+					),
+					Ingress: []api.IngressRule{
+						{
+							IngressCommonRule: api.IngressCommonRule{
+								FromEndpoints: []api.EndpointSelector{
+									{
+										LabelSelector: &slim_metav1.LabelSelector{
+											MatchLabels: map[string]string{},
+										},
+									},
+									{
+										LabelSelector: &slim_metav1.LabelSelector{
+											MatchLabels: map[string]string{
+												clusterPrefixLbl: "cluster2",
+											},
+										},
+									},
+								},
+							},
+						},
+					},
+				},
+			},
+			want: api.NewRule().WithEndpointSelector(
+				api.NewESFromMatchRequirements(
+					map[string]string{
+						role:      "backend",
+						namespace: "default",
+					},
+					nil,
+				),
+			).WithIngressRules(
+				[]api.IngressRule{
+					{
+						IngressCommonRule: api.IngressCommonRule{
+							FromEndpoints: []api.EndpointSelector{
+								api.NewESFromK8sLabelSelector(
+									labels.LabelSourceK8sKeyPrefix,
+									&slim_metav1.LabelSelector{
+										MatchLabels: map[string]string{
+											k8sConst.PodNamespaceLabel:  "default",
+											k8sConst.PolicyLabelCluster: "cluster1",
+										},
+									}),
+								api.NewESFromK8sLabelSelector(
+									labels.LabelSourceK8sKeyPrefix,
+									&slim_metav1.LabelSelector{
+										MatchLabels: map[string]string{
+											k8sConst.PodNamespaceLabel:  "default",
+											k8sConst.PolicyLabelCluster: "cluster2",
+										},
+									}),
+							},
+						},
+					},
+				},
+			).WithLabels(
+				labels.LabelArray{
+					{
+						Key:    "io.cilium.k8s.policy.derived-from",
+						Value:  "CiliumNetworkPolicy",
+						Source: labels.LabelSourceK8s,
+					},
+					{
+						Key:    "io.cilium.k8s.policy.name",
+						Value:  "set-cluster-by-default",
+						Source: labels.LabelSourceK8s,
+					},
+					{
+						Key:    "io.cilium.k8s.policy.namespace",
+						Value:  "default",
+						Source: labels.LabelSourceK8s,
+					},
+					{
+						Key:    "io.cilium.k8s.policy.uid",
+						Value:  string(uuid),
+						Source: labels.LabelSourceK8s,
+					},
+				},
+			),
+		},
 		{
 			// When the rule specifies namespace labels, namespace label is not added
 			// by the namespace where the rule was inserted.
@@ -595,7 +689,7 @@ func Test_ParseToCiliumRule(t *testing.T) {
 			} else {
 				option.Config.EnableNodeSelectorLabels = false
 			}
-			got := ParseToCiliumRule(hivetest.Logger(t), tt.args.namespace, tt.name, tt.args.uid, tt.args.rule)
+			got := ParseToCiliumRule(hivetest.Logger(t), tt.args.clusterName, tt.args.namespace, tt.name, tt.args.uid, tt.args.rule)
 
 			// Sanitize to set AggregatedSelectors field.
 			tt.want.Sanitize()
diff --git a/pkg/k8s/apis/cilium.io/v2/ccnp_types.go b/pkg/k8s/apis/cilium.io/v2/ccnp_types.go
index 9059252a47..7541be10ff 100644
--- a/pkg/k8s/apis/cilium.io/v2/ccnp_types.go
+++ b/pkg/k8s/apis/cilium.io/v2/ccnp_types.go
@@ -78,7 +78,7 @@ type CiliumClusterwideNetworkPolicyList struct {
 
 // Parse parses a CiliumClusterwideNetworkPolicy and returns a list of cilium
 // policy rules.
-func (r *CiliumClusterwideNetworkPolicy) Parse(logger *slog.Logger) (api.Rules, error) {
+func (r *CiliumClusterwideNetworkPolicy) Parse(logger *slog.Logger, clusterName string) (api.Rules, error) {
 	if r.ObjectMeta.Name == "" {
 		return nil, NewErrParse("CiliumClusterwideNetworkPolicy must have name")
 	}
@@ -96,16 +96,15 @@ func (r *CiliumClusterwideNetworkPolicy) Parse(logger *slog.Logger) (api.Rules,
 		if err := r.Spec.Sanitize(); err != nil {
 			return nil, NewErrParse(fmt.Sprintf("Invalid CiliumClusterwideNetworkPolicy spec: %s", err))
 		}
-		cr := k8sCiliumUtils.ParseToCiliumRule(logger, "", name, uid, r.Spec)
+		cr := k8sCiliumUtils.ParseToCiliumRule(logger, clusterName, "", name, uid, r.Spec)
 		retRules = append(retRules, cr)
 	}
 	if r.Specs != nil {
 		for _, rule := range r.Specs {
 			if err := rule.Sanitize(); err != nil {
 				return nil, NewErrParse(fmt.Sprintf("Invalid CiliumClusterwideNetworkPolicy specs: %s", err))
-
 			}
-			cr := k8sCiliumUtils.ParseToCiliumRule(logger, "", name, uid, rule)
+			cr := k8sCiliumUtils.ParseToCiliumRule(logger, clusterName, "", name, uid, rule)
 			retRules = append(retRules, cr)
 		}
 	}
diff --git a/pkg/k8s/apis/cilium.io/v2/cnp_types.go b/pkg/k8s/apis/cilium.io/v2/cnp_types.go
index 0ad09fa280..d6fd1d47c7 100644
--- a/pkg/k8s/apis/cilium.io/v2/cnp_types.go
+++ b/pkg/k8s/apis/cilium.io/v2/cnp_types.go
@@ -160,7 +160,7 @@ func (r *CiliumNetworkPolicy) SetDerivedPolicyStatus(derivativePolicyName string
 
 // Parse parses a CiliumNetworkPolicy and returns a list of cilium policy
 // rules.
-func (r *CiliumNetworkPolicy) Parse(logger *slog.Logger) (api.Rules, error) {
+func (r *CiliumNetworkPolicy) Parse(logger *slog.Logger, clusterName string) (api.Rules, error) {
 	if r.ObjectMeta.Name == "" {
 		return nil, NewErrParse("CiliumNetworkPolicy must have name")
 	}
@@ -177,7 +177,7 @@ func (r *CiliumNetworkPolicy) Parse(logger *slog.Logger) (api.Rules, error) {
 			Specs:      r.Specs,
 			Status:     r.Status,
 		}
-		return ccnp.Parse(logger)
+		return ccnp.Parse(logger, clusterName)
 	}
 	name := r.ObjectMeta.Name
 	uid := r.ObjectMeta.UID
@@ -195,16 +195,15 @@ func (r *CiliumNetworkPolicy) Parse(logger *slog.Logger) (api.Rules, error) {
 		if r.Spec.NodeSelector.LabelSelector != nil {
 			return nil, NewErrParse("Invalid CiliumNetworkPolicy spec: rule cannot have NodeSelector")
 		}
-		cr := k8sCiliumUtils.ParseToCiliumRule(logger, namespace, name, uid, r.Spec)
+		cr := k8sCiliumUtils.ParseToCiliumRule(logger, clusterName, namespace, name, uid, r.Spec)
 		retRules = append(retRules, cr)
 	}
 	if r.Specs != nil {
 		for _, rule := range r.Specs {
 			if err := rule.Sanitize(); err != nil {
 				return nil, NewErrParse(fmt.Sprintf("Invalid CiliumNetworkPolicy specs: %s", err))
-
 			}
-			cr := k8sCiliumUtils.ParseToCiliumRule(logger, namespace, name, uid, rule)
+			cr := k8sCiliumUtils.ParseToCiliumRule(logger, clusterName, namespace, name, uid, rule)
 			retRules = append(retRules, cr)
 		}
 	}
diff --git a/pkg/k8s/apis/cilium.io/v2/fuzz_test.go b/pkg/k8s/apis/cilium.io/v2/fuzz_test.go
index 64c70e350c..aebb7ae805 100644
--- a/pkg/k8s/apis/cilium.io/v2/fuzz_test.go
+++ b/pkg/k8s/apis/cilium.io/v2/fuzz_test.go
@@ -15,7 +15,8 @@ func FuzzCiliumNetworkPolicyParse(f *testing.F) {
 		ff := fuzz.NewConsumer(data)
 		r := &CiliumNetworkPolicy{}
 		ff.GenerateStruct(r)
-		_, _ = r.Parse(hivetest.Logger(t))
+		clusterName, _ := ff.GetString()
+		_, _ = r.Parse(hivetest.Logger(t), clusterName)
 	})
 }
 
@@ -24,6 +25,7 @@ func FuzzCiliumClusterwideNetworkPolicyParse(f *testing.F) {
 		ff := fuzz.NewConsumer(data)
 		r := &CiliumClusterwideNetworkPolicy{}
 		ff.GenerateStruct(r)
-		_, _ = r.Parse(hivetest.Logger(t))
+		clusterName, _ := ff.GetString()
+		_, _ = r.Parse(hivetest.Logger(t), clusterName)
 	})
 }
diff --git a/pkg/k8s/apis/cilium.io/v2/types_test.go b/pkg/k8s/apis/cilium.io/v2/types_test.go
index e8d346338c..b0ff7b700e 100644
--- a/pkg/k8s/apis/cilium.io/v2/types_test.go
+++ b/pkg/k8s/apis/cilium.io/v2/types_test.go
@@ -16,6 +16,7 @@ import (
 	"k8s.io/apimachinery/pkg/types"
 
 	eniTypes "github.com/cilium/cilium/pkg/aws/eni/types"
+	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	k8sConst "github.com/cilium/cilium/pkg/k8s/apis/cilium.io"
 	k8sUtils "github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils"
 	slim_metav1 "github.com/cilium/cilium/pkg/k8s/slim/k8s/apis/meta/v1"
@@ -313,7 +314,7 @@ func TestParseSpec(t *testing.T) {
 
 	logger := hivetest.Logger(t)
 
-	rules, err := expectedPolicyRule.Parse(logger)
+	rules, err := expectedPolicyRule.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.NoError(t, err)
 	require.Len(t, rules, 1)
 	require.Equal(t, *expectedSpecRule, *rules[0])
@@ -323,7 +324,7 @@ func TestParseSpec(t *testing.T) {
 	var expectedPolicyRuleUnmarshalled CiliumNetworkPolicy
 	err = json.Unmarshal(b, &expectedPolicyRuleUnmarshalled)
 	require.NoError(t, err)
-	expectedPolicyRuleUnmarshalled.Parse(logger)
+	expectedPolicyRuleUnmarshalled.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.Equal(t, *expectedPolicyRule, expectedPolicyRuleUnmarshalled)
 
 	cnpl := CiliumNetworkPolicy{}
@@ -338,7 +339,7 @@ func TestParseSpec(t *testing.T) {
 			UID:       uuidRule,
 		},
 	}
-	_, err = empty.Parse(logger)
+	_, err = empty.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.EqualValues(t, ErrEmptyCNP, err)
 
 	emptyCCNP := &CiliumClusterwideNetworkPolicy{
@@ -347,7 +348,7 @@ func TestParseSpec(t *testing.T) {
 			UID:  uuidRule,
 		},
 	}
-	_, err = emptyCCNP.Parse(logger)
+	_, err = emptyCCNP.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.EqualValues(t, ErrEmptyCCNP, err)
 }
 
@@ -405,7 +406,7 @@ func TestParseRules(t *testing.T) {
 
 	logger := hivetest.Logger(t)
 
-	rules, err := expectedPolicyRuleList.Parse(logger)
+	rules, err := expectedPolicyRuleList.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.NoError(t, err)
 	require.Len(t, rules, 2)
 	for i, rule := range rules {
@@ -417,7 +418,7 @@ func TestParseRules(t *testing.T) {
 	var expectedPolicyRuleUnmarshalled CiliumNetworkPolicy
 	err = json.Unmarshal(b, &expectedPolicyRuleUnmarshalled)
 	require.NoError(t, err)
-	expectedPolicyRuleUnmarshalled.Parse(logger)
+	expectedPolicyRuleUnmarshalled.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.Equal(t, *expectedPolicyRuleList, expectedPolicyRuleUnmarshalled)
 
 	cnpl := CiliumNetworkPolicy{}
@@ -488,7 +489,7 @@ func TestParseWithNodeSelector(t *testing.T) {
 		},
 		Spec: &rule,
 	}
-	_, err := cnpl.Parse(logger)
+	_, err := cnpl.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.ErrorContains(t, err, "Invalid CiliumNetworkPolicy spec: rule cannot have NodeSelector")
 
 	// CCNP parse is allowed to have a NodeSelector.
@@ -500,7 +501,7 @@ func TestParseWithNodeSelector(t *testing.T) {
 		},
 		Spec: cnpl.Spec,
 	}
-	_, err = ccnpl.Parse(logger)
+	_, err = ccnpl.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.NoError(t, err)
 
 	// CCNPs are received as CNP and initially parsed as CNP. Create a CNP with
@@ -513,7 +514,7 @@ func TestParseWithNodeSelector(t *testing.T) {
 		},
 		Spec: &rule,
 	}
-	_, err = ccnplAsCNP.Parse(logger)
+	_, err = ccnplAsCNP.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.NoError(t, err)
 
 	// Now test a CNP and CCNP with an EndpointSelector only.
@@ -521,11 +522,11 @@ func TestParseWithNodeSelector(t *testing.T) {
 	rule.NodeSelector = emptySelector
 
 	// CNP and CCNP parse is allowed to have an EndpointSelector.
-	_, err = cnpl.Parse(logger)
+	_, err = cnpl.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.NoError(t, err)
-	_, err = ccnpl.Parse(logger)
+	_, err = ccnpl.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.NoError(t, err)
-	_, err = ccnplAsCNP.Parse(logger)
+	_, err = ccnplAsCNP.Parse(logger, cmtypes.PolicyAnyCluster)
 	require.NoError(t, err)
 }
 
diff --git a/pkg/k8s/client/fake.go b/pkg/k8s/client/fake.go
index 09fa45e74a..2d1933b800 100644
--- a/pkg/k8s/client/fake.go
+++ b/pkg/k8s/client/fake.go
@@ -8,15 +8,18 @@ import (
 	"fmt"
 	"log/slog"
 	"os"
+	"reflect"
 	"strings"
-	"time"
+	"unsafe"
 
 	"github.com/cilium/hive"
 	"github.com/cilium/hive/cell"
 	"github.com/cilium/hive/script"
+	"github.com/cilium/statedb"
 	"github.com/spf13/pflag"
 	apiext_fake "k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset/fake"
 	"k8s.io/apimachinery/pkg/api/meta"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/runtime/schema"
 	"k8s.io/apimachinery/pkg/watch"
 	"k8s.io/client-go/discovery"
@@ -27,20 +30,23 @@ import (
 	mcsapi_fake "sigs.k8s.io/mcs-api/pkg/client/clientset/versioned/fake"
 	k8sYaml "sigs.k8s.io/yaml"
 
+	"github.com/cilium/cilium/pkg/container"
 	cilium_fake "github.com/cilium/cilium/pkg/k8s/client/clientset/versioned/fake"
 	slim_clientset "github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned"
 	slim_fake "github.com/cilium/cilium/pkg/k8s/slim/k8s/client/clientset/versioned/fake"
 	"github.com/cilium/cilium/pkg/k8s/testutils"
-	"github.com/cilium/cilium/pkg/lock"
-	"github.com/cilium/cilium/pkg/logging/logfields"
 )
 
 var FakeClientCell = cell.Module(
 	"k8s-fake-client",
 	"Fake Kubernetes client",
 
+	cell.ProvidePrivate(
+		newStateDBObjectTracker,
+	),
+
 	cell.Provide(
-		NewFakeClientset,
+		NewFakeClientsetWithTracker,
 		func(fc *FakeClientset) hive.ScriptCmdsOut {
 			return hive.NewScriptCmds(FakeClientCommands(fc))
 		},
@@ -66,9 +72,10 @@ type FakeClientset struct {
 
 	SlimFakeClientset *SlimFakeClientset
 
-	trackers map[string]k8sTesting.ObjectTracker
-
-	watchers lock.Map[string, struct{}]
+	trackers []struct {
+		domain  string
+		tracker k8sTesting.ObjectTracker
+	}
 }
 
 var _ Clientset = &FakeClientset{}
@@ -100,26 +107,32 @@ func (c *FakeClientset) RestConfig() *rest.Config {
 }
 
 func NewFakeClientset(log *slog.Logger) (*FakeClientset, Clientset) {
-	version := testutils.DefaultVersion
-	return NewFakeClientsetWithVersion(log, version)
+	return NewFakeClientsetWithTracker(log, nil)
 }
 
-// trackerPreference has the trackers in preference order,
-// e.g. which tracker to look into first for k8s/get or k8s/list.
-// We prefer the slim one over the kubernetes one as that's the one
-// likely used in Cilium.
-var trackerPreference = []string{
-	"slim",
-	"cilium",
-	"mcs",
-	"apiext",
-	"kubernetes",
+func NewFakeClientsetWithTracker(log *slog.Logger, ot *statedbObjectTracker) (*FakeClientset, Clientset) {
+	version := testutils.DefaultVersion
+	return NewFakeClientsetWithVersion(log, ot, version)
 }
 
-func NewFakeClientsetWithVersion(log *slog.Logger, version string) (*FakeClientset, Clientset) {
+func NewFakeClientsetWithVersion(log *slog.Logger, ot *statedbObjectTracker, version string) (*FakeClientset, Clientset) {
 	if version == "" {
 		version = testutils.DefaultVersion
 	}
+
+	if ot == nil {
+		// For easier use in tests we'll allow a nil [ot] and just create
+		// it from scratch here. We don't do that by default since we do
+		// want to use the main StateDB instance to make 'k8s-object-tracker'
+		// table inspectable.
+		db := statedb.New()
+		var err error
+		ot, err = newStateDBObjectTracker(db, log)
+		if err != nil {
+			panic(err)
+		}
+	}
+
 	resources, found := testutils.APIResources[version]
 	if !found {
 		panic("version " + version + " not found from testutils.APIResources")
@@ -136,12 +149,25 @@ func NewFakeClientsetWithVersion(log *slog.Logger, version string) (*FakeClients
 	client.SlimFakeClientset.Resources = resources
 	client.CiliumFakeClientset.Resources = resources
 	client.APIExtFakeClientset.Resources = resources
-	client.trackers = map[string]k8sTesting.ObjectTracker{
-		"slim":       augmentTracker(log, client.SlimFakeClientset, &client.watchers),
-		"cilium":     augmentTracker(log, client.CiliumFakeClientset, &client.watchers),
-		"mcs":        augmentTracker(log, client.MCSAPIFakeClientset, &client.watchers),
-		"kubernetes": augmentTracker(log, client.KubernetesFakeClientset, &client.watchers),
-		"apiext":     augmentTracker(log, client.APIExtFakeClientset, &client.watchers),
+
+	otx := ot.For("*", testutils.Scheme, testutils.Decoder())
+	prependReactors(client.SlimFakeClientset, otx)
+	prependReactors(client.CiliumFakeClientset, otx)
+	prependReactors(client.MCSAPIFakeClientset, otx)
+	prependReactors(client.APIExtFakeClientset, otx)
+
+	// Use a separate object tracker domain for the "kubernetes" objects. This is needed
+	// to avoid overlap with the Slim clientset since they have the same GVR but different
+	// Go types.
+	otk := ot.For("k8s", testutils.KubernetesScheme, testutils.KubernetesDecoder())
+	prependReactors(client.KubernetesFakeClientset, otk)
+
+	client.trackers = []struct {
+		domain  string
+		tracker k8sTesting.ObjectTracker
+	}{
+		{domain: "*", tracker: otx},
+		{domain: "k8s", tracker: otk},
 	}
 
 	fd := client.KubernetesFakeClientset.Discovery().(*fakediscovery.FakeDiscovery)
@@ -151,15 +177,44 @@ func NewFakeClientsetWithVersion(log *slog.Logger, version string) (*FakeClients
 	return &client, &client
 }
 
-var FakeClientBuilderCell = cell.Provide(FakeClientBuilder)
+var FakeClientBuilderCell = cell.Group(
+	cell.ProvidePrivate(newStateDBObjectTracker),
+	cell.Provide(FakeClientBuilder),
+)
 
-func FakeClientBuilder(log *slog.Logger) ClientBuilderFunc {
-	fc, _ := NewFakeClientset(log)
+func FakeClientBuilder(log *slog.Logger, ot *statedbObjectTracker) ClientBuilderFunc {
+	fc, _ := NewFakeClientsetWithTracker(log, ot)
 	return func(_ string) (Clientset, error) {
 		return fc, nil
 	}
 }
 
+type prepender interface {
+	PrependReactor(verb string, resource string, reaction k8sTesting.ReactionFunc)
+	PrependWatchReactor(resource string, reaction k8sTesting.WatchReactionFunc)
+	Tracker() k8sTesting.ObjectTracker
+}
+
+func prependReactors(cs prepender, ot *statedbObjectTracker) {
+	cs.PrependReactor("*", "*", k8sTesting.ObjectReaction(ot))
+	cs.PrependWatchReactor("*", func(action k8sTesting.Action) (handled bool, ret watch.Interface, err error) {
+		var opts metav1.ListOptions
+		if watchAction, ok := action.(k8sTesting.WatchActionImpl); ok {
+			opts = watchAction.ListOptions
+		}
+		gvr := action.GetResource()
+		ns := action.GetNamespace()
+		watch, err := ot.Watch(gvr, ns, opts)
+		if err != nil {
+			return false, nil, err
+		}
+		return true, watch, nil
+	})
+
+	// Switch out the tracker to our version.
+	overrideTracker(cs, ot)
+}
+
 func showGVR(gvr schema.GroupVersionResource) string {
 	if gvr.Group == "" {
 		return fmt.Sprintf("%s.%s", gvr.Version, gvr.Resource)
@@ -168,7 +223,8 @@ func showGVR(gvr schema.GroupVersionResource) string {
 }
 
 func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
-	seenResources := map[schema.GroupVersionKind]schema.GroupVersionResource{}
+	// Use a InsertOrderedMap to keep e.g. k8s/summary output stable.
+	seenResources := container.NewInsertOrderedMap[schema.GroupVersionKind, schema.GroupVersionResource]()
 
 	addUpdateOrDelete := func(s *script.State, action string, files []string) error {
 		for _, file := range files {
@@ -184,12 +240,13 @@ func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
 			if err != nil {
 				return fmt.Errorf("decode: %w", err)
 			}
+			kobj, _, _ := testutils.DecodeKubernetesObject(b)
 			gvr, _ := meta.UnsafeGuessKindToResource(*gvk)
 			objMeta, err := meta.Accessor(obj)
 			if err != nil {
 				return fmt.Errorf("accessor: %w", err)
 			}
-			seenResources[*gvk] = gvr
+			seenResources.Insert(*gvk, gvr)
 
 			name := objMeta.GetName()
 			ns := objMeta.GetNamespace()
@@ -201,22 +258,29 @@ func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
 			// err will get set to nil if any of the tracker methods succeed.
 			// start with a non-nil default error.
 			err = fmt.Errorf("none of the trackers of FakeClientset accepted %T", obj)
-			for trackerName, tracker := range fc.trackers {
+			for _, tc := range fc.trackers {
+				o := obj
+				if tc.domain == "k8s" {
+					o = kobj
+					if o == nil {
+						continue
+					}
+				}
 				var trackerErr error
 				switch action {
 				case "add":
-					trackerErr = tracker.Add(obj)
+					trackerErr = tc.tracker.Add(o)
 				case "update":
-					trackerErr = tracker.Update(gvr, obj, ns)
+					trackerErr = tc.tracker.Update(gvr, o, ns)
 				case "delete":
-					trackerErr = tracker.Delete(gvr, ns, name)
+					trackerErr = tc.tracker.Delete(gvr, ns, name)
 				}
 				if err != nil {
 					if trackerErr == nil {
 						// One of the trackers accepted the object, it's a success!
 						err = nil
 					} else {
-						err = errors.Join(err, fmt.Errorf("%s: %w", trackerName, trackerErr))
+						err = errors.Join(err, fmt.Errorf("%s: %w", tc.domain, trackerErr))
 					}
 				}
 			}
@@ -241,6 +305,7 @@ func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
 				if len(args) == 0 {
 					return nil, script.ErrUsage
 				}
+
 				return nil, addUpdateOrDelete(s, "add", args)
 			},
 		),
@@ -293,7 +358,7 @@ func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
 				}
 
 				var gvr schema.GroupVersionResource
-				for _, r := range seenResources {
+				for _, r := range seenResources.All() {
 					res := showGVR(r)
 					if res == args[0] {
 						gvr = r
@@ -316,9 +381,8 @@ func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
 
 				return func(s *script.State) (stdout string, stderr string, err error) {
 					var trackerErr error
-					for _, trackerName := range trackerPreference {
-						tracker := fc.trackers[trackerName]
-						obj, err := tracker.Get(gvr, ns, name)
+					for _, tc := range fc.trackers {
+						obj, err := tc.tracker.Get(gvr, ns, name)
 						if err == nil {
 							bs, err := k8sYaml.Marshal(obj)
 							if file != "" {
@@ -356,7 +420,7 @@ func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
 
 				var gvr schema.GroupVersionResource
 				var gvk schema.GroupVersionKind
-				for k, r := range seenResources {
+				for k, r := range seenResources.All() {
 					res := showGVR(r)
 					if res == args[0] {
 						gvr = r
@@ -375,9 +439,8 @@ func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
 
 				return func(s *script.State) (stdout string, stderr string, err error) {
 					var trackerErr error
-					for _, trackerName := range trackerPreference {
-						tracker := fc.trackers[trackerName]
-						obj, err := tracker.List(gvr, gvk, args[1])
+					for _, tc := range fc.trackers {
+						obj, err := tc.tracker.List(gvr, gvk, args[1])
 						if err == nil {
 							bs, err := k8sYaml.Marshal(obj)
 							if file != "" {
@@ -395,19 +458,28 @@ func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
 		"k8s/summary": script.Command(
 			script.CmdUsage{
 				Summary: "Show a summary of object trackers",
+				Args:    "(output file)",
 				Detail: []string{
-					"Lists each object tracker and the objects stored within.",
+					"Lists each object tracker and the objects stored within",
 				},
 			},
 			func(s *script.State, args ...string) (script.WaitFunc, error) {
-				for _, trackerName := range trackerPreference {
-					tracker := fc.trackers[trackerName]
-					s.Logf("%s:\n", trackerName)
-					for gvk, gvr := range seenResources {
-						objs, err := tracker.List(gvr, gvk, "")
+				out := s.LogWriter()
+				if len(args) == 1 {
+					f, err := os.OpenFile(s.Path(args[0]), os.O_CREATE|os.O_WRONLY, 0644)
+					if err != nil {
+						return nil, err
+					}
+					defer f.Close()
+					out = f
+				}
+				for _, tc := range fc.trackers {
+					fmt.Fprintf(out, "%s:\n", tc.domain)
+					for gvk, gvr := range seenResources.All() {
+						objs, err := tc.tracker.List(gvr, gvk, "")
 						if err == nil {
 							lst, _ := meta.ExtractList(objs)
-							s.Logf("- %s: %d\n", showGVR(gvr), len(lst))
+							fmt.Fprintf(out, "- %s: %d\n", showGVR(gvr), len(lst))
 						}
 					}
 				}
@@ -422,7 +494,7 @@ func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
 			func(s *script.State, args ...string) (script.WaitFunc, error) {
 				return func(s *script.State) (stdout string, stderr string, err error) {
 					var buf strings.Builder
-					for _, gvr := range seenResources {
+					for _, gvr := range seenResources.All() {
 						fmt.Fprintf(&buf, "%s\n", showGVR(gvr))
 					}
 					stdout = buf.String()
@@ -430,82 +502,23 @@ func FakeClientCommands(fc *FakeClientset) map[string]script.Cmd {
 				}, nil
 			},
 		),
-
-		"k8s/wait-watchers": script.Command(
-			script.CmdUsage{
-				Summary: "Wait for watchers for given resources to appear",
-				Detail: []string{
-					"Takes a list of resources and waits for a Watch() to appear for it.",
-					"",
-					"Useful when working with an informer/reflector that is not backed by",
-					"a StateDB table and thus cannot use 'db/initialized'.",
-				},
-				Args: "resources...",
-			},
-			func(s *script.State, args ...string) (script.WaitFunc, error) {
-				resources := map[string]struct{}{}
-				for _, r := range args {
-					resources[r] = struct{}{}
-				}
-				for s.Context().Err() == nil && len(resources) > 0 {
-					for r := range resources {
-						_, ok := fc.watchers.Load(r)
-						if ok {
-							delete(resources, r)
-						}
-					}
-					time.Sleep(10 * time.Millisecond)
-				}
-				if len(resources) > 0 {
-					seen := []string{}
-					fc.watchers.Range(func(key string, value struct{}) bool {
-						seen = append(seen, key)
-						return true
-					})
-					return nil, fmt.Errorf("watchers did not appear. saw: %v", seen)
-				}
-				return nil, nil
-			},
-		),
 	}
-
-}
-
-type fakeWithTracker interface {
-	PrependReactor(verb string, resource string, reaction k8sTesting.ReactionFunc)
-	PrependWatchReactor(resource string, reaction k8sTesting.WatchReactionFunc)
-	Tracker() k8sTesting.ObjectTracker
 }
 
-// augmentTracker augments the fake clientset to record watchers.
-// The reason we need to do this is the following: The k8s object tracker's implementation
-// of Watch is not equivalent to Watch on a real api-server, as it does not respect the
-// ResourceVersion from whence to start the watch. As a consequence, when informers (or
-// reflectors) call ListAndWatch, they miss events which occur between the end of List and
-// the establishment of Watch.
-func augmentTracker[T fakeWithTracker](log *slog.Logger, f T, watchers *lock.Map[string, struct{}]) k8sTesting.ObjectTracker {
-	o := f.Tracker()
-
-	f.PrependWatchReactor(
-		"*",
-		func(action k8sTesting.Action) (handled bool, ret watch.Interface, err error) {
-			w := action.(k8sTesting.WatchAction)
-			gvr := w.GetResource()
-			ns := w.GetNamespace()
-			watch, err := o.Watch(gvr, ns)
-			if err != nil {
-				return false, nil, err
-			}
-			watchName := showGVR(gvr)
-			if _, ok := watchers.Load(watchName); ok {
-				log.Warn("Multiple watches for resource intercepted. This highlights a potential cause for flakes", logfields.Resource, watchName)
-			}
-
-			log.Debug("Watch started", logfields.Resource, watchName)
-			watchers.Store(watchName, struct{}{})
+// overrideTracker changes the internal 'tracker' field in the generated
+// clientset to point to our object tracker. This allows using the Tracker()
+// method without ending up getting the wrong one.
+func overrideTracker(cs prepender, ot k8sTesting.ObjectTracker) {
+	type fakeLayout struct {
+		k8sTesting.Fake
+		discovery uintptr
+		tracker   k8sTesting.ObjectTracker
+	}
 
-			return true, watch, nil
-		})
+	f := (*fakeLayout)(unsafe.Pointer(reflect.ValueOf(cs).Pointer()))
+	f.tracker = ot
 
-	return o
+	if cs.Tracker() != ot {
+		panic("overrideTracker failed, layout changed?")
+	}
 }
diff --git a/pkg/k8s/client/object_tracker.go b/pkg/k8s/client/object_tracker.go
new file mode 100644
index 0000000000..32f41045e4
--- /dev/null
+++ b/pkg/k8s/client/object_tracker.go
@@ -0,0 +1,652 @@
+// SPDX-License-Identifier: Apache-2.0
+// Copyright Authors of Cilium
+
+package client
+
+import (
+	"encoding/json"
+	"fmt"
+	"log/slog"
+	"reflect"
+	"strconv"
+	"strings"
+	"sync"
+
+	"github.com/cilium/statedb"
+	"github.com/cilium/statedb/index"
+	apierrors "k8s.io/apimachinery/pkg/api/errors"
+	"k8s.io/apimachinery/pkg/api/meta"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/fields"
+	"k8s.io/apimachinery/pkg/runtime"
+	"k8s.io/apimachinery/pkg/runtime/schema"
+	"k8s.io/apimachinery/pkg/selection"
+	"k8s.io/apimachinery/pkg/types"
+	"k8s.io/apimachinery/pkg/util/sets"
+	"k8s.io/apimachinery/pkg/util/strategicpatch"
+	"k8s.io/apimachinery/pkg/watch"
+	"k8s.io/client-go/testing"
+	"k8s.io/client-go/util/jsonpath"
+
+	"github.com/cilium/cilium/pkg/k8s/testutils"
+	"github.com/cilium/cilium/pkg/logging/logfields"
+)
+
+const (
+	logfieldGVR             = "gvr" //  GroupVersIonResource
+	logfieldClientset       = "clientset"
+	logfieldResourceVersion = "resourceVersion"
+	logfieldFieldSelector   = "fieldSelector"
+)
+
+// statedbObjectTracker implements [testing.ObjectTracker] using a StateDB table.
+// This allows for proper implementation of Watch() that respects the ResourceVersion
+// allowing multiple Watch() calls in parallel.
+//
+// This object tracker is prepended to the reactor chain and will process the objects
+// and short-circuit the chain before the fake client's own object tracker.
+//
+// https://pkg.go.dev/k8s.io/client-go/testing#ObjectTracker
+type statedbObjectTracker struct {
+	domain  string
+	log     *slog.Logger
+	db      *statedb.DB
+	scheme  testing.ObjectScheme
+	decoder runtime.Decoder
+	tbl     statedb.RWTable[object]
+}
+
+func newStateDBObjectTracker(db *statedb.DB, log *slog.Logger) (*statedbObjectTracker, error) {
+	tbl, err := statedb.NewTable("k8s-object-tracker", objectIndex)
+	if err != nil {
+		return nil, err
+	}
+	if err := db.RegisterTable(tbl); err != nil {
+		return nil, err
+	}
+	return &statedbObjectTracker{
+		log:     log,
+		db:      db,
+		tbl:     tbl,
+		scheme:  testutils.Scheme,
+		decoder: testutils.Decoder(),
+	}, nil
+}
+
+type object struct {
+	objectId
+	o runtime.Object
+}
+
+func (o object) TableHeader() []string {
+	return []string{
+		"ID",
+		"Type",
+	}
+}
+
+func (o object) TableRow() []string {
+	return []string{
+		o.objectId.String(),
+		fmt.Sprintf("%T", o.o),
+	}
+}
+
+type objectId struct {
+	types.NamespacedName
+	gvr schema.GroupVersionResource
+
+	// domain to which this objects belongs, e.g. this is used
+	// to differentiate between the slim and kubernetes clientsets
+	// and avoid mixing them up since they have the same [gvr]
+	domain string
+}
+
+func newObjectId(clientset string, gvr schema.GroupVersionResource, namespace, name string) (o objectId) {
+	o.domain = clientset
+	o.gvr = gvr
+	o.Namespace = namespace
+	o.Name = name
+	return
+}
+
+func (oid objectId) String() string {
+	return oid.domain + ";" + oid.gvr.String() + ";" + oid.NamespacedName.String()
+}
+
+func (oid objectId) Key() index.Key {
+	return index.Stringer(oid)
+}
+
+var (
+	objectIndex = statedb.Index[object, objectId]{
+		Name: "id",
+		FromObject: func(obj object) index.KeySet {
+			return index.NewKeySet(obj.Key())
+		},
+		FromKey: index.Stringer[objectId],
+		FromString: func(key string) (index.Key, error) {
+			return index.String(key), nil
+		},
+		Unique: true,
+	}
+)
+
+// For returns a object tracker for a specific use-case (domain) that is separate from others.
+func (s *statedbObjectTracker) For(domain string, scheme *runtime.Scheme, decoder runtime.Decoder) *statedbObjectTracker {
+	o := *s
+	o.domain = domain
+	o.scheme = scheme
+	o.decoder = decoder
+	return &o
+}
+
+func (s *statedbObjectTracker) ObjectReaction() testing.ReactionFunc {
+	return testing.ObjectReaction(s)
+}
+
+func (s *statedbObjectTracker) addList(obj runtime.Object) error {
+	list, err := meta.ExtractList(obj)
+	if err != nil {
+		return err
+	}
+	errs := runtime.DecodeList(list, s.decoder)
+	if len(errs) > 0 {
+		return errs[0]
+	}
+	for _, obj := range list {
+		if err := s.Add(obj); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+// Add adds an object to the tracker. If object being added
+// is a list, its items are added separately.
+func (s *statedbObjectTracker) Add(obj runtime.Object) error {
+	if meta.IsListType(obj) {
+		return s.addList(obj)
+	}
+
+	wtxn := s.db.WriteTxn(s.tbl)
+	defer wtxn.Commit()
+
+	obj = obj.DeepCopyObject()
+	objMeta, err := meta.Accessor(obj)
+	if err != nil {
+		s.log.Debug("Add", logfields.Error, err)
+		return err
+	}
+
+	version := s.tbl.Revision(wtxn) + 1
+	objMeta.SetResourceVersion(strconv.FormatUint(version, 10))
+
+	gvks, _, err := s.scheme.ObjectKinds(obj)
+	if err != nil {
+		s.log.Debug("Add", logfields.Error, err)
+		return err
+	}
+
+	if partial, ok := obj.(*metav1.PartialObjectMetadata); ok && len(partial.TypeMeta.APIVersion) > 0 {
+		gvks = []schema.GroupVersionKind{partial.TypeMeta.GroupVersionKind()}
+	}
+
+	if len(gvks) == 0 {
+		err := fmt.Errorf("no registered kinds for %v", obj)
+		s.log.Debug("Add", logfields.Error, err)
+		return err
+	}
+	for _, gvk := range gvks {
+		// NOTE: UnsafeGuessKindToResource is a heuristic and default match. The
+		// actual registration in apiserver can specify arbitrary route for a
+		// gvk. If a test uses such objects, it cannot preset the tracker with
+		// objects via Add(). Instead, it should trigger the Create() function
+		// of the tracker, where an arbitrary gvr can be specified.
+		gvr, _ := meta.UnsafeGuessKindToResource(gvk)
+		// Resource doesn't have the concept of "__internal" version, just set it to "".
+		if gvr.Version == runtime.APIVersionInternal {
+			gvr.Version = ""
+		}
+
+		s.log.Debug(
+			"Add",
+			logfieldClientset, s.domain,
+			logfieldGVR, gvr,
+			logfields.K8sNamespace, objMeta.GetNamespace(),
+			logfields.Name, objMeta.GetName(),
+		)
+
+		s.tbl.Insert(wtxn, object{
+			objectId: newObjectId(s.domain, gvr, objMeta.GetNamespace(), objMeta.GetName()),
+			o:        obj})
+	}
+	return nil
+}
+
+// Apply applies an object in the tracker in the specified namespace.
+func (s *statedbObjectTracker) Apply(gvr schema.GroupVersionResource, applyConfiguration runtime.Object, ns string, opts ...metav1.PatchOptions) error {
+	log := s.log.With(
+		logfieldClientset, s.domain,
+		logfields.Object, applyConfiguration)
+
+	applyConfigurationMeta, err := meta.Accessor(applyConfiguration)
+	if err != nil {
+		log.Debug("Apply", logfields.Error, err)
+		return err
+	}
+
+	obj, err := s.Get(gvr, ns, applyConfigurationMeta.GetName(), metav1.GetOptions{})
+	if err != nil {
+		log.Debug("Apply", logfields.Error, err)
+		return err
+	}
+
+	old, err := json.Marshal(obj)
+	if err != nil {
+		log.Debug("Apply", logfields.Error, err)
+		return err
+	}
+
+	// reset the object in preparation to unmarshal, since unmarshal does not guarantee that fields
+	// in obj that are removed by patch are cleared
+	value := reflect.ValueOf(obj)
+	value.Elem().Set(reflect.New(value.Type().Elem()).Elem())
+
+	// For backward compatibility with behavior 1.30 and earlier, continue to handle apply
+	// via strategic merge patch (clients may use fake.NewClientset and ManagedFieldObjectTracker
+	// for full field manager support).
+	patch, err := json.Marshal(applyConfiguration)
+	if err != nil {
+		log.Debug("Apply", logfields.Error, err)
+		return err
+	}
+	mergedByte, err := strategicpatch.StrategicMergePatch(old, patch, obj)
+	if err != nil {
+		log.Debug("Apply", logfields.Error, err)
+		return err
+	}
+	if err = json.Unmarshal(mergedByte, obj); err != nil {
+		log.Debug("Apply", logfields.Error, err)
+		return err
+	}
+
+	err = s.Update(gvr, obj, ns)
+	s.log.Debug("Apply", logfields.Error, err)
+	return err
+}
+
+// Create adds an object to the tracker in the specified namespace. If the object exists an error is returned.
+func (s *statedbObjectTracker) Create(gvr schema.GroupVersionResource, obj runtime.Object, ns string, opts ...metav1.CreateOptions) error {
+	log := s.log.With(
+		logfieldClientset, s.domain,
+		logfields.Object, obj)
+
+	obj = obj.DeepCopyObject()
+	newMeta, err := meta.Accessor(obj)
+	if err != nil {
+		return err
+	}
+	if len(newMeta.GetNamespace()) == 0 {
+		newMeta.SetNamespace(ns)
+	}
+	wtxn := s.db.WriteTxn(s.tbl)
+	_, found, _ := s.tbl.Insert(wtxn, object{
+		objectId: newObjectId(s.domain, gvr, ns, newMeta.GetName()),
+		o:        obj})
+	if found {
+		wtxn.Abort()
+		gr := gvr.GroupResource()
+		err := apierrors.NewAlreadyExists(gr, newMeta.GetName())
+		log.Debug("Create", logfields.Error, err)
+	}
+	log.Debug("Create")
+	wtxn.Commit()
+	return nil
+}
+
+// Delete deletes an existing object from the tracker. If object
+// didn't exist in the tracker prior to deletion, Delete returns
+// no error.
+func (s *statedbObjectTracker) Delete(gvr schema.GroupVersionResource, ns string, name string, opts ...metav1.DeleteOptions) error {
+	log := s.log.With(
+		logfieldClientset, s.domain,
+		logfields.K8sNamespace, ns,
+		logfields.Name, name)
+
+	wtxn := s.db.WriteTxn(s.tbl)
+	_, found, _ := s.tbl.Delete(wtxn, object{objectId: newObjectId(s.domain, gvr, ns, name)})
+	if !found {
+		wtxn.Abort()
+		err := apierrors.NewNotFound(gvr.GroupResource(), name)
+		log.Debug("Delete", logfields.Error, err)
+		return err
+	}
+	wtxn.Commit()
+	log.Debug("Delete")
+	return nil
+}
+
+// Get retrieves the object by its kind, namespace and name.
+// Returns an error if object is not found.
+func (s *statedbObjectTracker) Get(gvr schema.GroupVersionResource, ns string, name string, opts ...metav1.GetOptions) (runtime.Object, error) {
+	log := s.log.With(
+		logfieldClientset, s.domain,
+		logfieldGVR, gvr,
+		logfields.K8sNamespace, ns,
+		logfields.Name, name)
+
+	txn := s.db.ReadTxn()
+	obj, rev, found := s.tbl.Get(txn, objectIndex.Query(newObjectId(s.domain, gvr, ns, name)))
+	if !found {
+		err := apierrors.NewNotFound(gvr.GroupResource(), name)
+		log.Debug("Get", logfields.Error, err)
+		return nil, err
+	}
+	log.Debug("Get", logfieldResourceVersion, rev)
+	return obj.o.DeepCopyObject(), nil
+}
+
+// List retrieves all objects of a given kind in the given
+// namespace. Only non-List kinds are accepted.
+func (s *statedbObjectTracker) List(gvr schema.GroupVersionResource, gvk schema.GroupVersionKind, ns string, opts ...metav1.ListOptions) (runtime.Object, error) {
+	// Heuristic for list kind: original kind + List suffix. Might
+	// not always be true but this tracker has a pretty limited
+	// understanding of the actual API model.
+	listGVK := gvk
+	listGVK.Kind = listGVK.Kind + "List"
+	// GVK does have the concept of "internal version". The scheme recognizes
+	// the runtime.APIVersionInternal, but not the empty string.
+	if listGVK.Version == "" {
+		listGVK.Version = runtime.APIVersionInternal
+	}
+
+	list, err := s.scheme.New(listGVK)
+	if err != nil {
+		return nil, err
+	}
+
+	if !meta.IsListType(list) {
+		return nil, fmt.Errorf("%q is not a list type", listGVK.Kind)
+	}
+
+	var fieldSelector fields.Selector
+	if len(opts) > 0 {
+		opt := opts[0]
+		if opt.FieldSelector != "" {
+			fieldSelector, err = fields.ParseSelector(opt.FieldSelector)
+			if err != nil {
+				return nil, err
+			}
+		}
+	}
+
+	matchingObjects := []runtime.Object{}
+	txn := s.db.ReadTxn()
+
+	for obj, rev := range s.tbl.All(txn) {
+		if obj.domain != s.domain || obj.gvr != gvr ||
+			(ns != "" && obj.Namespace != ns) {
+			continue
+		}
+
+		if fieldSelector != nil {
+			if !objectMatchesFieldSelector(obj.o, fieldSelector) {
+				s.log.Debug("List: Skipping object due to FieldSelector mismatch",
+					logfields.K8sNamespace, ns,
+					logfields.Name, obj.Name,
+					logfieldFieldSelector, fieldSelector)
+				continue
+			}
+		}
+
+		o := obj.o.DeepCopyObject()
+		m, _ := meta.Accessor(o)
+		m.SetResourceVersion(strconv.FormatUint(rev, 10))
+		matchingObjects = append(matchingObjects, o)
+	}
+	m, _ := meta.ListAccessor(list)
+	m.SetResourceVersion(strconv.FormatUint(s.tbl.Revision(txn), 10))
+
+	if err := meta.SetList(list, matchingObjects); err != nil {
+		return nil, err
+	}
+
+	s.log.Debug(
+		"List",
+		logfieldClientset, s.domain,
+		logfieldGVR, gvr,
+		logfields.K8sNamespace, ns,
+		logfields.Count, len(matchingObjects),
+	)
+	return list, nil
+}
+
+// Patch patches an existing object in the tracker in the specified namespace.
+//
+// The reactor functions take care of actually processing the patch
+// (objectTrackerReact.Patch in client-go/testing/fixture.go).
+func (s *statedbObjectTracker) Patch(gvr schema.GroupVersionResource, obj runtime.Object, ns string, opts ...metav1.PatchOptions) error {
+	return s.updateOrPatch("Patch", gvr, obj, ns)
+}
+
+// Update updates an existing object in the tracker in the specified namespace.
+// If the object does not exist an error is returned.
+func (s *statedbObjectTracker) Update(gvr schema.GroupVersionResource, obj runtime.Object, ns string, opts ...metav1.UpdateOptions) error {
+	return s.updateOrPatch("Update", gvr, obj, ns)
+}
+
+func (s *statedbObjectTracker) updateOrPatch(what string, gvr schema.GroupVersionResource, obj runtime.Object, ns string, opts ...metav1.UpdateOptions) error {
+	obj = obj.DeepCopyObject()
+	newMeta, err := meta.Accessor(obj)
+	if err != nil {
+		s.log.Debug(what, logfields.Error, err)
+		return err
+	}
+	if len(newMeta.GetNamespace()) == 0 {
+		newMeta.SetNamespace(ns)
+	}
+	wtxn := s.db.WriteTxn(s.tbl)
+	version := s.tbl.Revision(wtxn) + 1
+	newMeta.SetResourceVersion(strconv.FormatUint(version, 10))
+
+	log := s.log.With(
+		logfieldClientset, s.domain,
+		logfields.Object, obj,
+		logfieldResourceVersion, version)
+
+	_, found, _ := s.tbl.Insert(wtxn, object{objectId: newObjectId(s.domain, gvr, ns, newMeta.GetName()), o: obj})
+	if !found {
+		wtxn.Abort()
+		gr := gvr.GroupResource()
+		err := apierrors.NewNotFound(gr, newMeta.GetName())
+		log.Debug(what, logfields.Error, err)
+		return err
+	}
+	wtxn.Commit()
+	log.Debug(what)
+	return nil
+}
+
+// Watch watches objects from the tracker. Watch returns a channel
+// which will push added / modified / deleted object.
+func (s *statedbObjectTracker) Watch(gvr schema.GroupVersionResource, ns string, opts ...metav1.ListOptions) (watch.Interface, error) {
+	wtxn := s.db.WriteTxn(s.tbl)
+	changeIter, err := s.tbl.Changes(wtxn)
+	wtxn.Commit()
+	if err != nil {
+		// Impossible
+		panic(err)
+	}
+
+	var fieldSelector fields.Selector
+	version := uint64(0)
+	if len(opts) > 0 && opts[0].ResourceVersion != "" {
+		opt := opts[0]
+		version, err = strconv.ParseUint(opt.ResourceVersion, 10, 64)
+		if err != nil {
+			return nil, err
+		}
+
+		if opt.FieldSelector != "" {
+			fieldSelector, err = fields.ParseSelector(opt.FieldSelector)
+			if err != nil {
+				return nil, err
+			}
+		}
+	}
+
+	s.log.Debug("Watch",
+		logfieldClientset, s.domain,
+		logfieldGVR, gvr,
+		logfields.K8sNamespace, ns,
+		logfieldResourceVersion, version)
+
+	w := &statedbWatch{
+		clientset:     s.domain,
+		log:           s.log,
+		version:       version,
+		gvr:           gvr,
+		ns:            ns,
+		db:            s.db,
+		iter:          changeIter,
+		stop:          make(chan struct{}),
+		stopped:       make(chan struct{}),
+		events:        make(chan watch.Event, 1),
+		fieldSelector: fieldSelector,
+	}
+	go w.feed()
+
+	return w, nil
+}
+
+type statedbWatch struct {
+	clientset     string
+	log           *slog.Logger
+	gvr           schema.GroupVersionResource
+	ns            string
+	version       statedb.Revision
+	db            *statedb.DB
+	iter          statedb.ChangeIterator[object]
+	stop          chan struct{}
+	stopOnce      sync.Once
+	stopped       chan struct{}
+	events        chan watch.Event
+	fieldSelector fields.Selector
+}
+
+// ResultChan implements watch.Interface.
+func (w *statedbWatch) ResultChan() <-chan watch.Event {
+	return w.events
+}
+
+func (w *statedbWatch) feed() {
+	defer close(w.stopped)
+	defer close(w.events)
+	seen := sets.New[string]()
+	for {
+		changes, changesWatch := w.iter.Next(w.db.ReadTxn())
+		for change := range changes {
+			if change.Revision <= w.version {
+				continue
+			}
+			obj := change.Object
+			if obj.domain != w.clientset {
+				continue
+			}
+			if (w.ns != "" && obj.Namespace != w.ns) || obj.gvr != w.gvr {
+				continue
+			}
+
+			if w.fieldSelector != nil {
+				if !objectMatchesFieldSelector(obj.o, w.fieldSelector) {
+					w.log.Debug("Watch: Skipping event due to FieldSelector mismatch",
+						logfieldFieldSelector, w.fieldSelector)
+					continue
+				}
+			}
+
+			var ev watch.Event
+			ev.Object = obj.o.DeepCopyObject()
+
+			switch {
+			case change.Deleted:
+				ev.Type = watch.Deleted
+				seen.Delete(obj.Name)
+			case seen.Has(obj.Name):
+				ev.Type = watch.Modified
+			default:
+				ev.Type = watch.Added
+				seen.Insert(obj.Name)
+			}
+			w.log.Debug(
+				"Event",
+				logfieldGVR, obj.gvr,
+				logfields.K8sNamespace, obj.Namespace,
+				logfields.Name, obj.Name,
+				logfields.Type, ev.Type,
+				logfieldResourceVersion, change.Revision,
+				logfields.Object, ev.Object,
+			)
+			select {
+			case w.events <- ev:
+			case <-w.stop:
+				return
+			}
+		}
+		select {
+		case <-w.stop:
+			return
+		case <-changesWatch:
+		}
+
+	}
+}
+
+// Stop implements watch.Interface.
+func (w *statedbWatch) Stop() {
+	w.stopOnce.Do(func() {
+		close(w.stop)
+	})
+	<-w.stopped
+}
+
+var _ watch.Interface = &statedbWatch{}
+
+var _ testing.ObjectTracker = &statedbObjectTracker{}
+
+func objectMatchesFieldSelector(obj runtime.Object, sel fields.Selector) bool {
+	for _, req := range sel.Requirements() {
+		value, err := getFieldPathValue(obj, req.Field)
+		if err != nil {
+			return false
+		}
+		// https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/#supported-operators
+		// Only =, == and != are supported.
+		switch req.Operator {
+		case selection.DoubleEquals, selection.Equals:
+			if value != req.Value {
+				return false
+			}
+		case selection.NotEquals:
+			if value == req.Value {
+				return false
+			}
+		default:
+			panic(fmt.Sprintf("unsupported operator: %q", req.Operator))
+		}
+	}
+	return true
+}
+
+func getFieldPathValue(obj interface{}, fieldPath string) (string, error) {
+	p := jsonpath.New("").AllowMissingKeys(false)
+	if err := p.Parse("{$." + fieldPath + "}"); err != nil {
+		return "", err
+	}
+	var b strings.Builder
+	if err := p.Execute(&b, obj); err != nil {
+		return "", err
+	}
+	return b.String(), nil
+}
diff --git a/pkg/k8s/client/restConfig_provider.go b/pkg/k8s/client/restConfig_provider.go
index 2dfbf4689b..03293f95a0 100644
--- a/pkg/k8s/client/restConfig_provider.go
+++ b/pkg/k8s/client/restConfig_provider.go
@@ -320,7 +320,7 @@ func (r *restConfigManager) startK8sAPIServerFileWatcher() error {
 			err     error
 		)
 		wait.Until(func() {
-			watcher, err = fswatcher.New([]string{K8sAPIServerFilePath})
+			watcher, err = fswatcher.New(r.log, []string{K8sAPIServerFilePath})
 			if err == nil {
 				close(stop)
 				return
diff --git a/pkg/k8s/client/script_test.go b/pkg/k8s/client/script_test.go
new file mode 100644
index 0000000000..8c45e44d26
--- /dev/null
+++ b/pkg/k8s/client/script_test.go
@@ -0,0 +1,48 @@
+// SPDX-License-Identifier: Apache-2.0
+// Copyright Authors of Cilium
+
+package client
+
+import (
+	"context"
+	"log/slog"
+	"maps"
+	"testing"
+
+	"github.com/cilium/hive/hivetest"
+	"github.com/cilium/hive/script"
+	"github.com/cilium/hive/script/scripttest"
+	"github.com/spf13/pflag"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+
+	"github.com/cilium/cilium/pkg/hive"
+	"github.com/cilium/cilium/pkg/time"
+)
+
+// TestScript runs all the testdata/*.txtar script tests. The tests are
+// run in parallel. If you need to update the expected files inside the txtar
+// files you can run 'go test . -scripttest.update' to update the files.
+func TestScript(t *testing.T) {
+	log := hivetest.Logger(t, hivetest.LogLevel(slog.LevelDebug))
+	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
+	t.Cleanup(cancel)
+	scripttest.Test(t,
+		ctx,
+		func(t testing.TB, args []string) *script.Engine {
+			h := hive.New(
+				FakeClientCell,
+			)
+			flags := pflag.NewFlagSet("", pflag.ContinueOnError)
+			h.RegisterFlags(flags)
+			t.Cleanup(func() {
+				assert.NoError(t, h.Stop(log, context.TODO()))
+			})
+			cmds, err := h.ScriptCommands(log)
+			require.NoError(t, err, "ScriptCommands")
+			maps.Insert(cmds, maps.All(script.DefaultCmds()))
+			return &script.Engine{
+				Cmds: cmds,
+			}
+		}, []string{}, "testdata/*.txtar")
+}
diff --git a/pkg/k8s/client/testdata/fake.txtar b/pkg/k8s/client/testdata/fake.txtar
new file mode 100644
index 0000000000..56e7908a46
--- /dev/null
+++ b/pkg/k8s/client/testdata/fake.txtar
@@ -0,0 +1,137 @@
+# Tests for the fake k8s client and our custom object tracker.
+#
+# This adds+updates an object from each of the clientsets and checks that
+# they get correctly indexed and we can retrieve them via the k8s script commands.
+
+hive/start
+
+# Add object for the Kubernetes and Slim clientsets
+k8s/add service.yaml
+k8s/update service.yaml
+
+# Add object just for the Kubernetes clientset (has no slim counterpart)
+k8s/add limitrange.yaml
+k8s/update limitrange.yaml
+
+# Add object for the Cilium clientset
+k8s/add ciliumenvoyconfig.yaml
+k8s/update ciliumenvoyconfig.yaml
+
+# Add object for the apiext clientset
+k8s/add apiext_crd.yaml
+k8s/update apiext_crd.yaml
+
+# Add object for the MCSAPI clientset
+k8s/add mcs_svcexport.yaml
+k8s/update mcs_svcexport.yaml
+
+# Validate the summary to see that we've indexed everything
+k8s/summary summary.actual
+cmp summary.expected summary.actual
+
+# Verify that we can retrieve the service (this will prefer slim)
+k8s/get v1.services test/echo -o actual.yaml
+grep 'kind: Service' actual.yaml
+grep 'name: echo' actual.yaml
+
+# Verify that we can retrieve the limitrange from the Kubernetes clientset
+k8s/get v1.limitranges bar/foo -o actual.yaml
+grep 'kind: LimitRange' actual.yaml
+grep 'name: foo' actual.yaml
+
+# Verify that we can retrieve the service export from the MCS clientset
+k8s/get multicluster.x-k8s.io.v1alpha1.serviceexports test -o actual.yaml
+grep 'kind: ServiceExport' actual.yaml
+grep 'name: test' actual.yaml
+
+# Validate the table output of 'k8s-object-tracker'
+db/cmp k8s-object-tracker object-tracker.table
+
+# Delete objects from each clientset
+k8s/delete service.yaml limitrange.yaml ciliumenvoyconfig.yaml apiext_crd.yaml mcs_svcexport.yaml
+k8s/summary summary.actual
+cmp summary.empty summary.actual
+db/empty k8s-object-tracker
+
+###
+
+-- object-tracker.table --
+ID                                                                                           Type
+*;/v1, Resource=services;test/echo                                                           *v1.Service
+*;apiextensions.k8s.io/v1, Resource=customresourcedefinitions;/ciliumenvoyconfigs.cilium.io  *v1.CustomResourceDefinition
+*;cilium.io/v2, Resource=ciliumenvoyconfigs;default/cec                                      *v2.CiliumEnvoyConfig
+*;multicluster.x-k8s.io/v1alpha1, Resource=serviceexports;/test                              *v1alpha1.ServiceExport
+k8s;/v1, Resource=limitranges;bar/foo                                                        *v1.LimitRange
+k8s;/v1, Resource=services;test/echo                                                         *v1.Service
+
+-- summary.expected --
+*:
+- v1.services: 1
+- cilium.io.v2.ciliumenvoyconfigs: 1
+- apiextensions.k8s.io.v1.customresourcedefinitions: 1
+- multicluster.x-k8s.io.v1alpha1.serviceexports: 1
+k8s:
+- v1.services: 1
+- v1.limitranges: 1
+-- summary.empty --
+*:
+- v1.services: 0
+- cilium.io.v2.ciliumenvoyconfigs: 0
+- apiextensions.k8s.io.v1.customresourcedefinitions: 0
+- multicluster.x-k8s.io.v1alpha1.serviceexports: 0
+k8s:
+- v1.services: 0
+- v1.limitranges: 0
+-- service.yaml --
+apiVersion: v1
+kind: Service
+metadata:
+  name: echo
+  namespace: test
+  resourceVersion: "12345" # Will be ignored
+  uid: a49fe99c-3564-4754-acc4-780f2331a49b
+spec:
+  clusterIP: 10.96.50.104
+
+-- limitrange.yaml --
+apiVersion: v1
+kind: LimitRange
+metadata:
+  name: foo
+  namespace: bar
+
+-- ciliumenvoyconfig.yaml --
+apiVersion: cilium.io/v2
+kind: CiliumEnvoyConfig
+metadata:
+  name: cec
+  namespace: default
+  resourceVersion: "12345" # Will be ignored
+  uid: 094b684c-6a6a-4313-b07b-c7c124da8d1f
+spec:
+  backendServices:
+  - name: foo
+
+-- apiext_crd.yaml --
+apiVersion: apiextensions.k8s.io/v1
+kind: CustomResourceDefinition
+metadata:
+  name: ciliumenvoyconfigs.cilium.io
+spec:
+  group: cilium.io
+  names:
+    categories:
+    - cilium
+    kind: CiliumEnvoyConfig
+    listKind: CiliumEnvoyConfigList
+    plural: ciliumenvoyconfigs
+    shortNames:
+    - cec
+    singular: ciliumenvoyconfig
+  scope: Namespaced
+
+-- mcs_svcexport.yaml --
+apiVersion: multicluster.x-k8s.io/v1alpha1
+kind: ServiceExport
+metadata:
+  name: test
diff --git a/pkg/k8s/network_policy.go b/pkg/k8s/network_policy.go
index 397911c4b7..dc419e0dd6 100644
--- a/pkg/k8s/network_policy.go
+++ b/pkg/k8s/network_policy.go
@@ -10,6 +10,7 @@ import (
 	"slices"
 
 	"github.com/cilium/cilium/pkg/annotation"
+	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	k8sConst "github.com/cilium/cilium/pkg/k8s/apis/cilium.io"
 	k8sCiliumUtils "github.com/cilium/cilium/pkg/k8s/apis/cilium.io/utils"
 	slim_networkingv1 "github.com/cilium/cilium/pkg/k8s/slim/k8s/api/networking/v1"
@@ -56,12 +57,39 @@ func GetPolicyLabelsv1(logger *slog.Logger, np *slim_networkingv1.NetworkPolicy)
 	return k8sCiliumUtils.GetPolicyLabels(ns, policyName, policyUID, resourceTypeNetworkPolicy)
 }
 
-func parseNetworkPolicyPeer(namespace string, peer *slim_networkingv1.NetworkPolicyPeer) *api.EndpointSelector {
+func isPodSelectorSelectingCluster(podSelector *slim_metav1.LabelSelector) bool {
+	if podSelector == nil {
+		return false
+	}
+	if podSelector.MatchLabels[k8sConst.PolicyLabelCluster] != "" {
+		return true
+	}
+	for _, expr := range podSelector.MatchExpressions {
+		if expr.Key == k8sConst.PolicyLabelCluster {
+			return true
+		}
+	}
+
+	return false
+}
+
+func parseNetworkPolicyPeer(clusterName, namespace string, peer *slim_networkingv1.NetworkPolicyPeer) *api.EndpointSelector {
 	if peer == nil {
 		return nil
 	}
 
 	var retSel *api.EndpointSelector
+	// The PodSelector should only reflect to the configured cluster unless the selector
+	// explicitly targets another cluster already.
+	if clusterName != cmtypes.PolicyAnyCluster && !isPodSelectorSelectingCluster(peer.PodSelector) {
+		if peer.PodSelector == nil {
+			peer.PodSelector = &slim_metav1.LabelSelector{}
+		}
+		if peer.PodSelector.MatchLabels == nil {
+			peer.PodSelector.MatchLabels = map[string]slim_metav1.MatchLabelsValue{}
+		}
+		peer.PodSelector.MatchLabels[k8sConst.PolicyLabelCluster] = clusterName
+	}
 
 	if peer.NamespaceSelector != nil {
 		namespaceSelector := &slim_metav1.LabelSelector{
@@ -112,8 +140,7 @@ func hasV1PolicyType(pTypes []slim_networkingv1.PolicyType, typ slim_networkingv
 // ParseNetworkPolicy parses a k8s NetworkPolicy. Returns a list of
 // Cilium policy rules that can be added, along with an error if there was an
 // error sanitizing the rules.
-func ParseNetworkPolicy(logger *slog.Logger, np *slim_networkingv1.NetworkPolicy) (api.Rules, error) {
-
+func ParseNetworkPolicy(logger *slog.Logger, clusterName string, np *slim_networkingv1.NetworkPolicy) (api.Rules, error) {
 	if np == nil {
 		return nil, fmt.Errorf("cannot parse NetworkPolicy because it is nil")
 	}
@@ -130,7 +157,7 @@ func ParseNetworkPolicy(logger *slog.Logger, np *slim_networkingv1.NetworkPolicy
 		if len(iRule.From) > 0 {
 			for _, rule := range iRule.From {
 				ingress := api.IngressRule{}
-				endpointSelector := parseNetworkPolicyPeer(namespace, &rule)
+				endpointSelector := parseNetworkPolicyPeer(clusterName, namespace, &rule)
 
 				if endpointSelector != nil {
 					ingress.FromEndpoints = append(ingress.FromEndpoints, *endpointSelector)
@@ -175,7 +202,7 @@ func ParseNetworkPolicy(logger *slog.Logger, np *slim_networkingv1.NetworkPolicy
 			for _, rule := range eRule.To {
 				egress := api.EgressRule{}
 				if rule.NamespaceSelector != nil || rule.PodSelector != nil {
-					endpointSelector := parseNetworkPolicyPeer(namespace, &rule)
+					endpointSelector := parseNetworkPolicyPeer(clusterName, namespace, &rule)
 
 					if endpointSelector != nil {
 						egress.ToEndpoints = append(egress.ToEndpoints, *endpointSelector)
diff --git a/pkg/k8s/network_policy_test.go b/pkg/k8s/network_policy_test.go
index 3ef40913bb..2bc36e2796 100644
--- a/pkg/k8s/network_policy_test.go
+++ b/pkg/k8s/network_policy_test.go
@@ -13,6 +13,7 @@ import (
 	"k8s.io/apimachinery/pkg/types"
 
 	"github.com/cilium/cilium/pkg/annotation"
+	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	"github.com/cilium/cilium/pkg/identity"
 	k8sConst "github.com/cilium/cilium/pkg/k8s/apis/cilium.io"
 	slim_corev1 "github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1"
@@ -361,7 +362,7 @@ func TestParseNetworkPolicy(t *testing.T) {
 			err := tc.out.Sanitize()
 			require.NoError(t, err)
 
-			rules, err := ParseNetworkPolicy(hivetest.Logger(t), np)
+			rules, err := ParseNetworkPolicy(hivetest.Logger(t), cmtypes.PolicyAnyCluster, np)
 			require.NoError(t, err)
 			require.Len(t, rules, 1)
 			require.Equal(t, &tc.out, rules[0])
@@ -534,7 +535,7 @@ func TestParseNetworkPolicyNoSelectors(t *testing.T) {
 		expectedRule,
 	}
 
-	rules, err := ParseNetworkPolicy(hivetest.Logger(t), &np)
+	rules, err := ParseNetworkPolicy(hivetest.Logger(t), cmtypes.PolicyAnyCluster, &np)
 	require.NoError(t, err)
 	require.NotNil(t, rules)
 	require.Equal(t, expectedRules, rules)
@@ -597,7 +598,7 @@ func parseAndAddRules(t *testing.T, ps ...*slim_networkingv1.NetworkPolicy) *pol
 		if p.Namespace == "" {
 			p.Namespace = "default"
 		}
-		rules, err := ParseNetworkPolicy(hivetest.Logger(t), p)
+		rules, err := ParseNetworkPolicy(hivetest.Logger(t), cmtypes.PolicyAnyCluster, p)
 		require.NoError(t, err)
 		rev := repo.GetRevision()
 		_, id := repo.MustAddList(rules)
@@ -756,7 +757,7 @@ func TestParseNetworkPolicyNamedPort(t *testing.T) {
 		},
 	}
 
-	rules, err := ParseNetworkPolicy(hivetest.Logger(t), netPolicy)
+	rules, err := ParseNetworkPolicy(hivetest.Logger(t), cmtypes.PolicyAnyCluster, netPolicy)
 	require.NoError(t, err)
 	require.Len(t, rules, 1)
 }
@@ -774,7 +775,7 @@ func TestParseNetworkPolicyEmptyPort(t *testing.T) {
 		},
 	}
 
-	rules, err := ParseNetworkPolicy(hivetest.Logger(t), netPolicy)
+	rules, err := ParseNetworkPolicy(hivetest.Logger(t), cmtypes.PolicyAnyCluster, netPolicy)
 	require.NoError(t, err)
 	require.Len(t, rules, 1)
 	require.Len(t, rules[0].Ingress, 1)
@@ -815,7 +816,7 @@ func TestParseNetworkPolicyUnknownProto(t *testing.T) {
 		},
 	}
 
-	rules, err := ParseNetworkPolicy(hivetest.Logger(t), netPolicy)
+	rules, err := ParseNetworkPolicy(hivetest.Logger(t), cmtypes.PolicyAnyCluster, netPolicy)
 	require.Error(t, err)
 	require.Empty(t, rules)
 }
@@ -881,7 +882,7 @@ func TestParseNetworkPolicyNoIngress(t *testing.T) {
 		},
 	}
 
-	rules, err := ParseNetworkPolicy(hivetest.Logger(t), netPolicy)
+	rules, err := ParseNetworkPolicy(hivetest.Logger(t), cmtypes.PolicyAnyCluster, netPolicy)
 	require.NoError(t, err)
 	require.Len(t, rules, 1)
 }
@@ -941,7 +942,7 @@ func TestNetworkPolicyExamples(t *testing.T) {
 			err := json.Unmarshal(p, &np)
 			require.NoError(t, err, "Failed to unmarshal policy %d", i)
 
-			rules, err := ParseNetworkPolicy(hivetest.Logger(t), &np)
+			rules, err := ParseNetworkPolicy(hivetest.Logger(t), cmtypes.PolicyAnyCluster, &np)
 			require.NoError(t, err, "Failed to parse policy %d", i)
 			require.Len(t, rules, 1)
 
@@ -1409,7 +1410,7 @@ func TestCIDRPolicyExamples(t *testing.T) {
 	err := json.Unmarshal(ex1, &np)
 	require.NoError(t, err)
 
-	rules, err := ParseNetworkPolicy(hivetest.Logger(t), &np)
+	rules, err := ParseNetworkPolicy(hivetest.Logger(t), cmtypes.PolicyAnyCluster, &np)
 	require.NoError(t, err)
 	require.NotNil(t, rules)
 	require.Len(t, rules, 1)
@@ -1457,7 +1458,7 @@ func TestCIDRPolicyExamples(t *testing.T) {
 	err = json.Unmarshal(ex2, &np)
 	require.NoError(t, err)
 
-	rules, err = ParseNetworkPolicy(hivetest.Logger(t), &np)
+	rules, err = ParseNetworkPolicy(hivetest.Logger(t), cmtypes.PolicyAnyCluster, &np)
 	require.NoError(t, err)
 	require.NotNil(t, rules)
 	require.Len(t, rules, 1)
@@ -1481,10 +1482,88 @@ func getSelectorPointer(sel api.EndpointSelector) *api.EndpointSelector {
 	return &sel
 }
 
+func TestParseNetworkPolicyClusterLabel(t *testing.T) {
+	np := &slim_networkingv1.NetworkPolicy{
+		Spec: slim_networkingv1.NetworkPolicySpec{
+			PodSelector: slim_metav1.LabelSelector{
+				MatchLabels: map[string]string{
+					"foo": "bar",
+				},
+			},
+			Ingress: []slim_networkingv1.NetworkPolicyIngressRule{{
+				From: []slim_networkingv1.NetworkPolicyPeer{{
+					PodSelector: &slim_metav1.LabelSelector{
+						MatchLabels: map[string]string{},
+					},
+				}},
+			}},
+			Egress: []slim_networkingv1.NetworkPolicyEgressRule{{
+				To: []slim_networkingv1.NetworkPolicyPeer{{
+					PodSelector: &slim_metav1.LabelSelector{
+						MatchLabels: map[string]string{"io.cilium.k8s.policy.cluster": "cluster2"},
+					},
+				}},
+			}},
+		},
+	}
+	fromEndpoints := labels.LabelArray{
+		labels.NewLabel(k8sConst.PodNamespaceLabel, "default", labels.LabelSourceK8s),
+		labels.NewLabel("foo", "bar", labels.LabelSourceK8s),
+	}
+	epSelector := api.NewESFromLabels(fromEndpoints...)
+
+	expectedRule := api.NewRule().
+		WithEndpointSelector(epSelector).
+		WithIngressRules([]api.IngressRule{{
+			IngressCommonRule: api.IngressCommonRule{
+				FromEndpoints: []api.EndpointSelector{api.NewESFromK8sLabelSelector(
+					labels.LabelSourceK8sKeyPrefix,
+					&slim_metav1.LabelSelector{
+						MatchLabels: map[string]string{
+							"io.cilium.k8s.policy.cluster": "cluster1",
+							"io.kubernetes.pod.namespace":  "default",
+						},
+					},
+				)},
+			},
+		}}).
+		WithEgressRules([]api.EgressRule{{
+			EgressCommonRule: api.EgressCommonRule{
+				ToEndpoints: []api.EndpointSelector{api.NewESFromK8sLabelSelector(
+					labels.LabelSourceK8sKeyPrefix,
+					&slim_metav1.LabelSelector{
+						MatchLabels: map[string]string{
+							"io.cilium.k8s.policy.cluster": "cluster2",
+							"io.kubernetes.pod.namespace":  "default",
+						},
+					},
+				)},
+			},
+		}}).
+		WithLabels(labels.ParseLabelArray(
+			"k8s:"+k8sConst.PolicyLabelName+"=",
+			"k8s:"+k8sConst.PolicyLabelUID+"=",
+			"k8s:"+k8sConst.PolicyLabelNamespace+"=default",
+			"k8s:"+k8sConst.PolicyLabelDerivedFrom+"="+resourceTypeNetworkPolicy,
+		))
+
+	expectedRule.Sanitize()
+
+	expectedRules := api.Rules{
+		expectedRule,
+	}
+
+	rules, err := ParseNetworkPolicy(hivetest.Logger(t), "cluster1", np)
+	require.NoError(t, err)
+	require.NotNil(t, rules)
+	require.Equal(t, expectedRules, rules)
+}
+
 func Test_parseNetworkPolicyPeer(t *testing.T) {
 	type args struct {
-		namespace string
-		peer      *slim_networkingv1.NetworkPolicyPeer
+		namespace   string
+		clusterName string
+		peer        *slim_networkingv1.NetworkPolicyPeer
 	}
 	tests := []struct {
 		name string
@@ -1494,7 +1573,8 @@ func Test_parseNetworkPolicyPeer(t *testing.T) {
 		{
 			name: "peer-with-pod-selector",
 			args: args{
-				namespace: "foo-namespace",
+				namespace:   "foo-namespace",
+				clusterName: "cluster1",
 				peer: &slim_networkingv1.NetworkPolicyPeer{
 					PodSelector: &slim_metav1.LabelSelector{
 						MatchLabels: map[string]string{
@@ -1513,8 +1593,9 @@ func Test_parseNetworkPolicyPeer(t *testing.T) {
 			want: getSelectorPointer(
 				api.NewESFromMatchRequirements(
 					map[string]string{
-						"k8s.foo":                         "bar",
-						"k8s.io.kubernetes.pod.namespace": "foo-namespace",
+						"k8s.foo":                          "bar",
+						"k8s.io.kubernetes.pod.namespace":  "foo-namespace",
+						"k8s.io.cilium.k8s.policy.cluster": "cluster1",
 					},
 					[]slim_metav1.LabelSelectorRequirement{
 						{
@@ -1635,10 +1716,104 @@ func Test_parseNetworkPolicyPeer(t *testing.T) {
 				),
 			),
 		},
+		{
+			name: "peer-with-defaut-cluster",
+			args: args{
+				namespace:   "foo-namespace",
+				clusterName: "cluster1",
+				peer: &slim_networkingv1.NetworkPolicyPeer{
+					PodSelector: &slim_metav1.LabelSelector{
+						MatchLabels: map[string]string{
+							"foo": "bar",
+						},
+						MatchExpressions: []slim_metav1.LabelSelectorRequirement{
+							{
+								Key:      "foo",
+								Operator: slim_metav1.LabelSelectorOpIn,
+								Values:   []string{"bar", "baz"},
+							},
+						},
+					},
+				},
+			},
+			want: getSelectorPointer(
+				api.NewESFromMatchRequirements(
+					map[string]string{
+						"k8s.foo":                          "bar",
+						"k8s.io.cilium.k8s.policy.cluster": "cluster1",
+						"k8s.io.kubernetes.pod.namespace":  "foo-namespace",
+					},
+					[]slim_metav1.LabelSelectorRequirement{
+						{
+							Key:      "k8s.foo",
+							Operator: slim_metav1.LabelSelectorOpIn,
+							Values:   []string{"bar", "baz"},
+						},
+					},
+				),
+			),
+		},
+		{
+			name: "peer-with-cluster-selector",
+			args: args{
+				namespace:   "foo-namespace",
+				clusterName: "cluster1",
+				peer: &slim_networkingv1.NetworkPolicyPeer{
+					PodSelector: &slim_metav1.LabelSelector{
+						MatchLabels: map[string]string{
+							"foo":                          "bar",
+							"io.cilium.k8s.policy.cluster": "cluster2",
+						},
+					},
+				},
+			},
+			want: getSelectorPointer(
+				api.NewESFromMatchRequirements(
+					map[string]string{
+						"k8s.foo":                          "bar",
+						"k8s.io.kubernetes.pod.namespace":  "foo-namespace",
+						"k8s.io.cilium.k8s.policy.cluster": "cluster2",
+					},
+					nil,
+				),
+			),
+		},
+		{
+			name: "peer-with-cluster-selector-expr",
+			args: args{
+				namespace:   "foo-namespace",
+				clusterName: "cluster1",
+				peer: &slim_networkingv1.NetworkPolicyPeer{
+					PodSelector: &slim_metav1.LabelSelector{
+						MatchExpressions: []slim_metav1.LabelSelectorRequirement{
+							{
+								Key:      "io.cilium.k8s.policy.cluster",
+								Operator: slim_metav1.LabelSelectorOpIn,
+								Values:   []string{"bar", "baz"},
+							},
+						},
+					},
+				},
+			},
+			want: getSelectorPointer(
+				api.NewESFromMatchRequirements(
+					map[string]string{
+						"k8s.io.kubernetes.pod.namespace": "foo-namespace",
+					},
+					[]slim_metav1.LabelSelectorRequirement{
+						{
+							Key:      "k8s.io.cilium.k8s.policy.cluster",
+							Operator: slim_metav1.LabelSelectorOpIn,
+							Values:   []string{"bar", "baz"},
+						},
+					},
+				),
+			),
+		},
 	}
 	for _, tt := range tests {
 		t.Run(tt.name, func(t *testing.T) {
-			got := parseNetworkPolicyPeer(tt.args.namespace, tt.args.peer)
+			got := parseNetworkPolicyPeer(tt.args.clusterName, tt.args.namespace, tt.args.peer)
 			require.Equal(t, tt.want, got)
 		})
 	}
diff --git a/pkg/k8s/resource/resource_test.go b/pkg/k8s/resource/resource_test.go
index 13694cac48..05a2a86938 100644
--- a/pkg/k8s/resource/resource_test.go
+++ b/pkg/k8s/resource/resource_test.go
@@ -95,8 +95,8 @@ func TestResource_WithFakeClient(t *testing.T) {
 		nodeName = "some-node"
 		node     = &corev1.Node{
 			ObjectMeta: metav1.ObjectMeta{
-				Name:            nodeName,
-				ResourceVersion: "0",
+				Name:       nodeName,
+				Generation: 0,
 			},
 			Status: corev1.NodeStatus{
 				Phase: "init",
@@ -111,13 +111,13 @@ func TestResource_WithFakeClient(t *testing.T) {
 
 	// Create the initial version of the node. Do this before anything
 	// starts watching the resources to avoid a race.
-	fakeClient.KubernetesFakeClientset.Tracker().Create(
-		corev1.SchemeGroupVersion.WithResource("nodes"),
-		node.DeepCopy(), "")
-
 	ctx, cancel := context.WithTimeout(context.Background(), testTimeout)
 	defer cancel()
 
+	fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Create(
+		ctx,
+		node.DeepCopy(), metav1.CreateOptions{})
+
 	hive := hive.New(
 		cell.Provide(func() k8sClient.Clientset { return cs }),
 		nodesResource,
@@ -161,9 +161,10 @@ func TestResource_WithFakeClient(t *testing.T) {
 	// Update the node and check the update event
 	node.Status.Phase = "update1"
 	node.ObjectMeta.ResourceVersion = "1"
-	fakeClient.KubernetesFakeClientset.Tracker().Update(
-		corev1.SchemeGroupVersion.WithResource("nodes"),
-		node.DeepCopy(), "")
+
+	fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Update(
+		ctx,
+		node.DeepCopy(), metav1.UpdateOptions{})
 
 	ev, ok = <-events
 	require.True(t, ok, "events channel closed unexpectedly")
@@ -192,16 +193,16 @@ func TestResource_WithFakeClient(t *testing.T) {
 		ev2.Done(nil)
 
 		for i := 2; i <= 10; i++ {
-			version := fmt.Sprintf("%d", i)
 			node.Status.Phase = corev1.NodePhase(fmt.Sprintf("update%d", i))
-			node.ObjectMeta.ResourceVersion = version
-			fakeClient.KubernetesFakeClientset.Tracker().Update(
-				corev1.SchemeGroupVersion.WithResource("nodes"),
-				node.DeepCopy(), "")
+			node.ObjectMeta.Generation = int64(i)
+
+			fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Update(
+				ctx,
+				node.DeepCopy(), metav1.UpdateOptions{})
 			ev2, ok := <-events2
 			require.True(t, ok, "events channel closed unexpectedly")
 			require.Equal(t, resource.Upsert, ev2.Kind)
-			require.Equal(t, version, ev2.Object.ResourceVersion)
+			require.EqualValues(t, i, ev2.Object.Generation)
 			ev2.Done(nil)
 		}
 		cancel2()
@@ -216,25 +217,26 @@ func TestResource_WithFakeClient(t *testing.T) {
 	require.Equal(t, resource.Upsert, ev.Kind)
 	require.Equal(t, nodeName, ev.Key.Name)
 	ev.Done(nil)
-	if ev.Object.ResourceVersion != node.ObjectMeta.ResourceVersion {
+	if ev.Object.Generation != node.ObjectMeta.Generation {
 		ev, ok = <-events
 		require.True(t, ok, "events channel closed unexpectedly")
 		require.Equal(t, resource.Upsert, ev.Kind)
 		require.Equal(t, nodeName, ev.Key.Name)
-		require.Equal(t, node.ObjectMeta.ResourceVersion, ev.Object.ResourceVersion)
+		require.Equal(t, node.ObjectMeta.Generation, ev.Object.Generation)
 		ev.Done(nil)
 	}
 
 	// Finally delete the node
-	fakeClient.KubernetesFakeClientset.Tracker().Delete(
-		corev1.SchemeGroupVersion.WithResource("nodes"),
-		"", "some-node")
+	fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Delete(
+		ctx,
+		node.Name,
+		metav1.DeleteOptions{})
 
 	ev, ok = <-events
 	require.True(t, ok, "events channel closed unexpectedly")
 	require.Equal(t, resource.Delete, ev.Kind)
 	require.Equal(t, nodeName, ev.Key.Name)
-	require.Equal(t, node.ObjectMeta.ResourceVersion, ev.Object.ResourceVersion)
+	require.Equal(t, node.ObjectMeta.Generation, ev.Object.Generation)
 	ev.Done(nil)
 
 	// Cancel the subscriber context and verify that the stream gets completed.
@@ -489,9 +491,9 @@ func TestResource_WithTransform(t *testing.T) {
 		t.Fatalf("hive.Start failed: %s", err)
 	}
 
-	fakeClient.KubernetesFakeClientset.Tracker().Create(
-		corev1.SchemeGroupVersion.WithResource("nodes"),
-		node.DeepCopy(), "")
+	fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Create(
+		ctx,
+		node.DeepCopy(), metav1.CreateOptions{})
 
 	events := strippedNodes.Events(ctx)
 
@@ -528,9 +530,12 @@ func TestResource_WithoutIndexers(t *testing.T) {
 		fakeClient, cs = k8sClient.NewFakeClientset(hivetest.Logger(t))
 	)
 
-	fakeClient.KubernetesFakeClientset.Tracker().Create(
-		corev1.SchemeGroupVersion.WithResource("nodes"),
-		node.DeepCopy(), "")
+	ctx, cancel := context.WithTimeout(context.Background(), testTimeout)
+	defer cancel()
+
+	fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Create(
+		ctx,
+		node.DeepCopy(), metav1.CreateOptions{})
 
 	hive := hive.New(
 		cell.Provide(func() k8sClient.Clientset { return cs }),
@@ -546,9 +551,6 @@ func TestResource_WithoutIndexers(t *testing.T) {
 		}),
 	)
 
-	ctx, cancel := context.WithTimeout(context.Background(), testTimeout)
-	defer cancel()
-
 	tlog := hivetest.Logger(t)
 	if err := hive.Start(tlog, ctx); err != nil {
 		t.Fatalf("hive.Start failed: %s", err)
@@ -644,10 +646,13 @@ func TestResource_WithIndexers(t *testing.T) {
 		}
 	)
 
+	ctx, cancel := context.WithTimeout(context.Background(), testTimeout)
+	defer cancel()
+
 	for _, node := range nodes {
-		fakeClient.KubernetesFakeClientset.Tracker().Create(
-			corev1.SchemeGroupVersion.WithResource("nodes"),
-			node.DeepCopy(), "")
+		fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Create(
+			ctx,
+			node.DeepCopy(), metav1.CreateOptions{})
 	}
 
 	hive := hive.New(
@@ -667,9 +672,6 @@ func TestResource_WithIndexers(t *testing.T) {
 		}),
 	)
 
-	ctx, cancel := context.WithTimeout(context.Background(), testTimeout)
-	defer cancel()
-
 	tlog := hivetest.Logger(t)
 	if err := hive.Start(tlog, ctx); err != nil {
 		t.Fatalf("hive.Start failed: %s", err)
@@ -811,9 +813,9 @@ func TestResource_Retries(t *testing.T) {
 	}
 
 	// Create the initial version of the node.
-	fakeClient.KubernetesFakeClientset.Tracker().Create(
-		corev1.SchemeGroupVersion.WithResource("nodes"),
-		node, "")
+	fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Create(
+		ctx,
+		node.DeepCopy(), metav1.CreateOptions{})
 
 	// Test that update events are retried
 	{
@@ -849,9 +851,9 @@ func TestResource_Retries(t *testing.T) {
 			case resource.Sync:
 				ev.Done(nil)
 			case resource.Upsert:
-				fakeClient.KubernetesFakeClientset.Tracker().Delete(
-					corev1.SchemeGroupVersion.WithResource("nodes"),
-					"", node.Name)
+				fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Delete(
+					ctx,
+					node.Name, metav1.DeleteOptions{})
 				ev.Done(nil)
 			case resource.Delete:
 				numRetries.Add(1)
@@ -881,12 +883,14 @@ func TestResource_Observe(t *testing.T) {
 		fakeClient, cs = k8sClient.NewFakeClientset(hivetest.Logger(t))
 		nodes          resource.Resource[*corev1.Node]
 	)
+	ctx, cancel := context.WithTimeout(context.Background(), testTimeout)
+	defer cancel()
 
 	// Create the initial version of the node. Do this before anything
 	// starts watching the resources to avoid a race.
-	fakeClient.KubernetesFakeClientset.Tracker().Create(
-		corev1.SchemeGroupVersion.WithResource("nodes"),
-		node.DeepCopy(), "")
+	fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Create(
+		ctx,
+		node.DeepCopy(), metav1.CreateOptions{})
 
 	hive := hive.New(
 		cell.Provide(func() k8sClient.Clientset { return cs }),
@@ -895,9 +899,6 @@ func TestResource_Observe(t *testing.T) {
 			nodes = r
 		}))
 
-	ctx, cancel := context.WithTimeout(context.Background(), testTimeout)
-	defer cancel()
-
 	tlog := hivetest.Logger(t)
 	if err := hive.Start(tlog, ctx); err != nil {
 		t.Fatalf("hive.Start failed: %s", err)
@@ -1026,15 +1027,15 @@ func TestResource_SkippedDonePanics(t *testing.T) {
 		events         <-chan resource.Event[*corev1.Node]
 	)
 
-	// Create the initial version of the node. Do this before anything
-	// starts watching the resources to avoid a race.
-	fakeClient.KubernetesFakeClientset.Tracker().Create(
-		corev1.SchemeGroupVersion.WithResource("nodes"),
-		node.DeepCopy(), "")
-
 	ctx, cancel := context.WithTimeout(context.Background(), testTimeout)
 	defer cancel()
 
+	// Create the initial version of the node. Do this before anything
+	// starts watching the resources to avoid a race.
+	fakeClient.KubernetesFakeClientset.CoreV1().Nodes().Create(
+		ctx,
+		node.DeepCopy(), metav1.CreateOptions{})
+
 	hive := hive.New(
 		cell.Provide(func() k8sClient.Clientset { return cs }),
 		nodesResource,
diff --git a/pkg/k8s/resource_ctors.go b/pkg/k8s/resource_ctors.go
index f7723fa189..6613594464 100644
--- a/pkg/k8s/resource_ctors.go
+++ b/pkg/k8s/resource_ctors.go
@@ -61,6 +61,7 @@ const (
 // Flags implements the cell.Flagger interface.
 func (def Config) Flags(flags *pflag.FlagSet) {
 	flags.Bool("enable-k8s-endpoint-slice", def.EnableK8sEndpointSlice, "Enables k8s EndpointSlice feature in Cilium if the k8s cluster supports it")
+	flags.MarkDeprecated("enable-k8s-endpoint-slice", "The flag will be removed in v1.19. The feature will be unconditionally enabled by default.")
 	flags.String("k8s-service-proxy-name", def.K8sServiceProxyName, "Value of K8s service-proxy-name label for which Cilium handles the services (empty = all services without service.kubernetes.io/service-proxy-name label)")
 }
 
diff --git a/pkg/k8s/testutils/decoder.go b/pkg/k8s/testutils/decoder.go
index 48b282565a..d8a5afeb7e 100644
--- a/pkg/k8s/testutils/decoder.go
+++ b/pkg/k8s/testutils/decoder.go
@@ -29,6 +29,9 @@ var (
 	// lazily constructed.
 	Scheme = runtime.NewScheme()
 
+	// KubernetesScheme is the core Kubernetes scheme.
+	KubernetesScheme = runtime.NewScheme()
+
 	decoderOnce sync.Once
 	decoder     runtime.Decoder
 
@@ -46,6 +49,13 @@ func Decoder() runtime.Decoder {
 	return decoder
 }
 
+func KubernetesDecoder() runtime.Decoder {
+	kubernetesDecoderOnce.Do(func() {
+		kubernetesDecoder = serializer.NewCodecFactory(KubernetesScheme).UniversalDeserializer()
+	})
+	return kubernetesDecoder
+}
+
 func init() {
 	// Add corev1, discovery* and networking.
 	slim_fake.AddToScheme(Scheme)
@@ -63,6 +73,8 @@ func init() {
 
 	// Add multiclusterv1alpha1
 	mcsapi_fake.AddToScheme(Scheme)
+
+	fake.AddToScheme(KubernetesScheme)
 }
 
 func DecodeObject(bytes []byte) (runtime.Object, error) {
@@ -79,12 +91,7 @@ func DecodeObjectGVK(bytes []byte) (runtime.Object, *schema.GroupVersionKind, er
 }
 
 func DecodeKubernetesObject(bytes []byte) (runtime.Object, *schema.GroupVersionKind, error) {
-	kubernetesDecoderOnce.Do(func() {
-		scheme := runtime.NewScheme()
-		fake.AddToScheme(scheme)
-		kubernetesDecoder = serializer.NewCodecFactory(scheme).UniversalDeserializer()
-	})
-	return kubernetesDecoder.Decode(bytes, nil, nil)
+	return KubernetesDecoder().Decode(bytes, nil, nil)
 }
 
 func DecodeFile(path string) (runtime.Object, error) {
diff --git a/pkg/k8s/watchers/pod.go b/pkg/k8s/watchers/pod.go
index 3665b4ca32..ba20f241fd 100644
--- a/pkg/k8s/watchers/pod.go
+++ b/pkg/k8s/watchers/pod.go
@@ -15,11 +15,8 @@ import (
 	"slices"
 	"strconv"
 	"strings"
-	"sync"
 	"sync/atomic"
 
-	"github.com/cilium/ebpf"
-	"github.com/cilium/ebpf/asm"
 	"github.com/cilium/hive/cell"
 	"github.com/cilium/statedb"
 	k8sErrors "k8s.io/apimachinery/pkg/api/errors"
@@ -33,7 +30,6 @@ import (
 	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	"github.com/cilium/cilium/pkg/controller"
 	"github.com/cilium/cilium/pkg/datapath/linux/bandwidth"
-	"github.com/cilium/cilium/pkg/datapath/linux/probes"
 	datapathTables "github.com/cilium/cilium/pkg/datapath/tables"
 	"github.com/cilium/cilium/pkg/endpoint"
 	"github.com/cilium/cilium/pkg/endpoint/regeneration"
@@ -543,19 +539,6 @@ func (k *K8sPodWatcher) deleteK8sPodV1(pod *slim_corev1.Pod) error {
 	return err
 }
 
-var (
-	_netnsCookieSupported     bool
-	_netnsCookieSupportedOnce sync.Once
-)
-
-func netnsCookieSupported(logger *slog.Logger) bool {
-	_netnsCookieSupportedOnce.Do(func() {
-		_netnsCookieSupported = probes.HaveProgramHelper(logger, ebpf.CGroupSock, asm.FnGetNetnsCookie) == nil &&
-			probes.HaveProgramHelper(logger, ebpf.CGroupSockAddr, asm.FnGetNetnsCookie) == nil
-	})
-	return _netnsCookieSupported
-}
-
 func (k *K8sPodWatcher) genServiceMappings(pod *slim_corev1.Pod, podIPs []string, logger *slog.Logger) []loadbalancer.LegacySVC {
 	var (
 		svcs       []loadbalancer.LegacySVC
@@ -577,14 +560,6 @@ func (k *K8sPodWatcher) genServiceMappings(pod *slim_corev1.Pod, podIPs []string
 				continue
 			}
 
-			feIP, err := netip.ParseAddr(p.HostIP)
-			if err == nil && feIP.IsLoopback() && !netnsCookieSupported(logger) {
-				logger.Warn(
-					fmt.Sprintf("The requested loopback address for hostIP (%s) is not supported for kernels which don't provide netns cookies. Ignoring.",
-						feIP))
-				continue
-			}
-
 			proto, err := loadbalancer.NewL4Type(string(p.Protocol))
 			if err != nil {
 				continue
@@ -617,6 +592,7 @@ func (k *K8sPodWatcher) genServiceMappings(pod *slim_corev1.Pod, podIPs []string
 			// on this address but not via other addresses. When it's not set,
 			// then expose via all local addresses. Same when the user provides
 			// an unspecified address (0.0.0.0 / [::]).
+			feIP, _ := netip.ParseAddr(p.HostIP)
 			if feIP.IsValid() && !feIP.IsUnspecified() {
 				// Migrate the loopback address into a 0.0.0.0 / [::]
 				// surrogate, thus internal datapath handling can be
diff --git a/pkg/labels/labels.go b/pkg/labels/labels.go
index 09280a2d27..6c7b22da32 100644
--- a/pkg/labels/labels.go
+++ b/pkg/labels/labels.go
@@ -12,9 +12,8 @@ import (
 	"slices"
 	"strings"
 
-	"github.com/sirupsen/logrus"
-
 	"github.com/cilium/cilium/pkg/container/cache"
+	"github.com/cilium/cilium/pkg/logging"
 	"github.com/cilium/cilium/pkg/logging/logfields"
 )
 
@@ -328,7 +327,10 @@ func NewLabel(key string, value string, source string) Label {
 	if l.Source == LabelSourceCIDR {
 		c, err := LabelToPrefix(l.Key)
 		if err != nil {
-			logrus.WithField("key", l.Key).WithError(err).Error("Failed to parse CIDR label: invalid prefix.")
+			logging.DefaultSlogLogger.Error("Failed to parse CIDR label: invalid prefix.",
+				logfields.Error, err,
+				logfields.Key, l.Key,
+			)
 		} else {
 			l.cidr = &c
 		}
@@ -481,7 +483,10 @@ func (l *Label) UnmarshalJSON(data []byte) error {
 		if err == nil {
 			l.cidr = &c
 		} else {
-			logrus.WithField("key", l.Key).WithError(err).Error("Failed to parse CIDR label: invalid prefix.")
+			logging.DefaultSlogLogger.Error("Failed to parse CIDR label: invalid prefix.",
+				logfields.Error, err,
+				logfields.Key, l.Key,
+			)
 		}
 	}
 
@@ -810,11 +815,15 @@ func parseLabel(str string, delim byte) (lbl Label) {
 
 	if lbl.Source == LabelSourceCIDR {
 		if lbl.Value != "" {
-			logrus.WithField(logfields.Label, lbl.String()).Error("Invalid CIDR label: labels with source cidr cannot have values.")
+			logging.DefaultSlogLogger.Error("Invalid CIDR label: labels with source cidr cannot have values.",
+				logfields.Label, lbl,
+			)
 		}
 		c, err := LabelToPrefix(lbl.Key)
 		if err != nil {
-			logrus.WithField(logfields.Label, str).WithError(err).Error("Failed to parse CIDR label: invalid prefix.")
+			logging.DefaultSlogLogger.Error("Failed to parse CIDR label: invalid prefix.",
+				logfields.Label, lbl,
+			)
 		} else {
 			lbl.cidr = &c
 		}
diff --git a/pkg/loadbalancer/healthserver/testdata/healthserver.txtar b/pkg/loadbalancer/healthserver/testdata/healthserver.txtar
index 964bb8a51a..02ad029bef 100644
--- a/pkg/loadbalancer/healthserver/testdata/healthserver.txtar
+++ b/pkg/loadbalancer/healthserver/testdata/healthserver.txtar
@@ -3,9 +3,7 @@
 # Add a node address.
 db/insert node-addresses addrv4.yaml
 
-# Start and wait for initialization.
 hive start
-db/initialized
 
 env HEALTHPORT1=40000
 env HEALTHPORT2=40001
diff --git a/pkg/loadbalancer/reconciler/bpf_reconciler.go b/pkg/loadbalancer/reconciler/bpf_reconciler.go
index 0ea0333b88..5bf39513c0 100644
--- a/pkg/loadbalancer/reconciler/bpf_reconciler.go
+++ b/pkg/loadbalancer/reconciler/bpf_reconciler.go
@@ -870,6 +870,14 @@ func (ops *BPFOps) updateFrontend(fe *loadbalancer.Frontend) error {
 		}
 	}
 
+	if activeCount == 0 {
+		// If there are no active backends we can use the terminating backends.
+		// https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/1669-proxy-terminating-endpoints
+		activeCount = terminatingCount
+	} else {
+		inactiveCount += terminatingCount
+	}
+
 	// Update Maglev
 	if ops.useMaglev(fe) {
 		ops.log.Debug("Update Maglev", logfields.FrontendID, feID)
@@ -933,7 +941,7 @@ func (ops *BPFOps) updateFrontend(fe *loadbalancer.Frontend) error {
 		logfields.ProxyRedirect, fe.Service.ProxyRedirect,
 		logfields.Address, fe.Address,
 		logfields.Count, backendCount)
-	if err := ops.upsertMaster(svcKey, svcVal, fe, activeCount, terminatingCount, inactiveCount); err != nil {
+	if err := ops.upsertMaster(svcKey, svcVal, fe, activeCount, inactiveCount); err != nil {
 		return fmt.Errorf("upsert service master: %w", err)
 	}
 
@@ -944,7 +952,7 @@ func (ops *BPFOps) updateFrontend(fe *loadbalancer.Frontend) error {
 			logfields.ID, feID,
 			logfields.Count, backendCount,
 			logfields.Previous, numPreviousBackends)
-		if err := ops.cleanupSlots(svcKey, numPreviousBackends, activeCount+terminatingCount+inactiveCount); err != nil {
+		if err := ops.cleanupSlots(svcKey, numPreviousBackends, activeCount+inactiveCount); err != nil {
 			return fmt.Errorf("cleanup service slots: %w", err)
 		}
 	}
@@ -1016,17 +1024,10 @@ func (ops *BPFOps) upsertService(svcKey lbmap.ServiceKey, svcVal lbmap.ServiceVa
 	return err
 }
 
-func (ops *BPFOps) upsertMaster(svcKey lbmap.ServiceKey, svcVal lbmap.ServiceValue, fe *loadbalancer.Frontend, activeBackends, terminatingBackends, inactiveBackends int) error {
+func (ops *BPFOps) upsertMaster(svcKey lbmap.ServiceKey, svcVal lbmap.ServiceValue, fe *loadbalancer.Frontend, activeBackends, inactiveBackends int) error {
+	svcVal.SetCount(activeBackends)
+	svcVal.SetQCount(inactiveBackends)
 	svcKey.SetBackendSlot(0)
-	if activeBackends == 0 {
-		// If there are no active backends we can use the terminating backends.
-		// https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/1669-proxy-terminating-endpoints
-		svcVal.SetCount(terminatingBackends)
-		svcVal.SetQCount(inactiveBackends)
-	} else {
-		svcVal.SetCount(activeBackends)
-		svcVal.SetQCount(terminatingBackends + inactiveBackends)
-	}
 	svcVal.SetBackendID(0)
 	svcVal.SetLbAlg(ops.lbAlgorithm(fe))
 
diff --git a/pkg/loadbalancer/redirectpolicy/script_test.go b/pkg/loadbalancer/redirectpolicy/script_test.go
index 7c514b2b50..e02817fe39 100644
--- a/pkg/loadbalancer/redirectpolicy/script_test.go
+++ b/pkg/loadbalancer/redirectpolicy/script_test.go
@@ -37,6 +37,7 @@ import (
 	"github.com/cilium/cilium/pkg/maps/lbmap"
 	"github.com/cilium/cilium/pkg/metrics"
 	"github.com/cilium/cilium/pkg/node"
+	nodeTypes "github.com/cilium/cilium/pkg/node/types"
 	"github.com/cilium/cilium/pkg/option"
 	"github.com/cilium/cilium/pkg/source"
 	"github.com/cilium/cilium/pkg/testutils"
@@ -49,6 +50,7 @@ func TestScript(t *testing.T) {
 	defer goleak.VerifyNone(t)
 
 	version.Force(k8sTestutils.DefaultVersion)
+	nodeTypes.SetName("testnode")
 
 	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
 	t.Cleanup(cancel)
diff --git a/pkg/loadbalancer/redirectpolicy/testdata/address.txtar b/pkg/loadbalancer/redirectpolicy/testdata/address.txtar
index fe9e61337b..7123310328 100644
--- a/pkg/loadbalancer/redirectpolicy/testdata/address.txtar
+++ b/pkg/loadbalancer/redirectpolicy/testdata/address.txtar
@@ -1,5 +1,4 @@
 hive start
-db/initialized
 
 # Add a pod and an address-based redirection. Add IPv4 first then
 # IPv6 for consistent ordering.
@@ -107,6 +106,7 @@ spec:
         - containerPort: 80
           name: tcp
           protocol: TCP
+  nodeName: testnode
 status:
   hostIP: 172.19.0.3
   hostIPs:
diff --git a/pkg/loadbalancer/redirectpolicy/testdata/node-local-dns.txtar b/pkg/loadbalancer/redirectpolicy/testdata/node-local-dns.txtar
index 88b1c43bcd..57ab61981e 100644
--- a/pkg/loadbalancer/redirectpolicy/testdata/node-local-dns.txtar
+++ b/pkg/loadbalancer/redirectpolicy/testdata/node-local-dns.txtar
@@ -4,7 +4,6 @@
 
 # Start and wait for reflectors to catch up.
 hive start
-db/initialized
 
 # Add the kubedns service and endpoints
 k8s/add svc-kubedns.yaml eps-kubedns.yaml
@@ -180,14 +179,14 @@ endpoints:
     ready: true
     serving: true
     terminating: false
-  nodeName: kind-worker
+  nodeName: testnode
 - addresses:
   - 10.244.1.51
   conditions:
     ready: true
     serving: true
     terminating: false
-  nodeName: kind-worker
+  nodeName: testnode
 kind: EndpointSlice
 metadata:
   labels:
@@ -255,7 +254,7 @@ spec:
     - containerPort: 9253
       name: metrics
       protocol: TCP
-  nodeName: kind-control-plane
+  nodeName: testnode
 status:
   conditions:
   - lastProbeTime: null
@@ -301,7 +300,7 @@ spec:
     - containerPort: 9253
       name: metrics
       protocol: TCP
-  nodeName: kind-control-plane
+  nodeName: testnode
 status:
   conditions:
   - lastProbeTime: null
diff --git a/pkg/loadbalancer/redirectpolicy/testdata/service.txtar b/pkg/loadbalancer/redirectpolicy/testdata/service.txtar
index 835613b7e5..b297797957 100644
--- a/pkg/loadbalancer/redirectpolicy/testdata/service.txtar
+++ b/pkg/loadbalancer/redirectpolicy/testdata/service.txtar
@@ -1,12 +1,10 @@
 hive start
-db/initialized
 
 # Add pods, services and endpoints.
 k8s/add pod.yaml service.yaml endpointslice.yaml
 db/cmp services services-before.table
 db/cmp frontends frontends-before.table
 
-
 # Compare maps
 lb/maps-dump lbmaps.actual
 * cmp lbmaps.actual maps-before.expected
@@ -200,6 +198,7 @@ spec:
         - containerPort: 70
           name: udp
           protocol: UDP
+  nodeName: testnode
 status:
   hostIP: 172.19.0.3
   hostIPs:
diff --git a/pkg/loadbalancer/redirectpolicy/testdata/skiplb-addr.txtar b/pkg/loadbalancer/redirectpolicy/testdata/skiplb-addr.txtar
index 4dbd0e47c8..5036d7a47f 100644
--- a/pkg/loadbalancer/redirectpolicy/testdata/skiplb-addr.txtar
+++ b/pkg/loadbalancer/redirectpolicy/testdata/skiplb-addr.txtar
@@ -3,7 +3,6 @@
 # the load-balancing in datapath for packets originating from the backend.
 
 hive start
-db/initialized
 
 ### Case 1: 1) cookie, 2) pod 3) LRP
 
@@ -221,6 +220,7 @@ spec:
         - containerPort: 80
           name: tcp
           protocol: TCP
+  nodeName: testnode
 status:
   hostIP: 172.19.0.3
   hostIPs:
diff --git a/pkg/loadbalancer/redirectpolicy/testdata/skiplb.txtar b/pkg/loadbalancer/redirectpolicy/testdata/skiplb.txtar
index 82108af8c2..df39be61fe 100644
--- a/pkg/loadbalancer/redirectpolicy/testdata/skiplb.txtar
+++ b/pkg/loadbalancer/redirectpolicy/testdata/skiplb.txtar
@@ -3,7 +3,6 @@
 # the load-balancing in datapath for packets originating from the backend.
 
 hive start
-db/initialized
 
 ### Case 1: 1) cookie, 2) pod,service,eps, 3) LRP
 
@@ -12,17 +11,23 @@ db/initialized
 # coming from the EndpointManager.
 db/insert desired-skiplbmap pod-cookie.yaml
 
-# Add pods, services and endpoints.
-k8s/add endpointslice.yaml
+# Add services and endpoints.
+k8s/add service.yaml endpointslice.yaml
 db/cmp backends backends.table
 
-k8s/add pod.yaml service.yaml lrp-svc.yaml
+# Compare LB maps
+lb/maps-dump lbmaps.actual
+* cmp lbmaps.actual maps-case1-pre.expected
+
+# Add the override
+k8s/add pod.yaml lrp-svc.yaml
 db/cmp localredirectpolicies lrp.table
 db/cmp services services.table
 db/cmp frontends frontends.table
 db/cmp desired-skiplbmap skiplbmap.table
 
-# Compare LB maps
+# Compare LB maps. The original backends become orphans and
+# are removed a new backend is created for the pod.
 lb/maps-dump lbmaps.actual
 * cmp lbmaps.actual maps-case1.expected
 
@@ -161,6 +166,15 @@ Address                    Type        ServiceName   PortName   Backends
 169.254.169.254:8080/TCP   ClusterIP   test/echo     tcp        10.244.1.1:8080/TCP                     Done
 [1001::1]:8080/TCP         ClusterIP   test/echo     tcp        [2001::1]:8080/TCP                      Done
 
+-- maps-case1-pre.expected --
+BE: ID=1 ADDR=10.244.1.1:8080/TCP STATE=active
+BE: ID=2 ADDR=[2001::1]:8080/TCP STATE=active
+REV: ID=1 ADDR=169.254.169.254:8080
+REV: ID=2 ADDR=[1001::1]:8080
+SVC: ID=1 ADDR=169.254.169.254:8080/TCP SLOT=0 LBALG=undef AFFTimeout=0 COUNT=1 QCOUNT=0 FLAGS=ClusterIP+non-routable
+SVC: ID=1 ADDR=169.254.169.254:8080/TCP SLOT=1 BEID=1 COUNT=0 QCOUNT=0 FLAGS=ClusterIP+non-routable
+SVC: ID=2 ADDR=[1001::1]:8080/TCP SLOT=0 LBALG=undef AFFTimeout=0 COUNT=1 QCOUNT=0 FLAGS=ClusterIP+non-routable
+SVC: ID=2 ADDR=[1001::1]:8080/TCP SLOT=1 BEID=2 COUNT=0 QCOUNT=0 FLAGS=ClusterIP+non-routable
 -- maps-case1.expected --
 BE: ID=3 ADDR=10.244.2.1:80/TCP STATE=active
 BE: ID=4 ADDR=[2002::2]:80/TCP STATE=active
@@ -216,6 +230,7 @@ spec:
         - containerPort: 80
           name: tcp
           protocol: TCP
+  nodeName: testnode
 status:
   hostIP: 172.19.0.3
   hostIPs:
diff --git a/pkg/loadbalancer/reflectors/conversions.go b/pkg/loadbalancer/reflectors/conversions.go
index ce1ad337dc..2ccf42c660 100644
--- a/pkg/loadbalancer/reflectors/conversions.go
+++ b/pkg/loadbalancer/reflectors/conversions.go
@@ -81,6 +81,7 @@ func convertService(cfg loadbalancer.Config, extCfg loadbalancer.ExternalConfig,
 		Annotations:         svc.Annotations,
 		HealthCheckNodePort: uint16(svc.Spec.HealthCheckNodePort),
 		ForwardingMode:      loadbalancer.SVCForwardingModeUndef,
+		LoadBalancerClass:   svc.Spec.LoadBalancerClass,
 	}
 
 	if cfg.LBModeAnnotation {
diff --git a/pkg/loadbalancer/service.go b/pkg/loadbalancer/service.go
index a20cc7247b..a78a60aaa7 100644
--- a/pkg/loadbalancer/service.go
+++ b/pkg/loadbalancer/service.go
@@ -14,7 +14,6 @@ import (
 
 	"github.com/cilium/statedb"
 	"github.com/cilium/statedb/index"
-	"github.com/cilium/statedb/part"
 
 	"github.com/cilium/cilium/pkg/annotation"
 	"github.com/cilium/cilium/pkg/labels"
@@ -59,9 +58,17 @@ type Service struct {
 	// to the backend. If undefined the default mode is used (--bpf-lb-mode).
 	ForwardingMode SVCForwardingMode
 
-	SessionAffinity        bool
+	// SessionAffinity if true will enable the client IP based session affinity.
+	SessionAffinity bool
+
+	// SessionAffinityTimeout is the duration of inactivity before the session
+	// affinity is cleared for a specific client IP.
 	SessionAffinityTimeout time.Duration
 
+	// LoadBalancerClass if set specifies the load-balancer class to be used
+	// for a LoadBalancer service. If unset the default implementation is used.
+	LoadBalancerClass *string
+
 	// ProxyRedirect if non-nil redirects the traffic going to the frontends
 	// towards a locally running proxy.
 	ProxyRedirect *ProxyRedirect
@@ -86,10 +93,6 @@ type Service struct {
 	// TrafficDistribution if not default will influence how backends are chosen for
 	// frontends associated with this service.
 	TrafficDistribution TrafficDistribution
-
-	// Properties are additional untyped properties that can carry feature
-	// specific metadata about the service.
-	Properties part.Map[string, any]
 }
 
 type TrafficDistribution string
@@ -232,19 +235,14 @@ func (svc *Service) TableRow() []string {
 		flags = append(flags, "ForwardingMode="+string(svc.ForwardingMode))
 	}
 
-	if svc.Properties.Len() != 0 {
-		// Since the property is an "any", we'll just show the keys.
-		propKeys := make([]string, 0, svc.Properties.Len())
-		for k := range svc.Properties.All() {
-			propKeys = append(propKeys, k)
-		}
-		flags = append(flags, "Properties="+strings.Join(propKeys, ", "))
-	}
-
 	if svc.TrafficDistribution != TrafficDistributionDefault {
 		flags = append(flags, "TrafficDistribution="+string(svc.TrafficDistribution))
 	}
 
+	if svc.LoadBalancerClass != nil {
+		flags = append(flags, "LoadBalancerClass="+*svc.LoadBalancerClass)
+	}
+
 	sort.Strings(flags)
 
 	return []string{
diff --git a/pkg/loadbalancer/tests/main_test.go b/pkg/loadbalancer/tests/main_test.go
index 33a93a8796..27c96a54e9 100644
--- a/pkg/loadbalancer/tests/main_test.go
+++ b/pkg/loadbalancer/tests/main_test.go
@@ -16,5 +16,9 @@ func TestMain(m *testing.M) {
 		// and the agent specifics.
 		goleak.IgnoreTopFunction("net/http.(*persistConn).writeLoop"),
 		goleak.IgnoreTopFunction("internal/poll.runtime_pollWait"),
+
+		// Unfortunately we don't have a way for waiting for the DelayingWorkQueue's background goroutine
+		// to exit (used by pkg/k8s/resource), so we'll just need to ignore it.
+		goleak.IgnoreTopFunction("k8s.io/client-go/util/workqueue.(*delayingType[...]).waitingLoop"),
 	)
 }
diff --git a/pkg/loadbalancer/tests/testdata/clusterip.txtar b/pkg/loadbalancer/tests/testdata/clusterip.txtar
index 8d42230649..797a69ce5b 100644
--- a/pkg/loadbalancer/tests/testdata/clusterip.txtar
+++ b/pkg/loadbalancer/tests/testdata/clusterip.txtar
@@ -3,9 +3,6 @@
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-db/initialized
-
 # Add the service and then endpoints
 k8s/add service.yaml
 db/cmp services services.table
diff --git a/pkg/loadbalancer/tests/testdata/dualstack-maglev.txtar b/pkg/loadbalancer/tests/testdata/dualstack-maglev.txtar
index 2494a8c5fb..f46a29aa5e 100644
--- a/pkg/loadbalancer/tests/testdata/dualstack-maglev.txtar
+++ b/pkg/loadbalancer/tests/testdata/dualstack-maglev.txtar
@@ -10,9 +10,6 @@ db/cmp node-addresses nodeaddrs.table
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-db/initialized
-
 # For determinism, add the endpoints first and then services.
 k8s/add endpointslice-ipv4.yaml endpointslice-ipv6.yaml
 db/cmp backends backends.table
diff --git a/pkg/loadbalancer/tests/testdata/dualstack.txtar b/pkg/loadbalancer/tests/testdata/dualstack.txtar
index 1f96866cc9..10342ba289 100644
--- a/pkg/loadbalancer/tests/testdata/dualstack.txtar
+++ b/pkg/loadbalancer/tests/testdata/dualstack.txtar
@@ -9,9 +9,6 @@ db/cmp node-addresses nodeaddrs.table
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-db/initialized
-
 # For determinism, add the endpoints first.
 k8s/add endpointslice-ipv4.yaml endpointslice-ipv6.yaml
 db/cmp backends backends.table 
diff --git a/pkg/loadbalancer/tests/testdata/external-clusterip.txtar b/pkg/loadbalancer/tests/testdata/external-clusterip.txtar
index a3c9589d58..fff8c986d7 100644
--- a/pkg/loadbalancer/tests/testdata/external-clusterip.txtar
+++ b/pkg/loadbalancer/tests/testdata/external-clusterip.txtar
@@ -4,9 +4,6 @@
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-db/initialized
-
 k8s/add service.yaml endpointslice.yaml
 
 # Check the BPF maps. The service should not be marked as "non-routable"
diff --git a/pkg/loadbalancer/tests/testdata/file.txtar b/pkg/loadbalancer/tests/testdata/file.txtar
index f51b74bacb..57f12ac2d7 100644
--- a/pkg/loadbalancer/tests/testdata/file.txtar
+++ b/pkg/loadbalancer/tests/testdata/file.txtar
@@ -4,7 +4,6 @@
 #
 
 hive/start
-db/initialized
 
 # Starting with a missing state file. Tables should be empty
 db/empty services backends frontends
diff --git a/pkg/loadbalancer/tests/testdata/graceful-termination.txtar b/pkg/loadbalancer/tests/testdata/graceful-termination.txtar
index 1ee54be285..58e795b140 100644
--- a/pkg/loadbalancer/tests/testdata/graceful-termination.txtar
+++ b/pkg/loadbalancer/tests/testdata/graceful-termination.txtar
@@ -1,11 +1,11 @@
-#! --enable-experimental-lb
+#! --enable-experimental-lb --bpf-lb-algorithm=maglev --bpf-lb-external-clusterip=true
+# Test graceful termination with maglev to verify that both the services map and
+# the maglev table correctly use terminating backends when no active backends are
+# available.
 
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-db/initialized
-
 # Start with two backends that are both active.
 cp endpointslice.yaml.tmpl endpointslice.yaml
 replace '$TERM1' 'false' endpointslice.yaml
@@ -48,6 +48,10 @@ k8s/update endpointslice.yaml
 db/cmp frontends frontends-terminating2.table
 db/cmp backends backends-terminating2.table
 
+# Check BPF maps
+lb/maps-dump lbmaps.actual
+* cmp lbmaps-terminating2.expected lbmaps.actual
+
 # Cleanup
 k8s/delete service.yaml endpointslice.yaml
 
@@ -172,19 +176,24 @@ ports:
 -- lbmaps-active.expected --
 BE: ID=1 ADDR=10.244.0.112:8081/TCP STATE=active
 BE: ID=2 ADDR=10.244.0.113:8081/TCP STATE=active
+MAGLEV: ID=1 INNER=[1(511), 2(510)]
 REV: ID=1 ADDR=10.96.116.33:8081
-SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=0 LBALG=undef AFFTimeout=0 COUNT=2 QCOUNT=0 FLAGS=ClusterIP+non-routable
-SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=1 BEID=1 COUNT=0 QCOUNT=0 FLAGS=ClusterIP+non-routable
-SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=2 BEID=2 COUNT=0 QCOUNT=0 FLAGS=ClusterIP+non-routable
+SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=0 LBALG=undef AFFTimeout=0 COUNT=2 QCOUNT=0 FLAGS=ClusterIP
+SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=1 BEID=1 COUNT=0 QCOUNT=0 FLAGS=ClusterIP
+SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=2 BEID=2 COUNT=0 QCOUNT=0 FLAGS=ClusterIP
 -- lbmaps-terminating1.expected --
 BE: ID=1 ADDR=10.244.0.112:8081/TCP STATE=terminating
 BE: ID=2 ADDR=10.244.0.113:8081/TCP STATE=active
+MAGLEV: ID=1 INNER=[2(1021)]
 REV: ID=1 ADDR=10.96.116.33:8081
-SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=0 LBALG=undef AFFTimeout=0 COUNT=1 QCOUNT=1 FLAGS=ClusterIP+non-routable
-SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=1 BEID=2 COUNT=0 QCOUNT=0 FLAGS=ClusterIP+non-routable
-SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=2 BEID=1 COUNT=0 QCOUNT=0 FLAGS=ClusterIP+non-routable
+SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=0 LBALG=undef AFFTimeout=0 COUNT=1 QCOUNT=1 FLAGS=ClusterIP
+SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=1 BEID=2 COUNT=0 QCOUNT=0 FLAGS=ClusterIP
+SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=2 BEID=1 COUNT=0 QCOUNT=0 FLAGS=ClusterIP
 -- lbmaps-terminating2.expected --
 BE: ID=1 ADDR=10.244.0.112:8081/TCP STATE=terminating
+BE: ID=2 ADDR=10.244.0.113:8081/TCP STATE=terminating
+MAGLEV: ID=1 INNER=[1(511), 2(510)]
 REV: ID=1 ADDR=10.96.116.33:8081
-SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=0 LBALG=random AFFTimeout=0 COUNT=0 QCOUNT=1 FLAGS=ClusterIP+non-routable
-SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=1 BEID=1 COUNT=0 QCOUNT=0 FLAGS=ClusterIP+non-routable
+SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=0 LBALG=undef AFFTimeout=0 COUNT=2 QCOUNT=0 FLAGS=ClusterIP
+SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=1 BEID=1 COUNT=0 QCOUNT=0 FLAGS=ClusterIP
+SVC: ID=1 ADDR=10.96.116.33:8081/TCP SLOT=2 BEID=2 COUNT=0 QCOUNT=0 FLAGS=ClusterIP
diff --git a/pkg/loadbalancer/tests/testdata/headless.txtar b/pkg/loadbalancer/tests/testdata/headless.txtar
index b890a17dae..2b1f4482c4 100644
--- a/pkg/loadbalancer/tests/testdata/headless.txtar
+++ b/pkg/loadbalancer/tests/testdata/headless.txtar
@@ -3,9 +3,6 @@
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-db/initialized
-
 # Add a headless service with backends.
 k8s/add service_clusterip_none.yaml endpointslice.yaml
 
diff --git a/pkg/loadbalancer/tests/testdata/hostport.txtar b/pkg/loadbalancer/tests/testdata/hostport.txtar
index 37fbad126a..376347e40e 100644
--- a/pkg/loadbalancer/tests/testdata/hostport.txtar
+++ b/pkg/loadbalancer/tests/testdata/hostport.txtar
@@ -13,7 +13,6 @@ db/cmp node-addresses nodeaddrs.table
 
 # Start the test application
 hive start
-db/initialized
 
 # Add a pod with 'hostPort'. A synthetic service, frontend and backend will
 # be created for it.
@@ -173,7 +172,7 @@ spec:
       hostPort: 4444
       protocol: TCP
     resources: {}
-  nodeName: kind-worker
+  nodeName: testnode
   restartPolicy: Always
   schedulerName: default-scheduler
   securityContext: {}
diff --git a/pkg/loadbalancer/tests/testdata/ingress.txtar b/pkg/loadbalancer/tests/testdata/ingress.txtar
index 1aab38b74e..5a44406269 100644
--- a/pkg/loadbalancer/tests/testdata/ingress.txtar
+++ b/pkg/loadbalancer/tests/testdata/ingress.txtar
@@ -7,7 +7,6 @@
 # Based on https://docs.cilium.io/en/stable/network/servicemesh/http/
 
 hive/start
-db/initialized
 
 # Add the service and endpoints
 k8s/add svc-ingress.yaml eps-ingress.yaml
diff --git a/pkg/loadbalancer/tests/testdata/loadbalancer.txtar b/pkg/loadbalancer/tests/testdata/loadbalancer.txtar
index 38a8fd5426..e29cbe71ed 100644
--- a/pkg/loadbalancer/tests/testdata/loadbalancer.txtar
+++ b/pkg/loadbalancer/tests/testdata/loadbalancer.txtar
@@ -7,11 +7,6 @@ db/cmp node-addresses nodeaddrs.table
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-# (otherwise might miss events due to List() vs Watch() race in fake client).
-# NOTE: The 100ms delay you might see is from WaitForCacheSync polling at 100ms.
-db/initialized
-
 # Add the endpoints and service
 k8s/add endpointslice.yaml
 db/cmp backends backends.table
@@ -50,7 +45,7 @@ Address NodePort Primary DeviceName
 
 -- services.table --
 Name         Source   PortNames  TrafficPolicy   Flags
-test/echo    k8s      http=80    Cluster         
+test/echo    k8s      http=80    Cluster         LoadBalancerClass=test
 
 -- frontends.table --
 Address               Type         ServiceName   PortName   Status  Backends
@@ -115,6 +110,7 @@ spec:
   selector:
     name: echo
   sessionAffinity: None
+  loadBalancerClass: "test"
   type: LoadBalancer
 status:
   loadBalancer:
diff --git a/pkg/loadbalancer/tests/testdata/marshalling.txtar b/pkg/loadbalancer/tests/testdata/marshalling.txtar
index 6d125cad17..91a798c301 100644
--- a/pkg/loadbalancer/tests/testdata/marshalling.txtar
+++ b/pkg/loadbalancer/tests/testdata/marshalling.txtar
@@ -7,7 +7,6 @@
 #
 
 hive/start
-db/initialized
 
 # Add our test service and backends. To avoid undeterministic ID allocation
 # add the IPv4 endpoints first.
@@ -69,6 +68,7 @@ Address              Instances
   "ForwardingMode": "",
   "SessionAffinity": false,
   "SessionAffinityTimeout": 0,
+  "LoadBalancerClass": null,
   "ProxyRedirect": null,
   "HealthCheckNodePort": 0,
   "LoopbackHostPort": false,
@@ -76,8 +76,7 @@ Address              Instances
   "PortNames": {
     "http": 80
   },
-  "TrafficDistribution": "",
-  "Properties": []
+  "TrafficDistribution": ""
 }
 -- frontends-expected.json --
 {
@@ -264,6 +263,7 @@ inttrafficpolicy: Cluster
 forwardingmode: ""
 sessionaffinity: false
 sessionaffinitytimeout: 0s
+loadbalancerclass: null
 proxyredirect: null
 healthchecknodeport: 0
 loopbackhostport: false
@@ -271,7 +271,6 @@ sourceranges: []
 portnames:
     http: 80
 trafficdistribution: ""
-properties: []
 -- frontends-expected.yaml --
 frontendparams:
     address: 10.96.50.104:80/TCP
diff --git a/pkg/loadbalancer/tests/testdata/multiport.txtar b/pkg/loadbalancer/tests/testdata/multiport.txtar
index ca0a791330..74251e54ca 100644
--- a/pkg/loadbalancer/tests/testdata/multiport.txtar
+++ b/pkg/loadbalancer/tests/testdata/multiport.txtar
@@ -3,9 +3,6 @@
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-db/initialized
-
 k8s/add service.yaml endpointslice.yaml
 db/cmp services services.table
 db/cmp frontends frontends.table
diff --git a/pkg/loadbalancer/tests/testdata/nodeport-addr.txtar b/pkg/loadbalancer/tests/testdata/nodeport-addr.txtar
index 5ebcbf3003..d2f5800f6e 100644
--- a/pkg/loadbalancer/tests/testdata/nodeport-addr.txtar
+++ b/pkg/loadbalancer/tests/testdata/nodeport-addr.txtar
@@ -8,7 +8,6 @@ db/cmp node-addresses nodeaddrs.table
 
 # Start the test application
 hive start
-db/initialized
 
 # Add the first service and endpoints
 k8s/add endpointslice.yaml
diff --git a/pkg/loadbalancer/tests/testdata/nodeport-explicit-maglev.txtar b/pkg/loadbalancer/tests/testdata/nodeport-explicit-maglev.txtar
index 0833047494..073abfb986 100644
--- a/pkg/loadbalancer/tests/testdata/nodeport-explicit-maglev.txtar
+++ b/pkg/loadbalancer/tests/testdata/nodeport-explicit-maglev.txtar
@@ -7,11 +7,6 @@ db/cmp node-addresses nodeaddrs.table
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-# (otherwise might miss events due to List() vs Watch() race in fake client).
-# NOTE: The 100ms delay you might see is from WaitForCacheSync polling at 100ms.
-db/initialized
-
 # Add the first service and endpoints
 k8s/add endpointslice.yaml
 db/cmp backends backends1.table
diff --git a/pkg/loadbalancer/tests/testdata/nodeport-explicit-random.txtar b/pkg/loadbalancer/tests/testdata/nodeport-explicit-random.txtar
index 9af8f3bcc2..a265e440bf 100644
--- a/pkg/loadbalancer/tests/testdata/nodeport-explicit-random.txtar
+++ b/pkg/loadbalancer/tests/testdata/nodeport-explicit-random.txtar
@@ -7,11 +7,6 @@ db/cmp node-addresses nodeaddrs.table
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-# (otherwise might miss events due to List() vs Watch() race in fake client).
-# NOTE: The 100ms delay you might see is from WaitForCacheSync polling at 100ms.
-db/initialized
-
 # Add the first service and endpoints
 k8s/add endpointslice.yaml
 db/cmp backends backends1.table
diff --git a/pkg/loadbalancer/tests/testdata/nodeport-maglev.txtar b/pkg/loadbalancer/tests/testdata/nodeport-maglev.txtar
index f5e35452fd..e0980df141 100644
--- a/pkg/loadbalancer/tests/testdata/nodeport-maglev.txtar
+++ b/pkg/loadbalancer/tests/testdata/nodeport-maglev.txtar
@@ -8,11 +8,6 @@ db/cmp node-addresses nodeaddrs.table
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-# (otherwise might miss events due to List() vs Watch() race in fake client).
-# NOTE: The 100ms delay you might see is from WaitForCacheSync polling at 100ms.
-db/initialized
-
 # Add the first service and endpoints
 k8s/add endpointslice.yaml
 db/cmp backends backends1.table
diff --git a/pkg/loadbalancer/tests/testdata/nodeport.txtar b/pkg/loadbalancer/tests/testdata/nodeport.txtar
index 10c938e511..33a175305c 100644
--- a/pkg/loadbalancer/tests/testdata/nodeport.txtar
+++ b/pkg/loadbalancer/tests/testdata/nodeport.txtar
@@ -8,11 +8,6 @@ db/cmp node-addresses nodeaddrs.table
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-# (otherwise might miss events due to List() vs Watch() race in fake client).
-# NOTE: The 100ms delay you might see is from WaitForCacheSync polling at 100ms.
-db/initialized
-
 # Add the first service and endpoints
 k8s/add endpointslice.yaml
 db/cmp backends backends_only1.table
diff --git a/pkg/loadbalancer/tests/testdata/proxy-delegation.txtar b/pkg/loadbalancer/tests/testdata/proxy-delegation.txtar
index 0891a09bd7..10da21f814 100644
--- a/pkg/loadbalancer/tests/testdata/proxy-delegation.txtar
+++ b/pkg/loadbalancer/tests/testdata/proxy-delegation.txtar
@@ -2,7 +2,6 @@
 
 # Start the test application
 hive start
-db/initialized
 
 # Set the local node IP address. This is used when filtering for local backends
 # when proxy delegation is set.
diff --git a/pkg/loadbalancer/tests/testdata/pruning.txtar b/pkg/loadbalancer/tests/testdata/pruning.txtar
index 73c43ec9ab..d4290578e2 100644
--- a/pkg/loadbalancer/tests/testdata/pruning.txtar
+++ b/pkg/loadbalancer/tests/testdata/pruning.txtar
@@ -3,9 +3,6 @@
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-db/initialized
-
 k8s/add service.yaml endpointslice.yaml
 db/cmp services services.table
 db/cmp frontends frontends.table
diff --git a/pkg/loadbalancer/tests/testdata/quarantined.txtar b/pkg/loadbalancer/tests/testdata/quarantined.txtar
index 44f396044e..a8109cc22b 100644
--- a/pkg/loadbalancer/tests/testdata/quarantined.txtar
+++ b/pkg/loadbalancer/tests/testdata/quarantined.txtar
@@ -4,7 +4,6 @@
 # of quarantined backends.
 
 hive/start
-db/initialized
 
 # Add our test service and backends.
 k8s/add service.yaml endpointslice.yaml
diff --git a/pkg/loadbalancer/tests/testdata/queries.txtar b/pkg/loadbalancer/tests/testdata/queries.txtar
index e17f57d2c4..c4e1beb8a5 100644
--- a/pkg/loadbalancer/tests/testdata/queries.txtar
+++ b/pkg/loadbalancer/tests/testdata/queries.txtar
@@ -5,7 +5,6 @@
 
 # Start and wait for sync against the k8s fake client.
 hive start
-db/initialized
 
 # Show the registered tables and indexes
 db
diff --git a/pkg/loadbalancer/tests/testdata/reuse.txtar b/pkg/loadbalancer/tests/testdata/reuse.txtar
index ba3b046559..7fd1f1a0a9 100644
--- a/pkg/loadbalancer/tests/testdata/reuse.txtar
+++ b/pkg/loadbalancer/tests/testdata/reuse.txtar
@@ -18,7 +18,6 @@
 #
 
 hive start
-db/initialized
 
 # Make a copy of service.yaml with a different name
 cp service.yaml service2.yaml
diff --git a/pkg/loadbalancer/tests/testdata/source-ranges.txtar b/pkg/loadbalancer/tests/testdata/source-ranges.txtar
index 516920a91b..5e1c9bd165 100644
--- a/pkg/loadbalancer/tests/testdata/source-ranges.txtar
+++ b/pkg/loadbalancer/tests/testdata/source-ranges.txtar
@@ -3,9 +3,6 @@
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-db/initialized
-
 k8s/add service.yaml endpointslice.yaml
 db/cmp services services.table
 db/cmp frontends frontends.table
diff --git a/pkg/loadbalancer/tests/testdata/svc-forwarding-mode-annotation.txtar b/pkg/loadbalancer/tests/testdata/svc-forwarding-mode-annotation.txtar
index 00a8008706..66b972404a 100644
--- a/pkg/loadbalancer/tests/testdata/svc-forwarding-mode-annotation.txtar
+++ b/pkg/loadbalancer/tests/testdata/svc-forwarding-mode-annotation.txtar
@@ -2,7 +2,6 @@
 
 # Start the test application
 hive start
-db/initialized
 
 # Add service w/o "service.cilium.io/forwarding-mode"
 k8s/add service.yaml endpointslice.yaml
diff --git a/pkg/loadbalancer/tests/testdata/svc-node-exposure.txtar b/pkg/loadbalancer/tests/testdata/svc-node-exposure.txtar
index 023ce5976b..8623a215b6 100644
--- a/pkg/loadbalancer/tests/testdata/svc-node-exposure.txtar
+++ b/pkg/loadbalancer/tests/testdata/svc-node-exposure.txtar
@@ -3,9 +3,7 @@
 # Test service node exposure
 #
 
-# Start and wait for watchers
 hive start
-db/initialized
 
 # Add the endpoints and service
 k8s/add endpointslice.yaml
diff --git a/pkg/loadbalancer/tests/testdata/svc-type-annotation.txtar b/pkg/loadbalancer/tests/testdata/svc-type-annotation.txtar
index 13b4eea8d2..9d84851ef7 100644
--- a/pkg/loadbalancer/tests/testdata/svc-type-annotation.txtar
+++ b/pkg/loadbalancer/tests/testdata/svc-type-annotation.txtar
@@ -3,11 +3,6 @@
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-# (otherwise might miss events due to List() vs Watch() race in fake client).
-# NOTE: The 100ms delay you might see is from WaitForCacheSync polling at 100ms.
-db/initialized
-
 # Add the endpoints and service
 k8s/add endpointslice.yaml
 db/cmp backends backends.table
diff --git a/pkg/loadbalancer/tests/testdata/topology-aware.txtar b/pkg/loadbalancer/tests/testdata/topology-aware.txtar
index 8cc0e4a614..bec2fbd7d2 100644
--- a/pkg/loadbalancer/tests/testdata/topology-aware.txtar
+++ b/pkg/loadbalancer/tests/testdata/topology-aware.txtar
@@ -8,7 +8,6 @@ test/set-node-labels topology.kubernetes.io/zone=foo
 
 # Start and wait for watchers
 hive start
-db/initialized
 
 # Add the service (with topology-mode=auto) and endpoints
 # 10.244.1.1 (for zones foo & bar), 10.244.2.1 (for zone quux).
diff --git a/pkg/loadbalancer/tests/testdata/trafficpolicy.txtar b/pkg/loadbalancer/tests/testdata/trafficpolicy.txtar
index 636c1e45e8..f50daa905e 100644
--- a/pkg/loadbalancer/tests/testdata/trafficpolicy.txtar
+++ b/pkg/loadbalancer/tests/testdata/trafficpolicy.txtar
@@ -3,9 +3,6 @@
 # Start the test application
 hive start
 
-# Wait for tables to initialize (e.g. reflector to start) before adding more objects.
-db/initialized
-
 # Create a LoadBalancer service with Local traffic policies and associate two backends 
 # to it, one of them on this node ("testnode") and one on another node.
 k8s/add service_tp_local.yaml endpointslice.yaml
diff --git a/pkg/loadinfo/loadinfo.go b/pkg/loadinfo/loadinfo.go
index d812bf6022..a7f47c242a 100644
--- a/pkg/loadinfo/loadinfo.go
+++ b/pkg/loadinfo/loadinfo.go
@@ -6,6 +6,7 @@ package loadinfo
 import (
 	"context"
 	"fmt"
+	"log/slog"
 	"path/filepath"
 	"strconv"
 	"time"
@@ -13,9 +14,7 @@ import (
 	"github.com/mackerelio/go-osstat/loadavg"
 	"github.com/mackerelio/go-osstat/memory"
 	"github.com/prometheus/procfs"
-	"github.com/sirupsen/logrus"
 
-	"github.com/cilium/cilium/pkg/logging"
 	"github.com/cilium/cilium/pkg/logging/logfields"
 )
 
@@ -28,8 +27,6 @@ const (
 	cpuWatermark = 1.0
 )
 
-var log = logging.DefaultLogger.WithField(logfields.LogSubsys, "loadinfo")
-
 // LogFunc is the function to used to log the system load
 type LogFunc func(format string, args ...any)
 
@@ -42,7 +39,7 @@ func toPercent(part uint64, total uint64) float64 {
 }
 
 func pids() (pids []int, err error) {
-	//scan /proc/*/exe to find all active processes
+	// scan /proc/*/exe to find all active processes
 	matches, err := filepath.Glob("/proc/[0-9]*/exe")
 	if err != nil {
 		return nil, err
@@ -50,7 +47,7 @@ func pids() (pids []int, err error) {
 
 	pids = []int{}
 	for _, file := range matches {
-		//extract the pid from the path
+		// extract the pid from the path
 		pid := filepath.Base(filepath.Dir(file))
 		ipid, _ := strconv.Atoi(pid)
 		pids = append(pids, ipid)
@@ -143,6 +140,13 @@ func LogPeriodicSystemLoad(ctx context.Context, logFunc LogFunc, interval time.D
 }
 
 // StartBackgroundLogger starts background logging
-func StartBackgroundLogger() {
-	LogPeriodicSystemLoad(context.Background(), log.WithFields(logrus.Fields{"type": "background"}).Debugf, backgroundInterval)
+func StartBackgroundLogger(logger *slog.Logger) {
+	l := logger.With(logfields.Type, "background")
+	ctx := context.Background()
+	logFunc := LogFunc(func(format string, args ...any) {
+		if l.Enabled(ctx, slog.LevelDebug) {
+			l.Debug(fmt.Sprintf(format, args...))
+		}
+	})
+	LogPeriodicSystemLoad(ctx, logFunc, backgroundInterval)
 }
diff --git a/pkg/logging/logfields/logfields.go b/pkg/logging/logfields/logfields.go
index 762c7ae2d8..0c182c3fbe 100644
--- a/pkg/logging/logfields/logfields.go
+++ b/pkg/logging/logfields/logfields.go
@@ -1544,6 +1544,8 @@ const (
 
 	Candidates = "candidates"
 
+	Location = "location"
+
 	Skipped = "skipped"
 
 	AliveEntries = "aliveEntries"
diff --git a/pkg/logging/logging.go b/pkg/logging/logging.go
index 8550923971..726f3fffce 100644
--- a/pkg/logging/logging.go
+++ b/pkg/logging/logging.go
@@ -62,9 +62,7 @@ var DefaultLogger = initializeDefaultLogger()
 
 var klogErrorOverrides = []logLevelOverride{
 	{
-		// TODO: We can drop the misspelled case here once client-go version is bumped to include:
-		//	https://github.com/kubernetes/client-go/commit/ae43527480ee9d8750fbcde3d403363873fd3d89
-		matcher:     regexp.MustCompile("Failed to update lock (optimitically|optimistically).*falling back to slow path"),
+		matcher:     regexp.MustCompile("Failed to update lock optimistically.*falling back to slow path"),
 		targetLevel: logrus.InfoLevel,
 	},
 }
diff --git a/pkg/maps/egressmap/policy.go b/pkg/maps/egressmap/policy.go
index a1939e1371..07aa40b131 100644
--- a/pkg/maps/egressmap/policy.go
+++ b/pkg/maps/egressmap/policy.go
@@ -56,9 +56,11 @@ type EgressPolicyKey6 struct {
 
 // EgressPolicyVal6 is the value of an egress policy map.
 type EgressPolicyVal6 struct {
-	EgressIP  types.IPv6 `align:"egress_ip"`
-	GatewayIP types.IPv4 `align:"gateway_ip"`
-	Reserved  [3]uint32  `align:"reserved"`
+	EgressIP      types.IPv6 `align:"egress_ip"`
+	GatewayIP     types.IPv4 `align:"gateway_ip"`
+	Reserved      [3]uint32  `align:"reserved"`
+	EgressIfindex uint32     `align:"egress_ifindex"`
+	Reserved2     uint32     `align:"reserved2"`
 }
 
 type PolicyConfig struct {
diff --git a/pkg/maps/nodemap/cell.go b/pkg/maps/nodemap/cell.go
index 753bbf51d4..721a1a6467 100644
--- a/pkg/maps/nodemap/cell.go
+++ b/pkg/maps/nodemap/cell.go
@@ -11,7 +11,6 @@ import (
 	"github.com/spf13/pflag"
 
 	"github.com/cilium/cilium/pkg/bpf"
-	"github.com/cilium/cilium/pkg/maps/encrypt"
 )
 
 // Cell provides the nodemap.MapV2 which contains information about node IDs, SPIs, and their IP addresses.
@@ -41,16 +40,13 @@ func newNodeMap(lifecycle cell.Lifecycle, conf Config, logger *slog.Logger) (bpf
 		return bpf.MapOut[MapV2]{}, fmt.Errorf("creating node map: bpf-node-map-max cannot be less than %d (%d)",
 			DefaultMaxEntries, conf.NodeMapMax)
 	}
-	nodeMap := newMapV2(logger, MapNameV2, MapName, conf)
+	nodeMap := newMapV2(logger, MapNameV2, conf)
 
 	lifecycle.Append(cell.Hook{
 		OnStart: func(context cell.HookContext) error {
-			if err := nodeMap.init(); err != nil {
-				return err
-			}
+			nodeMap.migrateV1("cilium_node_map")
 
-			// do v1 to v2 map migration if necessary
-			return nodeMap.migrateV1(MapName, encrypt.MapName)
+			return nodeMap.init()
 		},
 		OnStop: func(context cell.HookContext) error {
 			return nodeMap.close()
diff --git a/pkg/maps/nodemap/fake/node_map.go b/pkg/maps/nodemap/fake/node_map.go
deleted file mode 100644
index dcd238f6d4..0000000000
--- a/pkg/maps/nodemap/fake/node_map.go
+++ /dev/null
@@ -1,49 +0,0 @@
-// SPDX-License-Identifier: Apache-2.0
-// Copyright Authors of Cilium
-
-package fake
-
-import (
-	"fmt"
-	"net"
-
-	"github.com/cilium/cilium/pkg/maps/nodemap"
-)
-
-type fakeNodeMap struct {
-	ids map[string]uint16
-}
-
-var _ nodemap.Map = &fakeNodeMap{}
-
-func NewFakeNodeMap() *fakeNodeMap {
-	return &fakeNodeMap{
-		ids: map[string]uint16{},
-	}
-}
-
-func (f fakeNodeMap) Update(ip net.IP, nodeID uint16) error {
-	f.ids[ip.String()] = nodeID
-	return nil
-}
-
-func (f fakeNodeMap) Size() uint32 {
-	return nodemap.DefaultMaxEntries
-}
-
-func (f fakeNodeMap) Delete(ip net.IP) error {
-	delete(f.ids, ip.String())
-	return nil
-}
-
-// This function is used only for tests.
-func (f fakeNodeMap) Lookup(ip net.IP) (uint16, error) {
-	if nodeID, exists := f.ids[ip.String()]; exists {
-		return nodeID, nil
-	}
-	return 0, fmt.Errorf("IP not found in node ID map")
-}
-
-func (f fakeNodeMap) IterateWithCallback(cb nodemap.NodeIterateCallback) error {
-	return nil
-}
diff --git a/pkg/maps/nodemap/fake/node_map_v2.go b/pkg/maps/nodemap/fake/node_map_v2.go
index 962159a5a2..0be58f53b5 100644
--- a/pkg/maps/nodemap/fake/node_map_v2.go
+++ b/pkg/maps/nodemap/fake/node_map_v2.go
@@ -14,8 +14,6 @@ type fakeNodeMapV2 struct {
 	ids map[string]nodemap.NodeValueV2
 }
 
-var _ nodemap.Map = &fakeNodeMap{}
-
 func NewFakeNodeMapV2() *fakeNodeMapV2 {
 	return &fakeNodeMapV2{
 		ids: map[string]nodemap.NodeValueV2{},
diff --git a/pkg/maps/nodemap/node_map.go b/pkg/maps/nodemap/node_map.go
deleted file mode 100644
index 1a665a4d9c..0000000000
--- a/pkg/maps/nodemap/node_map.go
+++ /dev/null
@@ -1,144 +0,0 @@
-// SPDX-License-Identifier: Apache-2.0
-// Copyright Authors of Cilium
-
-package nodemap
-
-import (
-	"fmt"
-	"log/slog"
-	"net"
-	"unsafe"
-
-	"golang.org/x/sys/unix"
-
-	"github.com/cilium/cilium/pkg/bpf"
-	"github.com/cilium/cilium/pkg/ebpf"
-	"github.com/cilium/cilium/pkg/types"
-)
-
-const (
-	MapName           = "cilium_node_map"
-	DefaultMaxEntries = 16384
-)
-
-// Map provides access to the eBPF map node.
-type Map interface {
-	// Update inserts or updates the node map object associated with the provided
-	// IP and node id.
-	Update(ip net.IP, nodeID uint16) error
-
-	// Delete deletes the node map object associated with the provided
-	// IP.
-	Delete(ip net.IP) error
-
-	// IterateWithCallback iterates through all the keys/values of a node map,
-	// passing each key/value pair to the cb callback.
-	IterateWithCallback(cb NodeIterateCallback) error
-
-	// Size returns what how many entries the node map is configured to hold.
-	Size() uint32
-}
-
-type nodeMap struct {
-	conf   Config
-	bpfMap *ebpf.Map
-}
-
-func newMap(logger *slog.Logger, mapName string, conf Config) *nodeMap {
-	return &nodeMap{
-		conf: conf,
-		bpfMap: ebpf.NewMap(logger, &ebpf.MapSpec{
-			Name:       mapName,
-			Type:       ebpf.Hash,
-			KeySize:    uint32(unsafe.Sizeof(NodeKey{})),
-			ValueSize:  uint32(unsafe.Sizeof(NodeValue{})),
-			MaxEntries: conf.NodeMapMax,
-			Flags:      unix.BPF_F_NO_PREALLOC,
-			Pinning:    ebpf.PinByName,
-		}),
-	}
-}
-
-type NodeKey struct {
-	Pad1   uint16 `align:"pad1"`
-	Pad2   uint8  `align:"pad2"`
-	Family uint8  `align:"family"`
-	// represents both IPv6 and IPv4 (in the lowest four bytes)
-	IP types.IPv6 `align:"$union0"`
-}
-
-func (k *NodeKey) String() string {
-	switch k.Family {
-	case bpf.EndpointKeyIPv4:
-		return net.IP(k.IP[:net.IPv4len]).String()
-	case bpf.EndpointKeyIPv6:
-		return k.IP.String()
-	}
-	return "<unknown>"
-}
-
-func newNodeKey(ip net.IP) NodeKey {
-	result := NodeKey{}
-	if ip4 := ip.To4(); ip4 != nil {
-		result.Family = bpf.EndpointKeyIPv4
-		copy(result.IP[:], ip4)
-	} else {
-		result.Family = bpf.EndpointKeyIPv6
-		copy(result.IP[:], ip)
-	}
-	return result
-}
-
-type NodeValue struct {
-	NodeID uint16
-}
-
-func (m *nodeMap) Update(ip net.IP, nodeID uint16) error {
-	key := newNodeKey(ip)
-	val := NodeValue{NodeID: nodeID}
-	return m.bpfMap.Update(key, val, 0)
-}
-
-func (m *nodeMap) Size() uint32 {
-	return m.conf.NodeMapMax
-}
-
-func (m *nodeMap) Delete(ip net.IP) error {
-	key := newNodeKey(ip)
-	return m.bpfMap.Map.Delete(key)
-}
-
-// NodeIterateCallback represents the signature of the callback function
-// expected by the IterateWithCallback method, which in turn is used to iterate
-// all the keys/values of a node map.
-type NodeIterateCallback func(*NodeKey, *NodeValue)
-
-func (m *nodeMap) IterateWithCallback(cb NodeIterateCallback) error {
-	return m.bpfMap.IterateWithCallback(&NodeKey{}, &NodeValue{},
-		func(k, v any) {
-			key := k.(*NodeKey)
-			value := v.(*NodeValue)
-
-			cb(key, value)
-		})
-}
-
-// LoadNodeMap loads the pre-initialized node map for access.
-// This should only be used from components which aren't capable of using hive - mainly the Cilium CLI.
-// It needs to initialized beforehand via the Cilium Agent.
-func LoadNodeMap(logger *slog.Logger) (Map, error) {
-	bpfMap, err := ebpf.LoadRegisterMap(logger, MapName)
-	if err != nil {
-		return nil, fmt.Errorf("failed to load bpf map: %w", err)
-	}
-
-	return &nodeMap{bpfMap: bpfMap}, nil
-}
-
-func (m *nodeMap) init() error {
-	if err := m.bpfMap.OpenOrCreate(); err != nil {
-		return fmt.Errorf("failed to init bpf map: %w", err)
-	}
-
-	return nil
-}
diff --git a/pkg/maps/nodemap/node_map_privileged_test.go b/pkg/maps/nodemap/node_map_privileged_test.go
deleted file mode 100644
index f7907ed91a..0000000000
--- a/pkg/maps/nodemap/node_map_privileged_test.go
+++ /dev/null
@@ -1,65 +0,0 @@
-// SPDX-License-Identifier: Apache-2.0
-// Copyright Authors of Cilium
-
-package nodemap
-
-import (
-	"net"
-	"testing"
-
-	"github.com/cilium/ebpf/rlimit"
-	"github.com/cilium/hive/hivetest"
-	"github.com/stretchr/testify/require"
-
-	"github.com/cilium/cilium/pkg/bpf"
-	"github.com/cilium/cilium/pkg/testutils"
-)
-
-func setupNodeMapSuite(tb testing.TB) {
-	testutils.PrivilegedTest(tb)
-
-	logger := hivetest.Logger(tb)
-	bpf.CheckOrMountFS(logger, "")
-	err := rlimit.RemoveMemlock()
-	require.NoError(tb, err)
-}
-
-func TestNodeMap(t *testing.T) {
-	setupNodeMapSuite(t)
-	logger := hivetest.Logger(t)
-	nodeMap := newMap(logger, "test_cilium_node_map", defaultConfig)
-	err := nodeMap.init()
-	require.NoError(t, err)
-	defer nodeMap.bpfMap.Unpin()
-
-	bpfNodeIDMap := map[uint16]string{}
-	toMap := func(key *NodeKey, val *NodeValue) {
-		address := key.IP.String()
-		if key.Family == bpf.EndpointKeyIPv4 {
-			address = net.IP(key.IP[:net.IPv4len]).String()
-		}
-		bpfNodeIDMap[val.NodeID] = address
-	}
-
-	err = nodeMap.IterateWithCallback(toMap)
-	require.NoError(t, err)
-	require.Empty(t, bpfNodeIDMap)
-
-	err = nodeMap.Update(net.ParseIP("10.1.0.0"), 10)
-	require.NoError(t, err)
-	err = nodeMap.Update(net.ParseIP("10.1.0.1"), 20)
-	require.NoError(t, err)
-
-	bpfNodeIDMap = map[uint16]string{}
-	err = nodeMap.IterateWithCallback(toMap)
-	require.NoError(t, err)
-	require.Len(t, bpfNodeIDMap, 2)
-
-	err = nodeMap.Delete(net.ParseIP("10.1.0.0"))
-	require.NoError(t, err)
-
-	bpfNodeIDMap = map[uint16]string{}
-	err = nodeMap.IterateWithCallback(toMap)
-	require.NoError(t, err)
-	require.Len(t, bpfNodeIDMap, 1)
-}
diff --git a/pkg/maps/nodemap/node_map_v2.go b/pkg/maps/nodemap/node_map_v2.go
index 165b62409e..bdaab43651 100644
--- a/pkg/maps/nodemap/node_map_v2.go
+++ b/pkg/maps/nodemap/node_map_v2.go
@@ -4,10 +4,11 @@
 package nodemap
 
 import (
-	"errors"
 	"fmt"
 	"log/slog"
 	"net"
+	"os"
+	"path/filepath"
 	"unsafe"
 
 	"golang.org/x/sys/unix"
@@ -15,18 +16,18 @@ import (
 	"github.com/cilium/cilium/pkg/bpf"
 	"github.com/cilium/cilium/pkg/ebpf"
 	"github.com/cilium/cilium/pkg/logging/logfields"
+	"github.com/cilium/cilium/pkg/types"
 )
 
 // compile time check of MapV2 interface
 var _ MapV2 = (*nodeMapV2)(nil)
 
 const (
-	MapNameV2 = "cilium_node_map_v2"
+	MapNameV2         = "cilium_node_map_v2"
+	DefaultMaxEntries = 16384
 )
 
 // MapV2 provides access to the eBPF map node.
-//
-// MapV2 will mirror all writes into MapV1.
 type MapV2 interface {
 	// Update inserts or updates the node map object associated with the provided
 	// IP, node id, and SPI.
@@ -50,21 +51,12 @@ type nodeMapV2 struct {
 	logger *slog.Logger
 	conf   Config
 	bpfMap *ebpf.Map
-	v1Map  *nodeMap
 }
 
-func newMapV2(logger *slog.Logger, mapName string, v1MapName string, conf Config) *nodeMapV2 {
-	v1Map := newMap(logger, v1MapName, conf)
-
-	if err := v1Map.init(); err != nil {
-		logger.Error("failed to init v1 node map", logfields.Error, err)
-		return nil
-	}
-
+func newMapV2(logger *slog.Logger, mapName string, conf Config) *nodeMapV2 {
 	return &nodeMapV2{
 		logger: logger,
 		conf:   conf,
-		v1Map:  v1Map,
 		bpfMap: ebpf.NewMap(logger, &ebpf.MapSpec{
 			Name:       mapName,
 			Type:       ebpf.Hash,
@@ -77,6 +69,36 @@ func newMapV2(logger *slog.Logger, mapName string, v1MapName string, conf Config
 	}
 }
 
+type NodeKey struct {
+	Pad1   uint16 `align:"pad1"`
+	Pad2   uint8  `align:"pad2"`
+	Family uint8  `align:"family"`
+	// represents both IPv6 and IPv4 (in the lowest four bytes)
+	IP types.IPv6 `align:"$union0"`
+}
+
+func (k *NodeKey) String() string {
+	switch k.Family {
+	case bpf.EndpointKeyIPv4:
+		return net.IP(k.IP[:net.IPv4len]).String()
+	case bpf.EndpointKeyIPv6:
+		return k.IP.String()
+	}
+	return "<unknown>"
+}
+
+func newNodeKey(ip net.IP) NodeKey {
+	result := NodeKey{}
+	if ip4 := ip.To4(); ip4 != nil {
+		result.Family = bpf.EndpointKeyIPv4
+		copy(result.IP[:], ip4)
+	} else {
+		result.Family = bpf.EndpointKeyIPv6
+		copy(result.IP[:], ip)
+	}
+	return result
+}
+
 type NodeValueV2 struct {
 	NodeID uint16
 	SPI    uint8
@@ -90,11 +112,6 @@ func (m *nodeMapV2) Update(ip net.IP, nodeID uint16, SPI uint8) error {
 		return fmt.Errorf("failed to update node map: %w", err)
 	}
 
-	// mirror write
-	if err := m.v1Map.Update(ip, nodeID); err != nil {
-		return fmt.Errorf("failed to mirror write to v1 node map: %w", err)
-	}
-
 	return nil
 }
 
@@ -108,11 +125,6 @@ func (m *nodeMapV2) Delete(ip net.IP) error {
 		return fmt.Errorf("failed to delete node map: %w", err)
 	}
 
-	// mirror write
-	if err := m.v1Map.Delete(ip); err != nil {
-		return fmt.Errorf("failed to mirror delete to v1 node map: %w", err)
-	}
-
 	return nil
 }
 
@@ -157,75 +169,9 @@ func LoadNodeMapV2(logger *slog.Logger) (MapV2, error) {
 	return &nodeMapV2{bpfMap: bpfMap}, nil
 }
 
-// migrateV1 will migrate the v1 NodeMap to this NodeMapv2
-//
-// Ensure this always occurs BEFORE we begin handling K8s Node events or else
-// both this migration and the node events will be writing to the map.
-//
-// This migration will leave the v1 NodeMap preset on the filesystem.
-// This is due to some unfortunately versioning requirements which forced this
-// migration to occur within a patch release.
-//
-// When Cilium reaches v1.17 the v1 NodeMap will no longer be required and we
-// can unpin the map after migration.
-func (m *nodeMapV2) migrateV1(NodeMapName string, EncryptMapName string) error {
-	m.logger.Debug("Detecting V1 to V2 migration")
-
-	// load v1 node map
-	nodeMapPath := bpf.MapPath(m.logger, NodeMapName)
-	v1, err := ebpf.LoadPinnedMap(m.logger, nodeMapPath)
-	if errors.Is(err, unix.ENOENT) {
-		m.logger.Debug("No v1 node map found, skipping migration")
-		return nil
-	}
-	if err != nil {
-		return err
-	}
-	nodeMap := nodeMap{
-		bpfMap: v1,
-	}
-
-	// load encrypt map to get current SPI
-	encryptMapPath := bpf.MapPath(m.logger, EncryptMapName)
-	en, err := ebpf.LoadPinnedMap(m.logger, encryptMapPath)
-	if errors.Is(err, unix.ENOENT) {
-		m.logger.Debug("No encrypt map found, skipping migration")
-		return nil
-	}
-	if err != nil {
-		return err
-	}
-	defer en.Close()
-
-	var SPI uint8
-	if err = en.Lookup(uint32(0), &SPI); err != nil {
-		return err
-	}
-
-	// reads v1 map entries and writes them to V2 with the latest SPI found
-	// from EncryptMap
-	count := 0
-	parse := func(k *NodeKey, v *NodeValue) {
-		v2 := NodeValueV2{
-			NodeID: v.NodeID,
-			SPI:    SPI,
-		}
-		count++
-		m.bpfMap.Put(k, &v2)
-	}
-
-	m.logger.Debug(
-		"Migrated V1 node map entries to V2",
-		logfields.SPI, SPI,
-		logfields.Entries, count,
-	)
-
-	err = nodeMap.IterateWithCallback(parse)
-	if err != nil {
-		return fmt.Errorf("failed to iterate v1 node map %w", err)
-	}
-
-	return nil
+// Clean up the v1 map. TODO remove this in v1.19.
+func (m *nodeMapV2) migrateV1(NodeMapName string) {
+	os.Remove(filepath.Join(bpf.TCGlobalsPath(), NodeMapName))
 }
 
 func (m *nodeMapV2) init() error {
diff --git a/pkg/maps/nodemap/node_map_v2_privileged_test.go b/pkg/maps/nodemap/node_map_v2_privileged_test.go
index e1c2753db3..1e4ec11786 100644
--- a/pkg/maps/nodemap/node_map_v2_privileged_test.go
+++ b/pkg/maps/nodemap/node_map_v2_privileged_test.go
@@ -7,13 +7,11 @@ import (
 	"net"
 	"testing"
 
-	ciliumebpf "github.com/cilium/ebpf"
 	"github.com/cilium/ebpf/rlimit"
 	"github.com/cilium/hive/hivetest"
 	"github.com/stretchr/testify/require"
 
 	"github.com/cilium/cilium/pkg/bpf"
-	"github.com/cilium/cilium/pkg/maps/encrypt"
 	"github.com/cilium/cilium/pkg/testutils"
 )
 
@@ -28,7 +26,7 @@ func setupNodeMapV2TestSuite(tb testing.TB) {
 func TestNodeMapV2(t *testing.T) {
 	setupNodeMapV2TestSuite(t)
 	logger := hivetest.Logger(t)
-	nodeMap := newMapV2(logger, "test_cilium_node_map_v2", "test_cilium_node_map", Config{
+	nodeMap := newMapV2(logger, "test_cilium_node_map_v2", Config{
 		NodeMapMax: 1024,
 	})
 	err := nodeMap.init()
@@ -72,89 +70,4 @@ func TestNodeMapV2(t *testing.T) {
 	require.NoError(t, err)
 	require.Len(t, bpfNodeIDMap, 1)
 	require.Len(t, bpfNodeSPI, 1)
-
-	// ensure we see mirrored writes in MapV1
-	_, err = ciliumebpf.LoadPinnedMap(bpf.MapPath(logger, "test_cilium_node_map"), nil)
-	require.NoError(t, err)
-
-	toMapV1 := func(key *NodeKey, val *NodeValue) {
-		address := key.IP.String()
-		if key.Family == bpf.EndpointKeyIPv4 {
-			address = net.IP(key.IP[:net.IPv4len]).String()
-		}
-		require.Equal(t, address, bpfNodeIDMap[val.NodeID])
-	}
-
-	err = nodeMap.v1Map.IterateWithCallback(toMapV1)
-	require.NoError(t, err)
-}
-
-func TestNodeMapMigration(t *testing.T) {
-	setupNodeMapV2TestSuite(t)
-	name1 := "test_cilium_node_map"
-	name2 := "test_cilium_node_map_v2"
-	emName := "test_cilium_encrypt_state"
-
-	IP1 := net.ParseIP("10.1.0.0")
-	IP2 := net.ParseIP("10.1.0.1")
-
-	var ID1 uint16 = 10
-	var ID2 uint16 = 20
-
-	logger := hivetest.Logger(t)
-	nodeMapV1 := newMap(logger, name1, Config{
-		NodeMapMax: 1024,
-	})
-	err := nodeMapV1.init()
-	require.NoError(t, err)
-
-	nodeMapV2 := newMapV2(logger, name2, name1, Config{
-		NodeMapMax: 1024,
-	})
-	err = nodeMapV2.init()
-	require.NoError(t, err)
-	defer nodeMapV2.bpfMap.Unpin()
-
-	encryptMap := encrypt.NewMap(emName)
-	err = encryptMap.OpenOrCreate()
-	require.NoError(t, err)
-
-	encrypt.MapUpdateContextWithMap(encryptMap, 0, 3)
-
-	err = nodeMapV1.Update(IP1, ID1)
-	require.NoError(t, err)
-	err = nodeMapV1.Update(IP2, ID2)
-	require.NoError(t, err)
-
-	// done with nodeMapV2 so we can close the FD.
-	nodeMapV2.close()
-
-	// do migration
-	err = nodeMapV2.migrateV1(name1, emName)
-	require.NoError(t, err)
-
-	// confirm we see the correct migrated values
-	parse := func(k *NodeKey, v *NodeValueV2) {
-		// family must be IPv4
-		if k.Family != bpf.EndpointKeyIPv4 {
-			t.Fatalf("want: %v, got: %v", bpf.EndpointKeyIPv4, k.Family)
-		}
-		ipv4 := net.IP(k.IP[:4])
-
-		// IP must equal one of our two test IPs
-		if !ipv4.Equal(IP1) && !ipv4.Equal(IP2) {
-			t.Fatalf("migrated NodeValue2 did not match any IP under test: %v", ipv4)
-		}
-
-		// SPI must equal 3
-		if v.SPI != 3 {
-			t.Fatalf("wanted: 3, got: %v", v.SPI)
-		}
-	}
-	MapV2(nodeMapV2).IterateWithCallback(parse)
-
-	// confirm that the map is not removed, we need it around to mirror writes
-	m, err := ciliumebpf.LoadPinnedMap(bpf.MapPath(logger, name1), nil)
-	require.NoError(t, err)
-	require.NotNil(t, m)
 }
diff --git a/pkg/metrics/bpf.go b/pkg/metrics/bpf.go
index 56c926ebff..17e3dd42ca 100644
--- a/pkg/metrics/bpf.go
+++ b/pkg/metrics/bpf.go
@@ -6,15 +6,16 @@ package metrics
 import (
 	"errors"
 	"fmt"
+	"log/slog"
 	"os"
 	"slices"
 	"strings"
 
+	"github.com/cilium/ebpf"
 	"github.com/prometheus/client_golang/prometheus"
-	"github.com/sirupsen/logrus"
 	"golang.org/x/sync/singleflight"
 
-	"github.com/cilium/ebpf"
+	"github.com/cilium/cilium/pkg/logging/logfields"
 )
 
 // This file contains a Prometheus collector that collects the memory usage of
@@ -164,7 +165,8 @@ func (v *bpfVisitor) visitMap(id ebpf.MapID) error {
 }
 
 type bpfCollector struct {
-	sfg singleflight.Group
+	logger *slog.Logger
+	sfg    singleflight.Group
 
 	bpfMapsCount      *prometheus.Desc
 	bpfMapsMemory     *prometheus.Desc
@@ -172,8 +174,9 @@ type bpfCollector struct {
 	bpfProgramsMemory *prometheus.Desc
 }
 
-func newbpfCollector() *bpfCollector {
+func newbpfCollector(logger *slog.Logger) *bpfCollector {
 	return &bpfCollector{
+		logger: logger,
 		bpfMapsCount: prometheus.NewDesc(
 			prometheus.BuildFQName(Namespace, "", "bpf_maps"),
 			"Total count of BPF maps.",
@@ -209,7 +212,7 @@ func (s *bpfCollector) Collect(ch chan<- prometheus.Metric) {
 	})
 
 	if err != nil {
-		logrus.WithError(err).Error("retrieving BPF maps & programs usage")
+		s.logger.Error("retrieving BPF maps & programs usage", logfields.Error, err)
 		return
 	}
 
diff --git a/pkg/metrics/features/metrics.go b/pkg/metrics/features/metrics.go
index 55946170b5..52077b44b5 100644
--- a/pkg/metrics/features/metrics.go
+++ b/pkg/metrics/features/metrics.go
@@ -95,6 +95,7 @@ const (
 	networkChainingModeCalico      = "calico"
 	networkChainingModeFlannel     = "flannel"
 	networkChainingModeGenericVeth = "generic-veth"
+	networkChainingModePortmap     = "portmap"
 
 	networkIPv4      = "ipv4-only"
 	networkIPv6      = "ipv6-only"
@@ -146,6 +147,7 @@ var (
 		networkChainingModeCalico,
 		networkChainingModeFlannel,
 		networkChainingModeGenericVeth,
+		networkChainingModePortmap,
 	}
 
 	defaultIPAddressFamilies = []string{
diff --git a/pkg/metrics/metric/metric.go b/pkg/metrics/metric/metric.go
index 39e13e9d8f..37a5b38439 100644
--- a/pkg/metrics/metric/metric.go
+++ b/pkg/metrics/metric/metric.go
@@ -6,16 +6,20 @@ package metric
 import (
 	"fmt"
 	"maps"
+	"os"
 	"slices"
+	"strconv"
 
 	"github.com/prometheus/client_golang/prometheus"
-	"github.com/sirupsen/logrus"
 
-	"github.com/cilium/cilium/pkg/logging/logfields"
 	"github.com/cilium/cilium/pkg/metrics/metric/collections"
 )
 
-var logger = logrus.WithField(logfields.LogSubsys, "metric")
+var invalidMetricValueDetectionEnabled = false
+
+func init() {
+	invalidMetricValueDetectionEnabled, _ = strconv.ParseBool(os.Getenv("CILIUM_INVALID_METRIC_VALUE_DETECTOR"))
+}
 
 // WithMetadata is the interface implemented by any metric defined in this package. These typically embed existing
 // prometheus metric types and add additional metadata. In addition, these metrics have the concept of being enabled
@@ -56,12 +60,11 @@ func (b *metric) checkLabelValues(lvs ...string) {
 	if b.labels == nil {
 		return
 	}
-	if err := b.labels.checkLabelValues(lvs); err != nil {
-		logger.WithError(err).
-			WithFields(logrus.Fields{
-				"metric": b.opts.Name,
-			}).
-			Warning("metric label constraints violated, metric will still be collected")
+
+	if invalidMetricValueDetectionEnabled {
+		if err := b.labels.checkLabelValues(lvs); err != nil {
+			panic("metric label constraints violated for metric " + b.opts.Name + ": " + err.Error())
+		}
 	}
 }
 
@@ -70,12 +73,10 @@ func (b *metric) checkLabels(labels prometheus.Labels) {
 		return
 	}
 
-	if err := b.labels.checkLabels(labels); err != nil {
-		logger.WithError(err).
-			WithFields(logrus.Fields{
-				"metric": b.opts.Name,
-			}).
-			Warning("metric label constraints violated, metric will still be collected")
+	if invalidMetricValueDetectionEnabled {
+		if err := b.labels.checkLabels(labels); err != nil {
+			panic("metric label constraints violated for metric " + b.opts.Name + ": " + err.Error())
+		}
 	}
 }
 
diff --git a/pkg/metrics/metrics.go b/pkg/metrics/metrics.go
index 42cd54535b..89260469ce 100644
--- a/pkg/metrics/metrics.go
+++ b/pkg/metrics/metrics.go
@@ -15,8 +15,8 @@ import (
 
 	"github.com/prometheus/client_golang/prometheus"
 	dto "github.com/prometheus/client_model/go"
-	"github.com/sirupsen/logrus"
 
+	"github.com/cilium/cilium/pkg/logging/logfields"
 	"github.com/cilium/cilium/pkg/metrics/metric"
 	"github.com/cilium/cilium/pkg/promise"
 	"github.com/cilium/cilium/pkg/source"
@@ -223,19 +223,6 @@ const (
 	// LabelTargetCluster is the label for target cluster name
 	LabelTargetCluster = "target_cluster"
 
-	// LabelTargetNodeIP is the label for target node IP
-	LabelTargetNodeIP = "target_node_ip"
-
-	// LabelTargetNodeName is the label for target node name
-	LabelTargetNodeName = "target_node_name"
-
-	// LabelTargetNodeType is the label for target node type (local_node, remote_intra_cluster, vs remote_inter_cluster)
-	LabelTargetNodeType = "target_node_type"
-
-	LabelLocationLocalNode          = "local_node"
-	LabelLocationRemoteIntraCluster = "remote_intra_cluster"
-	LabelLocationRemoteInterCluster = "remote_inter_cluster"
-
 	// Rule label is a label for a L7 rule name.
 	LabelL7Rule = "rule"
 
@@ -282,14 +269,6 @@ var (
 
 	// Status
 
-	// NodeConnectivityStatus is the connectivity status between local node to
-	// other node intra or inter cluster.
-	NodeConnectivityStatus = NoOpGaugeDeletableVec
-
-	// NodeConnectivityLatency is the connectivity latency between local node to
-	// other node intra or inter cluster.
-	NodeConnectivityLatency = NoOpGaugeDeletableVec
-
 	// NodeHealthConnectivityStatus is the number of connections with connectivity status
 	// between local node to other node intra or inter cluster.
 	NodeHealthConnectivityStatus = NoOpGaugeVec
@@ -327,15 +306,6 @@ var (
 	// Policy is the number of policies loaded into the agent
 	Policy = NoOpGauge
 
-	// PolicyRegenerationCount is the total number of successful policy
-	// regenerations.
-	// Deprecated: Use EndpointRegenerationTotal.
-	PolicyRegenerationCount = NoOpCounter
-
-	// PolicyRegenerationTimeStats is the total time taken to generate policies.
-	// Deprecated: Use EndpointRegenerationTimeStats.
-	PolicyRegenerationTimeStats = NoOpObserverVec
-
 	// PolicyRevision is the current policy revision number for this agent
 	PolicyRevision = NoOpGauge
 
@@ -358,12 +328,6 @@ var (
 	// directly added to policy maps without a full policy recalculation.
 	PolicyIncrementalUpdateDuration = NoOpObserverVec
 
-	// CIDRGroup
-
-	// CIDRGroupsReferenced is the number of CNPs and CCNPs referencing at least one CiliumCIDRGroup.
-	// CNPs with empty or non-existing CIDRGroupRefs are not considered.
-	CIDRGroupsReferenced = NoOpGauge
-
 	// Identity
 
 	// Identity is the number of identities currently in use on the node by type
@@ -477,10 +441,6 @@ var (
 	// kube-apiserver.
 	KubernetesAPICallsTotal = NoOpCounterVec
 
-	// KubernetesCNPStatusCompletion is the number of seconds it takes to
-	// complete a CNP status update
-	KubernetesCNPStatusCompletion = NoOpObserverVec
-
 	// TerminatingEndpointsEvents is the number of terminating endpoint events received from kubernetes.
 	TerminatingEndpointsEvents = NoOpCounter
 
@@ -660,8 +620,6 @@ var (
 type LegacyMetrics struct {
 	BootstrapTimes                   metric.Vec[metric.Gauge]
 	APIInteractions                  metric.Vec[metric.Observer]
-	NodeConnectivityStatus           metric.DeletableVec[metric.Gauge]
-	NodeConnectivityLatency          metric.DeletableVec[metric.Gauge]
 	NodeHealthConnectivityStatus     metric.Vec[metric.Gauge]
 	NodeHealthConnectivityLatency    metric.Vec[metric.Observer]
 	Endpoint                         metric.GaugeFunc
@@ -671,14 +629,11 @@ type LegacyMetrics struct {
 	EndpointRegenerationTimeStats    metric.Vec[metric.Observer]
 	EndpointPropagationDelay         metric.Vec[metric.Observer]
 	Policy                           metric.Gauge
-	PolicyRegenerationCount          metric.Counter
-	PolicyRegenerationTimeStats      metric.Vec[metric.Observer]
 	PolicyRevision                   metric.Gauge
 	PolicyChangeTotal                metric.Vec[metric.Counter]
 	PolicyEndpointStatus             metric.Vec[metric.Gauge]
 	PolicyImplementationDelay        metric.Vec[metric.Observer]
 	PolicyIncrementalUpdateDuration  metric.Vec[metric.Observer]
-	CIDRGroupsReferenced             metric.Gauge
 	Identity                         metric.Vec[metric.Gauge]
 	IdentityLabelSources             metric.Vec[metric.Gauge]
 	EventTS                          metric.Vec[metric.Gauge]
@@ -706,7 +661,6 @@ type LegacyMetrics struct {
 	KubernetesAPIInteractions        metric.Vec[metric.Observer]
 	KubernetesAPIRateLimiterLatency  metric.Vec[metric.Observer]
 	KubernetesAPICallsTotal          metric.Vec[metric.Counter]
-	KubernetesCNPStatusCompletion    metric.Vec[metric.Observer]
 	TerminatingEndpointsEvents       metric.Counter
 	IPAMEvent                        metric.Vec[metric.Counter]
 	IPAMCapacity                     metric.Vec[metric.Gauge]
@@ -769,7 +723,7 @@ func NewLegacyMetrics() *LegacyMetrics {
 		}, metric.Labels{
 			{
 				Name:   LabelOutcome,
-				Values: metric.NewValues(LabelValueOutcomeSuccess, LabelValueOutcomeFailure),
+				Values: metric.NewValues(LabelValueOutcomeSuccess, LabelValueOutcomeFail),
 			},
 		}),
 
@@ -797,20 +751,6 @@ func NewLegacyMetrics() *LegacyMetrics {
 			Help:       "Number of policies currently loaded",
 		}),
 
-		PolicyRegenerationCount: metric.NewCounter(metric.CounterOpts{
-			ConfigName: Namespace + "_policy_regeneration_total",
-			Namespace:  Namespace,
-			Name:       "policy_regeneration_total",
-			Help:       "Total number of successful policy regenerations",
-		}),
-
-		PolicyRegenerationTimeStats: metric.NewHistogramVec(metric.HistogramOpts{
-			ConfigName: Namespace + "_policy_regeneration_time_stats_seconds",
-			Namespace:  Namespace,
-			Name:       "policy_regeneration_time_stats_seconds",
-			Help:       "Policy regeneration time stats labeled by the scope",
-		}, []string{LabelScope, LabelStatus}),
-
 		PolicyRevision: metric.NewGauge(metric.GaugeOpts{
 			ConfigName: Namespace + "_policy_max_revision",
 			Namespace:  Namespace,
@@ -861,14 +801,6 @@ func NewLegacyMetrics() *LegacyMetrics {
 			Buckets:   prometheus.ExponentialBuckets(10e-6, 10, 8),
 		}, []string{"scope"}),
 
-		CIDRGroupsReferenced: metric.NewGauge(metric.GaugeOpts{
-			ConfigName: Namespace + "cidrgroups_referenced",
-
-			Namespace: Namespace,
-			Name:      "cidrgroups_referenced",
-			Help:      "Number of CNPs and CCNPs referencing at least one CiliumCIDRGroup. CNPs with empty or non-existing CIDRGroupRefs are not considered",
-		}),
-
 		Identity: metric.NewGaugeVec(metric.GaugeOpts{
 			ConfigName: Namespace + "_identity",
 
@@ -1089,14 +1021,6 @@ func NewLegacyMetrics() *LegacyMetrics {
 			Help:       "Number of API calls made to kube-apiserver labeled by host, method and return code.",
 		}, []string{"host", LabelMethod, LabelAPIReturnCode}),
 
-		KubernetesCNPStatusCompletion: metric.NewHistogramVec(metric.HistogramOpts{
-			ConfigName: Namespace + "_" + SubsystemK8s + "_cnp_status_completion_seconds",
-			Namespace:  Namespace,
-			Subsystem:  SubsystemK8s,
-			Name:       "cnp_status_completion_seconds",
-			Help:       "Duration in seconds in how long it took to complete a CNP status update",
-		}, []string{LabelAttempts, LabelOutcome}),
-
 		TerminatingEndpointsEvents: metric.NewCounter(metric.CounterOpts{
 			ConfigName: Namespace + "_" + SubsystemK8s + "_terminating_endpoints_events_total",
 			Namespace:  Namespace,
@@ -1310,37 +1234,6 @@ func NewLegacyMetrics() *LegacyMetrics {
 			Buckets:    []float64{.05, .1, 1, 5, 30, 60, 120, 240, 300, 600},
 		}, []string{}),
 
-		NodeConnectivityStatus: metric.NewGaugeVec(metric.GaugeOpts{
-			ConfigName: Namespace + "_node_connectivity_status",
-			Namespace:  Namespace,
-			Name:       "node_connectivity_status",
-			Help:       "The last observed status of both ICMP and HTTP connectivity between the current Cilium agent and other Cilium nodes",
-		}, []string{
-			LabelSourceCluster,
-			LabelSourceNodeName,
-			LabelTargetCluster,
-			LabelTargetNodeName,
-			LabelTargetNodeType,
-			LabelType,
-		}),
-
-		NodeConnectivityLatency: metric.NewGaugeVec(metric.GaugeOpts{
-			ConfigName: Namespace + "_node_connectivity_latency_seconds",
-			Namespace:  Namespace,
-			Name:       "node_connectivity_latency_seconds",
-			Help:       "The last observed latency between the current Cilium agent and other Cilium nodes in seconds",
-		}, []string{
-			LabelSourceCluster,
-			LabelSourceNodeName,
-			LabelTargetCluster,
-			LabelTargetNodeName,
-			LabelTargetNodeIP,
-			LabelTargetNodeType,
-			LabelType,
-			LabelProtocol,
-			LabelAddressType,
-		}),
-
 		NodeHealthConnectivityStatus: metric.NewGaugeVec(metric.GaugeOpts{
 			ConfigName: Namespace + "_node_health_connectivity_status",
 			Namespace:  Namespace,
@@ -1391,8 +1284,6 @@ func NewLegacyMetrics() *LegacyMetrics {
 
 	BootstrapTimes = lm.BootstrapTimes
 	APIInteractions = lm.APIInteractions
-	NodeConnectivityStatus = lm.NodeConnectivityStatus
-	NodeConnectivityLatency = lm.NodeConnectivityLatency
 	NodeHealthConnectivityStatus = lm.NodeHealthConnectivityStatus
 	NodeHealthConnectivityLatency = lm.NodeHealthConnectivityLatency
 	Endpoint = lm.Endpoint
@@ -1402,14 +1293,11 @@ func NewLegacyMetrics() *LegacyMetrics {
 	EndpointRegenerationTimeStats = lm.EndpointRegenerationTimeStats
 	EndpointPropagationDelay = lm.EndpointPropagationDelay
 	Policy = lm.Policy
-	PolicyRegenerationCount = lm.PolicyRegenerationCount
-	PolicyRegenerationTimeStats = lm.PolicyRegenerationTimeStats
 	PolicyRevision = lm.PolicyRevision
 	PolicyChangeTotal = lm.PolicyChangeTotal
 	PolicyEndpointStatus = lm.PolicyEndpointStatus
 	PolicyImplementationDelay = lm.PolicyImplementationDelay
 	PolicyIncrementalUpdateDuration = lm.PolicyIncrementalUpdateDuration
-	CIDRGroupsReferenced = lm.CIDRGroupsReferenced
 	Identity = lm.Identity
 	IdentityLabelSources = lm.IdentityLabelSources
 	EventTS = lm.EventTS
@@ -1437,7 +1325,6 @@ func NewLegacyMetrics() *LegacyMetrics {
 	KubernetesAPIInteractions = lm.KubernetesAPIInteractions
 	KubernetesAPIRateLimiterLatency = lm.KubernetesAPIRateLimiterLatency
 	KubernetesAPICallsTotal = lm.KubernetesAPICallsTotal
-	KubernetesCNPStatusCompletion = lm.KubernetesCNPStatusCompletion
 	TerminatingEndpointsEvents = lm.TerminatingEndpointsEvents
 	IPAMEvent = lm.IPAMEvent
 	IPAMCapacity = lm.IPAMCapacity
@@ -1502,13 +1389,16 @@ func (gwt *GaugeWithThreshold) Set(value float64) {
 	if gwt.active && !overThreshold {
 		gwt.active = !gwt.reg.Unregister(gwt.gauge)
 		if gwt.active {
-			logrus.WithField("metric", gwt.gauge.Desc().String()).Warning("Failed to unregister metric")
+			gwt.reg.params.Logger.Warn("Failed to unregister metric", logfields.MetricConfig, gwt.gauge.Desc())
 		}
 	} else if !gwt.active && overThreshold {
 		err := gwt.reg.Register(gwt.gauge)
 		gwt.active = err == nil
 		if err != nil {
-			logrus.WithField("metric", gwt.gauge.Desc().String()).WithError(err).Warning("Failed to register metric")
+			gwt.reg.params.Logger.Warn("Failed to register metric",
+				logfields.Error, err,
+				logfields.MetricConfig, gwt.gauge.Desc(),
+			)
 		}
 	}
 
@@ -1516,7 +1406,7 @@ func (gwt *GaugeWithThreshold) Set(value float64) {
 }
 
 // NewGaugeWithThresholdForRegistry creates a new GaugeWithThreshold.
-func (reg *Registry) NewGaugeWithThreshold(name string, subsystem string, desc string, labels map[string]string, threshold float64) *GaugeWithThreshold {
+func (reg *Registry) NewGaugeWithThreshold(name, subsystem, desc string, labels map[string]string, threshold float64) *GaugeWithThreshold {
 	return &GaugeWithThreshold{
 		reg: reg,
 		gauge: prometheus.NewGauge(prometheus.GaugeOpts{
diff --git a/pkg/metrics/metrics_test.go b/pkg/metrics/metrics_test.go
index 6cb9072d30..d59744d29d 100644
--- a/pkg/metrics/metrics_test.go
+++ b/pkg/metrics/metrics_test.go
@@ -6,16 +6,19 @@ package metrics
 import (
 	"testing"
 
+	"github.com/cilium/hive/hivetest"
 	"github.com/stretchr/testify/require"
 
 	"github.com/cilium/cilium/pkg/option"
 )
 
 func TestGaugeWithThreshold(t *testing.T) {
+	logger := hivetest.Logger(t)
 	threshold := 1.0
 	underThreshold := threshold - 0.5
 	overThreshold := threshold + 0.5
 	reg := NewRegistry(RegistryParams{
+		Logger:       logger,
 		DaemonConfig: &option.DaemonConfig{},
 	})
 
diff --git a/pkg/metrics/registry.go b/pkg/metrics/registry.go
index c426696e07..322c0f5722 100644
--- a/pkg/metrics/registry.go
+++ b/pkg/metrics/registry.go
@@ -5,6 +5,8 @@ package metrics
 
 import (
 	"errors"
+	"fmt"
+	"log/slog"
 	"net/http"
 	"regexp"
 	"strings"
@@ -14,10 +16,10 @@ import (
 	"github.com/prometheus/client_golang/prometheus"
 	"github.com/prometheus/client_golang/prometheus/collectors"
 	"github.com/prometheus/client_golang/prometheus/promhttp"
-	"github.com/sirupsen/logrus"
 	"github.com/spf13/pflag"
 
 	"github.com/cilium/cilium/pkg/lock"
+	"github.com/cilium/cilium/pkg/logging/logfields"
 	metricpkg "github.com/cilium/cilium/pkg/metrics/metric"
 	"github.com/cilium/cilium/pkg/option"
 )
@@ -42,7 +44,7 @@ func (rc RegistryConfig) Flags(flags *pflag.FlagSet) {
 type RegistryParams struct {
 	cell.In
 
-	Logger     logrus.FieldLogger
+	Logger     *slog.Logger
 	Shutdowner hive.Shutdowner
 	Lifecycle  cell.Lifecycle
 
@@ -91,7 +93,7 @@ func NewRegistry(params RegistryParams) *Registry {
 		params.Lifecycle.Append(cell.Hook{
 			OnStart: func(hc cell.HookContext) error {
 				go func() {
-					params.Logger.Infof("Serving prometheus metrics on %s", params.Config.PrometheusServeAddr)
+					params.Logger.Info("Serving prometheus metrics", logfields.Address, params.Config.PrometheusServeAddr)
 					err := srv.ListenAndServe()
 					if err != nil && !errors.Is(err, http.ErrServerClosed) {
 						params.Shutdowner.Shutdown(hive.ShutdownWithError(err))
@@ -136,8 +138,8 @@ func (r *Registry) Reinitialize() {
 
 	// Don't register status and BPF collectors into the [r.collectors] as it is
 	// expensive to sample and currently not terrible useful to keep data on.
-	r.inner.MustRegister(metricpkg.EnabledCollector{C: newStatusCollector()})
-	r.inner.MustRegister(metricpkg.EnabledCollector{C: newbpfCollector()})
+	r.inner.MustRegister(metricpkg.EnabledCollector{C: newStatusCollector(r.params.Logger)})
+	r.inner.MustRegister(metricpkg.EnabledCollector{C: newbpfCollector(r.params.Logger)})
 
 	metrics := make(map[string]metricpkg.WithMetadata)
 	for i, autoMetric := range r.params.AutoMetrics {
@@ -173,9 +175,10 @@ func (r *Registry) Reinitialize() {
 		case '-':
 			metric.SetEnabled(false)
 		default:
-			r.params.Logger.Warning(
-				"--metrics flag contains value which does not start with + or -, '%s', ignoring",
-				metricFlag,
+			r.params.Logger.Warn(
+				fmt.Sprintf(
+					"--metrics flag contains value which does not start with + or -, '%s', ignoring",
+					metricFlag),
 			)
 		}
 	}
diff --git a/pkg/metrics/sampler_test.go b/pkg/metrics/sampler_test.go
index aaefe615e7..0969063fd5 100644
--- a/pkg/metrics/sampler_test.go
+++ b/pkg/metrics/sampler_test.go
@@ -32,7 +32,7 @@ func TestSamplerMaxMemoryUsage(t *testing.T) {
 
 func TestSampler(t *testing.T) {
 	log := hivetest.Logger(t)
-	reg := &Registry{inner: prometheus.NewPedanticRegistry()}
+	reg := &Registry{params: RegistryParams{Logger: log}, inner: prometheus.NewPedanticRegistry()}
 
 	counter := prometheus.NewCounter(prometheus.CounterOpts{Name: "counter"})
 	reg.Register(counter)
diff --git a/pkg/metrics/status.go b/pkg/metrics/status.go
index 98c4c9d9e0..ed4644990c 100644
--- a/pkg/metrics/status.go
+++ b/pkg/metrics/status.go
@@ -4,14 +4,18 @@
 package metrics
 
 import (
+	"log/slog"
+
 	"github.com/prometheus/client_golang/prometheus"
-	"github.com/sirupsen/logrus"
 
 	clientPkg "github.com/cilium/cilium/pkg/client"
 	healthClientPkg "github.com/cilium/cilium/pkg/health/client"
+	"github.com/cilium/cilium/pkg/logging"
+	"github.com/cilium/cilium/pkg/logging/logfields"
 )
 
 type statusCollector struct {
+	logger                   *slog.Logger
 	daemonHealthGetter       daemonHealthGetter
 	connectivityStatusGetter connectivityStatusGetter
 
@@ -21,23 +25,24 @@ type statusCollector struct {
 	unreachableHealthEndpointsDesc *prometheus.Desc
 }
 
-func newStatusCollector() *statusCollector {
+func newStatusCollector(logger *slog.Logger) *statusCollector {
 	ciliumClient, err := clientPkg.NewClient("")
 	if err != nil {
-		logrus.WithError(err).Fatal("Error while creating Cilium API client")
+		logging.Fatal(logger, "Error while creating Cilium API client", logfields.Error, err)
 	}
 
 	healthClient, err := healthClientPkg.NewClient("")
 	if err != nil {
-		logrus.WithError(err).Fatal("Error while creating cilium-health API client")
+		logging.Fatal(logger, "Error while creating cilium-health API client", logfields.Error, err)
 	}
 
-	return newStatusCollectorWithClients(ciliumClient.Daemon, healthClient.Connectivity)
+	return newStatusCollectorWithClients(logger, ciliumClient.Daemon, healthClient.Connectivity)
 }
 
 // newStatusCollectorWithClients provides a constructor with injected clients
-func newStatusCollectorWithClients(d daemonHealthGetter, c connectivityStatusGetter) *statusCollector {
+func newStatusCollectorWithClients(logger *slog.Logger, d daemonHealthGetter, c connectivityStatusGetter) *statusCollector {
 	return &statusCollector{
+		logger:                   logger,
 		daemonHealthGetter:       d,
 		connectivityStatusGetter: c,
 		controllersFailingDesc: prometheus.NewDesc(
@@ -73,7 +78,7 @@ func (s *statusCollector) Describe(ch chan<- *prometheus.Desc) {
 func (s *statusCollector) Collect(ch chan<- prometheus.Metric) {
 	statusResponse, err := s.daemonHealthGetter.GetHealthz(nil)
 	if err != nil {
-		logrus.WithError(err).Error("Error while getting Cilium status")
+		s.logger.Error("Error while getting Cilium status", logfields.Error, err)
 		return
 	}
 
@@ -118,7 +123,7 @@ func (s *statusCollector) Collect(ch chan<- prometheus.Metric) {
 
 	healthStatusResponse, err := s.connectivityStatusGetter.GetStatus(nil)
 	if err != nil {
-		logrus.WithError(err).Error("Error while getting cilium-health status")
+		s.logger.Error("Error while getting cilium-health status", logfields.Error, err)
 		return
 	}
 
diff --git a/pkg/metrics/status_test.go b/pkg/metrics/status_test.go
index 06223c876a..ac97a066c8 100644
--- a/pkg/metrics/status_test.go
+++ b/pkg/metrics/status_test.go
@@ -7,6 +7,7 @@ import (
 	"strings"
 	"testing"
 
+	"github.com/cilium/hive/hivetest"
 	"github.com/prometheus/client_golang/prometheus/testutil"
 	"github.com/stretchr/testify/require"
 
@@ -123,6 +124,7 @@ func (f *fakeDaemonClient) GetHealthz(params *daemon.GetHealthzParams, opts ...d
 }
 
 func Test_statusCollector_Collect(t *testing.T) {
+	logger := hivetest.Logger(t)
 	tests := []struct {
 		name                 string
 		healthResponse       *daemon.GetHealthzOK
@@ -141,7 +143,7 @@ func Test_statusCollector_Collect(t *testing.T) {
 
 	for _, tt := range tests {
 		t.Log("Test :", tt.name)
-		collector := newStatusCollectorWithClients(&fakeDaemonClient{
+		collector := newStatusCollectorWithClients(logger, &fakeDaemonClient{
 			response: tt.healthResponse,
 		}, &fakeConnectivityClient{
 			response: tt.connectivityResponse,
diff --git a/pkg/node/address.go b/pkg/node/address.go
index e0d655955e..eaefa177dd 100644
--- a/pkg/node/address.go
+++ b/pkg/node/address.go
@@ -173,15 +173,15 @@ func clone(ip net.IP) net.IP {
 	return dup
 }
 
-// GetIPv4Loopback returns the loopback IPv4 address of this node.
-func GetIPv4Loopback(logger *slog.Logger) net.IP {
-	return getLocalNode(logger).IPv4Loopback
+// GetServiceLoopbackIPv4 returns the service loopback IPv4 address of this node.
+func GetServiceLoopbackIPv4(logger *slog.Logger) net.IP {
+	return getLocalNode(logger).ServiceLoopbackIPv4
 }
 
-// SetIPv4Loopback sets the loopback IPv4 address of this node.
-func SetIPv4Loopback(ip net.IP) {
+// SetIPv4Loopback sets the service loopback IPv4 address of this node.
+func SetServiceLoopbackIPv4(ip net.IP) {
 	localNode.Update(func(n *LocalNode) {
-		n.IPv4Loopback = ip
+		n.ServiceLoopbackIPv4 = ip
 	})
 }
 
diff --git a/pkg/node/address_test.go b/pkg/node/address_test.go
index c42a346a9e..7d02aab5b2 100644
--- a/pkg/node/address_test.go
+++ b/pkg/node/address_test.go
@@ -36,7 +36,6 @@ func Test_getCiliumHostIPsFromFile(t *testing.T) {
 
 #define ENABLE_IPV4 1
 #define IPV4_GATEWAY 0x100000a
-#define IPV4_LOOPBACK 0x5dd0000a
 #define IPV4_MASK 0xffff
 #define HOST_IP 0xfd, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xb
 #define HOST_ID 1
diff --git a/pkg/node/local_node_store.go b/pkg/node/local_node_store.go
index e72accb399..03782f3c86 100644
--- a/pkg/node/local_node_store.go
+++ b/pkg/node/local_node_store.go
@@ -38,7 +38,9 @@ type LocalNode struct {
 	IPv4NativeRoutingCIDR *cidr.CIDR
 	// v6 CIDR in which pod IPs are routable
 	IPv6NativeRoutingCIDR *cidr.CIDR
-	IPv4Loopback          net.IP
+	// ServiceLoopbackIPv4 is the source address used for SNAT when a Pod talks to
+	// itself through a Service.
+	ServiceLoopbackIPv4 net.IP
 }
 
 // LocalNodeSynchronizer specifies how to build, and keep synchronized the local
diff --git a/pkg/node/manager/manager.go b/pkg/node/manager/manager.go
index 3be90db974..6527977f50 100644
--- a/pkg/node/manager/manager.go
+++ b/pkg/node/manager/manager.go
@@ -24,7 +24,6 @@ import (
 	"github.com/cilium/statedb"
 	"github.com/google/renameio/v2"
 	jsoniter "github.com/json-iterator/go"
-	"github.com/prometheus/client_golang/prometheus"
 	"go4.org/netipx"
 	"golang.org/x/sys/unix"
 	"golang.org/x/time/rate"
@@ -255,31 +254,6 @@ type nodeMetrics struct {
 	DatapathValidations metric.Counter
 }
 
-// ProcessNodeDeletion upon node deletion ensures metrics associated
-// with the deleted node are no longer reported.
-// Notably for metrics node connectivity status and latency metrics
-func (*nodeMetrics) ProcessNodeDeletion(clusterName, nodeName string) {
-	// Removes all connectivity status associated with the deleted node.
-	_ = metrics.NodeConnectivityStatus.DeletePartialMatch(prometheus.Labels{
-		metrics.LabelSourceCluster:  clusterName,
-		metrics.LabelSourceNodeName: nodeName,
-	})
-	_ = metrics.NodeConnectivityStatus.DeletePartialMatch(prometheus.Labels{
-		metrics.LabelTargetCluster:  clusterName,
-		metrics.LabelTargetNodeName: nodeName,
-	})
-
-	// Removes all connectivity latency associated with the deleted node.
-	_ = metrics.NodeConnectivityLatency.DeletePartialMatch(prometheus.Labels{
-		metrics.LabelSourceCluster:  clusterName,
-		metrics.LabelSourceNodeName: nodeName,
-	})
-	_ = metrics.NodeConnectivityLatency.DeletePartialMatch(prometheus.Labels{
-		metrics.LabelTargetCluster:  clusterName,
-		metrics.LabelTargetNodeName: nodeName,
-	})
-}
-
 func NewNodeMetrics() *nodeMetrics {
 	return &nodeMetrics{
 		EventsReceived: metric.NewCounterVec(metric.CounterOpts{
@@ -1291,7 +1265,6 @@ func (m *manager) NodeDeleted(n nodeTypes.Node) {
 	}
 
 	m.metrics.NumNodes.Dec()
-	m.metrics.ProcessNodeDeletion(n.Cluster, n.Name)
 
 	entry.mutex.Lock()
 	delete(m.nodes, nodeIdentifier)
diff --git a/pkg/node/zz_generated.deepequal.go b/pkg/node/zz_generated.deepequal.go
index 15e86c02d1..724179907b 100644
--- a/pkg/node/zz_generated.deepequal.go
+++ b/pkg/node/zz_generated.deepequal.go
@@ -44,8 +44,8 @@ func (in *LocalNode) DeepEqual(other *LocalNode) bool {
 		}
 	}
 
-	if ((in.IPv4Loopback != nil) && (other.IPv4Loopback != nil)) || ((in.IPv4Loopback == nil) != (other.IPv4Loopback == nil)) {
-		in, other := &in.IPv4Loopback, &other.IPv4Loopback
+	if ((in.ServiceLoopbackIPv4 != nil) && (other.ServiceLoopbackIPv4 != nil)) || ((in.ServiceLoopbackIPv4 == nil) != (other.ServiceLoopbackIPv4 == nil)) {
+		in, other := &in.ServiceLoopbackIPv4, &other.ServiceLoopbackIPv4
 		if other == nil {
 			return false
 		}
diff --git a/pkg/option/config.go b/pkg/option/config.go
index 55b5e0d2d0..75462507b4 100644
--- a/pkg/option/config.go
+++ b/pkg/option/config.go
@@ -819,8 +819,8 @@ const (
 	// EndpointRegenInterval is the interval of the periodic endpoint regeneration loop.
 	EndpointRegenInterval = "endpoint-regen-interval"
 
-	// LoopbackIPv4 is the address to use for service loopback SNAT
-	LoopbackIPv4 = "ipv4-service-loopback-address"
+	// ServiceLoopbackIPv4 is the address to use for service loopback SNAT
+	ServiceLoopbackIPv4 = "ipv4-service-loopback-address"
 
 	// LocalRouterIPv4 is the link-local IPv4 address to use for Cilium router device
 	LocalRouterIPv4 = "local-router-ipv4"
@@ -1021,10 +1021,6 @@ const (
 	// BGP router-id allocation IP pool
 	BGPRouterIDAllocationIPPool = "bgp-router-id-allocation-ip-pool"
 
-	// EnableRuntimeDeviceDetection is the name of the option to enable detection
-	// of new and removed datapath devices during the agent runtime.
-	EnableRuntimeDeviceDetection = "enable-runtime-device-detection"
-
 	// EnablePMTUDiscovery enables path MTU discovery to send ICMP
 	// fragmentation-needed replies to the client (when needed).
 	EnablePMTUDiscovery = "enable-pmtu-discovery"
@@ -1272,11 +1268,6 @@ type DaemonConfig struct {
 	EncryptInterface   []string // Set of network facing interface to encrypt over
 	EncryptNode        bool     // Set to true for encrypting node IP traffic
 
-	// If set to true the daemon will detect new and deleted datapath devices
-	// at runtime and reconfigure the datapath to load programs onto the new
-	// devices.
-	EnableRuntimeDeviceDetection bool
-
 	DatapathMode string // Datapath mode
 	RoutingMode  string // Routing mode
 
@@ -1746,8 +1737,8 @@ type DaemonConfig struct {
 	// the specified maximum value.
 	ConntrackGCMaxInterval time.Duration
 
-	// LoopbackIPv4 is the address to use for service loopback SNAT
-	LoopbackIPv4 string
+	// ServiceLoopbackIPv4 is the address to use for service loopback SNAT
+	ServiceLoopbackIPv4 string
 
 	// LocalRouterIPv4 is the link-local IPv4 address used for Cilium's router device
 	LocalRouterIPv4 string
@@ -2139,7 +2130,7 @@ var (
 		FixedIdentityMapping:            make(map[string]string),
 		KVStoreOpt:                      make(map[string]string),
 		LogOpt:                          make(map[string]string),
-		LoopbackIPv4:                    defaults.LoopbackIPv4,
+		ServiceLoopbackIPv4:             defaults.ServiceLoopbackIPv4,
 		EnableEndpointRoutes:            defaults.EnableEndpointRoutes,
 		AnnotateK8sNode:                 defaults.AnnotateK8sNode,
 		K8sServiceCacheSize:             defaults.K8sServiceCacheSize,
@@ -2785,7 +2776,7 @@ func (c *DaemonConfig) Populate(logger *slog.Logger, vp *viper.Viper) {
 	c.Labels = vp.GetStringSlice(Labels)
 	c.LibDir = vp.GetString(LibDir)
 	c.LogSystemLoadConfig = vp.GetBool(LogSystemLoadConfigName)
-	c.LoopbackIPv4 = vp.GetString(LoopbackIPv4)
+	c.ServiceLoopbackIPv4 = vp.GetString(ServiceLoopbackIPv4)
 	c.LocalRouterIPv4 = vp.GetString(LocalRouterIPv4)
 	c.LocalRouterIPv6 = vp.GetString(LocalRouterIPv6)
 	c.EnableBPFClockProbe = vp.GetBool(EnableBPFClockProbe)
@@ -2857,7 +2848,6 @@ func (c *DaemonConfig) Populate(logger *slog.Logger, vp *viper.Viper) {
 	}
 
 	c.populateLoadBalancerSettings(logger, vp)
-	c.EnableRuntimeDeviceDetection = vp.GetBool(EnableRuntimeDeviceDetection)
 	c.EgressMultiHomeIPRuleCompat = vp.GetBool(EgressMultiHomeIPRuleCompat)
 	c.InstallUplinkRoutesForDelegatedIPAM = vp.GetBool(InstallUplinkRoutesForDelegatedIPAM)
 
diff --git a/pkg/policy/cell/policy_importer_test.go b/pkg/policy/cell/policy_importer_test.go
index da2c6b18ee..75bd3fd73a 100644
--- a/pkg/policy/cell/policy_importer_test.go
+++ b/pkg/policy/cell/policy_importer_test.go
@@ -14,6 +14,7 @@ import (
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	k8sTypes "k8s.io/apimachinery/pkg/types"
 
+	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	"github.com/cilium/cilium/pkg/container/set"
 	"github.com/cilium/cilium/pkg/identity"
 	"github.com/cilium/cilium/pkg/ipcache"
@@ -510,7 +511,7 @@ func TestAddCiliumNetworkPolicyByLabels(t *testing.T) {
 			args.repo.GetSelectorCache().SetLocalIdentityNotifier(testidentity.NewDummyIdentityNotifier())
 			want.repo.GetSelectorCache().SetLocalIdentityNotifier(testidentity.NewDummyIdentityNotifier())
 
-			rules, policyImportErr := args.cnp.Parse(hivetest.Logger(t))
+			rules, policyImportErr := args.cnp.Parse(hivetest.Logger(t), cmtypes.PolicyAnyCluster)
 			require.Equal(t, want.err, policyImportErr)
 
 			// Only add policies if we have successfully parsed them. Otherwise, if
diff --git a/pkg/policy/directory/cell.go b/pkg/policy/directory/cell.go
index 3a2b91bb22..263635d08b 100644
--- a/pkg/policy/directory/cell.go
+++ b/pkg/policy/directory/cell.go
@@ -11,6 +11,7 @@ import (
 	"github.com/cilium/hive/cell"
 	"github.com/spf13/pflag"
 
+	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	cilium_v2 "github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2"
 	policycell "github.com/cilium/cilium/pkg/policy/cell"
 )
@@ -33,9 +34,11 @@ type DirectoryWatcherReadStatus interface {
 type PolicyWatcherParams struct {
 	cell.In
 
-	Lifecycle cell.Lifecycle
-	Logger    *slog.Logger
-	Importer  policycell.PolicyImporter
+	Lifecycle               cell.Lifecycle
+	Logger                  *slog.Logger
+	Importer                policycell.PolicyImporter
+	ClusterInfo             cmtypes.ClusterInfo
+	ClusterMeshPolicyConfig cmtypes.PolicyConfig
 }
 
 type Config struct {
@@ -83,6 +86,7 @@ func newPolicyWatcher(p PolicyWatcherParams, cfg Config) *policyWatcher {
 		log:                p.Logger,
 		policyImporter:     p.Importer,
 		config:             cfg,
+		clusterName:        cmtypes.LocalClusterNameForPolicies(p.ClusterMeshPolicyConfig, p.ClusterInfo.Name),
 		fileNameToCnpCache: make(map[string]*cilium_v2.CiliumNetworkPolicy),
 	}
 	w.synced.Add(1)
diff --git a/pkg/policy/directory/watcher.go b/pkg/policy/directory/watcher.go
index 4a91b18c17..d8dd6b338f 100644
--- a/pkg/policy/directory/watcher.go
+++ b/pkg/policy/directory/watcher.go
@@ -34,6 +34,7 @@ type policyWatcher struct {
 	log            *slog.Logger
 	config         Config
 	policyImporter policycell.PolicyImporter
+	clusterName    string
 	synced         sync.WaitGroup
 	// maps cnp file name to cnp object. this is required to retrieve data during delete.
 	fileNameToCnpCache map[string]*cilium_v2.CiliumNetworkPolicy
@@ -96,7 +97,7 @@ func (p *policyWatcher) addToPolicyEngine(cnp *cilium_v2.CiliumNetworkPolicy, cn
 	)
 
 	// convert to rules
-	rules, err := cnp.Parse(p.log)
+	rules, err := cnp.Parse(p.log, p.clusterName)
 	if err != nil {
 		return err
 	}
diff --git a/pkg/policy/groups/actions.go b/pkg/policy/groups/actions.go
index 313cf5eba9..fbb6819385 100644
--- a/pkg/policy/groups/actions.go
+++ b/pkg/policy/groups/actions.go
@@ -26,7 +26,7 @@ var (
 
 // AddDerivativePolicyIfNeeded will create a new CNP if the given CNP has any rules
 // that need to create a new derivative policy.
-func AddDerivativePolicyIfNeeded(logger *slog.Logger, clientset client.Clientset, cnp *cilium_v2.CiliumNetworkPolicy, clusterScoped bool) {
+func AddDerivativePolicyIfNeeded(logger *slog.Logger, clientset client.Clientset, clusterName string, cnp *cilium_v2.CiliumNetworkPolicy, clusterScoped bool) {
 	if !cnp.RequiresDerivative() {
 		logger.Debug(
 			"Policy does not have derivative policies, skipped",
@@ -40,7 +40,7 @@ func AddDerivativePolicyIfNeeded(logger *slog.Logger, clientset client.Clientset
 		controller.ControllerParams{
 			Group: addDerivativePolicyControllerGroup,
 			DoFunc: func(ctx context.Context) error {
-				return addDerivativePolicy(ctx, logger, clientset, cnp, clusterScoped)
+				return addDerivativePolicy(ctx, logger, clientset, clusterName, cnp, clusterScoped)
 			},
 		})
 }
@@ -51,7 +51,7 @@ func AddDerivativePolicyIfNeeded(logger *slog.Logger, clientset client.Clientset
 // one, it will delete the old policy.
 // The function returns true if an update is required for the derivative policy
 // and false otherwise.
-func UpdateDerivativePolicyIfNeeded(logger *slog.Logger, clientset client.Clientset, newCNP *cilium_v2.CiliumNetworkPolicy, oldCNP *cilium_v2.CiliumNetworkPolicy, clusterScoped bool) bool {
+func UpdateDerivativePolicyIfNeeded(logger *slog.Logger, clientset client.Clientset, clusterName string, newCNP *cilium_v2.CiliumNetworkPolicy, oldCNP *cilium_v2.CiliumNetworkPolicy, clusterScoped bool) bool {
 	if !newCNP.RequiresDerivative() && oldCNP.RequiresDerivative() {
 		logger.Info(
 			"New policy does not have derivative policy, but old had. Deleting old policies",
@@ -95,7 +95,7 @@ func UpdateDerivativePolicyIfNeeded(logger *slog.Logger, clientset client.Client
 		controller.ControllerParams{
 			Group: updateDerivativePolicyControllerGroup,
 			DoFunc: func(ctx context.Context) error {
-				return addDerivativePolicy(ctx, logger, clientset, newCNP, clusterScoped)
+				return addDerivativePolicy(ctx, logger, clientset, clusterName, newCNP, clusterScoped)
 			},
 		})
 	return true
@@ -107,7 +107,7 @@ func DeleteDerivativeFromCache(cnp *cilium_v2.CiliumNetworkPolicy) {
 	groupsCNPCache.DeleteCNP(cnp)
 }
 
-func addDerivativePolicy(ctx context.Context, logger *slog.Logger, clientset client.Clientset, cnp *cilium_v2.CiliumNetworkPolicy, clusterScoped bool) error {
+func addDerivativePolicy(ctx context.Context, logger *slog.Logger, clientset client.Clientset, clusterName string, cnp *cilium_v2.CiliumNetworkPolicy, clusterScoped bool) error {
 	var (
 		scopedLog          *slog.Logger
 		derivativePolicy   v1.Object
@@ -130,10 +130,10 @@ func addDerivativePolicy(ctx context.Context, logger *slog.Logger, clientset cli
 	// the derivative status in the parent policy  will be updated with the
 	// error.
 	if clusterScoped {
-		derivativeCCNP, derivativeErr = createDerivativeCCNP(ctx, logger, cnp)
+		derivativeCCNP, derivativeErr = createDerivativeCCNP(ctx, logger, clusterName, cnp)
 		derivativePolicy = derivativeCCNP
 	} else {
-		derivativeCNP, derivativeErr = createDerivativeCNP(ctx, logger, cnp)
+		derivativeCNP, derivativeErr = createDerivativeCNP(ctx, logger, clusterName, cnp)
 		derivativePolicy = derivativeCNP
 	}
 
diff --git a/pkg/policy/groups/controllers.go b/pkg/policy/groups/controllers.go
index aaeff8c78d..712a7f2718 100644
--- a/pkg/policy/groups/controllers.go
+++ b/pkg/policy/groups/controllers.go
@@ -20,7 +20,7 @@ const (
 // from providers.  To avoid issues with rate-limiting this function will
 // execute the addDerivative function with a max number of concurrent calls,
 // defined on maxConcurrentUpdates.
-func UpdateCNPInformation(logger *slog.Logger, clientset client.Clientset) {
+func UpdateCNPInformation(logger *slog.Logger, clientset client.Clientset, clusterName string) {
 	cnpToUpdate := groupsCNPCache.GetAllCNP()
 	sem := make(chan bool, maxConcurrentUpdates)
 	for _, cnp := range cnpToUpdate {
@@ -29,11 +29,10 @@ func UpdateCNPInformation(logger *slog.Logger, clientset client.Clientset) {
 			defer func() { <-sem }()
 			// We use the same cache for Clusterwide and Namespaced cilium policies
 			if cnp.ObjectMeta.Namespace == "" {
-				addDerivativePolicy(context.TODO(), logger, clientset, cnp, true)
+				addDerivativePolicy(context.TODO(), logger, clientset, clusterName, cnp, true)
 			} else {
-				addDerivativePolicy(context.TODO(), logger, clientset, cnp, false)
+				addDerivativePolicy(context.TODO(), logger, clientset, clusterName, cnp, false)
 			}
-
 		}(cnp)
 	}
 }
diff --git a/pkg/policy/groups/helpers.go b/pkg/policy/groups/helpers.go
index 20c5c40cab..707e90fcf8 100644
--- a/pkg/policy/groups/helpers.go
+++ b/pkg/policy/groups/helpers.go
@@ -27,7 +27,7 @@ func getDerivativeName(obj v1.Object) string {
 }
 
 // createDerivativeCNP will return a new CNP based on the given rule.
-func createDerivativeCNP(ctx context.Context, logger *slog.Logger, cnp *cilium_v2.CiliumNetworkPolicy) (*cilium_v2.CiliumNetworkPolicy, error) {
+func createDerivativeCNP(ctx context.Context, logger *slog.Logger, clusterName string, cnp *cilium_v2.CiliumNetworkPolicy) (*cilium_v2.CiliumNetworkPolicy, error) {
 	// CNP informer may provide a CNP object without APIVersion or Kind.
 	// Setting manually to make sure that the derivative policy works ok.
 	derivativeCNP := &cilium_v2.CiliumNetworkPolicy{
@@ -52,8 +52,7 @@ func createDerivativeCNP(ctx context.Context, logger *slog.Logger, cnp *cilium_v
 		err   error
 	)
 
-	rules, err = cnp.Parse(logger)
-
+	rules, err = cnp.Parse(logger, clusterName)
 	if err != nil {
 		// We return a valid pointer for derivative policy here instead of nil.
 		// This object is used to get generated name for the derivative policy
@@ -67,7 +66,7 @@ func createDerivativeCNP(ctx context.Context, logger *slog.Logger, cnp *cilium_v
 }
 
 // createDerivativeCCNP will return a new CCNP based on the given rule.
-func createDerivativeCCNP(ctx context.Context, logger *slog.Logger, cnp *cilium_v2.CiliumNetworkPolicy) (*cilium_v2.CiliumClusterwideNetworkPolicy, error) {
+func createDerivativeCCNP(ctx context.Context, logger *slog.Logger, clusterName string, cnp *cilium_v2.CiliumNetworkPolicy) (*cilium_v2.CiliumClusterwideNetworkPolicy, error) {
 	ccnp := &cilium_v2.CiliumClusterwideNetworkPolicy{
 		TypeMeta:   cnp.TypeMeta,
 		ObjectMeta: cnp.ObjectMeta,
@@ -100,8 +99,7 @@ func createDerivativeCCNP(ctx context.Context, logger *slog.Logger, cnp *cilium_
 		err   error
 	)
 
-	rules, err = ccnp.Parse(logger)
-
+	rules, err = ccnp.Parse(logger, clusterName)
 	if err != nil {
 		// We return a valid pointer for derivative policy here instead of nil.
 		// This object is used to get generated name for the derivative policy
diff --git a/pkg/policy/groups/helpers_test.go b/pkg/policy/groups/helpers_test.go
index b99410069f..cfb545ef18 100644
--- a/pkg/policy/groups/helpers_test.go
+++ b/pkg/policy/groups/helpers_test.go
@@ -13,6 +13,7 @@ import (
 	"github.com/stretchr/testify/require"
 	"k8s.io/apimachinery/pkg/types"
 
+	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	cilium_v2 "github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2"
 	slim_metav1 "github.com/cilium/cilium/pkg/k8s/slim/k8s/apis/meta/v1"
 	"github.com/cilium/cilium/pkg/policy/api"
@@ -42,14 +43,14 @@ func TestCorrectDerivativeName(t *testing.T) {
 	name := "test"
 	cnp := getSamplePolicy(name, "testns")
 	logger := hivetest.Logger(t)
-	cnpDerivedPolicy, err := createDerivativeCNP(context.TODO(), logger, cnp)
+	cnpDerivedPolicy, err := createDerivativeCNP(context.TODO(), logger, cmtypes.PolicyAnyCluster, cnp)
 	require.NoError(t, err)
 	require.Equal(t, fmt.Sprintf("%s-groups-%s", name, cnp.ObjectMeta.UID), cnpDerivedPolicy.ObjectMeta.Name)
 
 	// Test clusterwide policy helper functions
 	ccnpName := "ccnp-test"
 	ccnp := getSamplePolicy(ccnpName, "")
-	ccnpDerivedPolicy, err := createDerivativeCCNP(context.TODO(), logger, ccnp)
+	ccnpDerivedPolicy, err := createDerivativeCCNP(context.TODO(), logger, cmtypes.PolicyAnyCluster, ccnp)
 
 	require.NoError(t, err)
 	require.Equal(t, fmt.Sprintf("%s-groups-%s", ccnpName, ccnp.ObjectMeta.UID), ccnpDerivedPolicy.ObjectMeta.Name)
@@ -74,7 +75,7 @@ func TestDerivativePoliciesAreDeletedIfNogroups(t *testing.T) {
 	cnp.Spec.Egress = egressRule
 
 	logger := hivetest.Logger(t)
-	cnpDerivedPolicy, err := createDerivativeCNP(context.TODO(), logger, cnp)
+	cnpDerivedPolicy, err := createDerivativeCNP(context.TODO(), logger, cmtypes.PolicyAnyCluster, cnp)
 	require.NoError(t, err)
 	require.Equal(t, cnp.Spec.Egress, cnpDerivedPolicy.Specs[0].Egress)
 	require.Len(t, cnpDerivedPolicy.Specs, 1)
@@ -84,7 +85,7 @@ func TestDerivativePoliciesAreDeletedIfNogroups(t *testing.T) {
 	ccnp := getSamplePolicy(ccnpName, "")
 	ccnp.Spec.Egress = egressRule
 
-	ccnpDerivedPolicy, err := createDerivativeCCNP(context.TODO(), logger, ccnp)
+	ccnpDerivedPolicy, err := createDerivativeCCNP(context.TODO(), logger, cmtypes.PolicyAnyCluster, ccnp)
 	require.NoError(t, err)
 	require.Equal(t, ccnp.Spec.Egress, ccnpDerivedPolicy.Specs[0].Egress)
 	require.Len(t, ccnpDerivedPolicy.Specs, 1)
@@ -125,7 +126,7 @@ func TestDerivativePoliciesAreInheritCorrectly(t *testing.T) {
 
 	cnp.Spec.Egress = egressRule
 
-	cnpDerivedPolicy, err := createDerivativeCNP(context.TODO(), hivetest.Logger(t), cnp)
+	cnpDerivedPolicy, err := createDerivativeCNP(context.TODO(), hivetest.Logger(t), cmtypes.PolicyAnyCluster, cnp)
 	require.NoError(t, err)
 	require.Nil(t, cnpDerivedPolicy.Spec)
 	require.Len(t, cnpDerivedPolicy.Specs, 1)
@@ -137,7 +138,7 @@ func TestDerivativePoliciesAreInheritCorrectly(t *testing.T) {
 	ccnp := getSamplePolicy(ccnpName, "")
 	ccnp.Spec.Egress = egressRule
 
-	ccnpDerivedPolicy, err := createDerivativeCCNP(context.TODO(), hivetest.Logger(t), ccnp)
+	ccnpDerivedPolicy, err := createDerivativeCCNP(context.TODO(), hivetest.Logger(t), cmtypes.PolicyAnyCluster, ccnp)
 	require.NoError(t, err)
 	require.Nil(t, ccnpDerivedPolicy.Spec)
 	require.Len(t, ccnpDerivedPolicy.Specs, 1)
diff --git a/pkg/policy/k8s/cell.go b/pkg/policy/k8s/cell.go
index 5f9aa1bb60..f71a51f7ba 100644
--- a/pkg/policy/k8s/cell.go
+++ b/pkg/policy/k8s/cell.go
@@ -11,6 +11,7 @@ import (
 	"github.com/cilium/hive/cell"
 	"k8s.io/apimachinery/pkg/util/sets"
 
+	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	"github.com/cilium/cilium/pkg/ipcache"
 	"github.com/cilium/cilium/pkg/k8s"
 	cilium_v2 "github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2"
@@ -65,9 +66,10 @@ type PolicyWatcherParams struct {
 
 	Lifecycle cell.Lifecycle
 
-	ClientSet client.Clientset
-	Config    *option.DaemonConfig
-	Logger    *slog.Logger
+	ClientSet               client.Clientset
+	Config                  *option.DaemonConfig
+	ClusterMeshPolicyConfig cmtypes.PolicyConfig
+	Logger                  *slog.Logger
 
 	K8sResourceSynced *synced.Resources
 	K8sAPIGroups      *synced.APIGroups
@@ -96,6 +98,7 @@ func startK8sPolicyWatcher(params PolicyWatcherParams) {
 	p := &policyWatcher{
 		log:                              params.Logger,
 		config:                           params.Config,
+		clusterMeshPolicyConfig:          params.ClusterMeshPolicyConfig,
 		policyImporter:                   params.PolicyImporter,
 		k8sResourceSynced:                params.K8sResourceSynced,
 		k8sAPIGroups:                     params.K8sAPIGroups,
diff --git a/pkg/policy/k8s/cilium_network_policy.go b/pkg/policy/k8s/cilium_network_policy.go
index e2eefc853c..06d2de7ec7 100644
--- a/pkg/policy/k8s/cilium_network_policy.go
+++ b/pkg/policy/k8s/cilium_network_policy.go
@@ -7,6 +7,7 @@ import (
 	"context"
 	"fmt"
 
+	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	ipcacheTypes "github.com/cilium/cilium/pkg/ipcache/types"
 	"github.com/cilium/cilium/pkg/k8s/resource"
 	"github.com/cilium/cilium/pkg/k8s/types"
@@ -139,7 +140,7 @@ func (p *policyWatcher) upsertCiliumNetworkPolicyV2(cnp *types.SlimCNP, initialR
 		p.metricsManager.AddCNP(cnp.CiliumNetworkPolicy)
 	}
 
-	rules, err := cnp.Parse(scopedLog)
+	rules, err := cnp.Parse(scopedLog, cmtypes.LocalClusterNameForPolicies(p.clusterMeshPolicyConfig, p.config.ClusterName))
 	if err != nil {
 		scopedLog.Warn(
 			"Unable to add CiliumNetworkPolicy",
diff --git a/pkg/policy/k8s/network_policy.go b/pkg/policy/k8s/network_policy.go
index 787ef2079a..f9985c3ee5 100644
--- a/pkg/policy/k8s/network_policy.go
+++ b/pkg/policy/k8s/network_policy.go
@@ -14,12 +14,12 @@ import (
 	"github.com/cilium/cilium/pkg/source"
 )
 
-func (p *policyWatcher) addK8sNetworkPolicyV1(k8sNP *slim_networkingv1.NetworkPolicy, apiGroup string, dc chan uint64) error {
+func (p *policyWatcher) addK8sNetworkPolicyV1(k8sNP *slim_networkingv1.NetworkPolicy, apiGroup string, dc chan uint64, clusterName string) error {
 	defer func() {
 		p.k8sResourceSynced.SetEventTimestamp(apiGroup)
 	}()
 
-	rules, err := k8s.ParseNetworkPolicy(p.log, k8sNP)
+	rules, err := k8s.ParseNetworkPolicy(p.log, clusterName, k8sNP)
 	if err != nil {
 		metrics.PolicyChangeTotal.WithLabelValues(metrics.LabelValueOutcomeFail).Inc()
 		p.log.Error(
diff --git a/pkg/policy/k8s/watcher.go b/pkg/policy/k8s/watcher.go
index e63b5480fb..afc6513b16 100644
--- a/pkg/policy/k8s/watcher.go
+++ b/pkg/policy/k8s/watcher.go
@@ -11,6 +11,7 @@ import (
 
 	"k8s.io/apimachinery/pkg/util/sets"
 
+	cmtypes "github.com/cilium/cilium/pkg/clustermesh/types"
 	ipcacheTypes "github.com/cilium/cilium/pkg/ipcache/types"
 	"github.com/cilium/cilium/pkg/k8s"
 	cilium_v2 "github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2"
@@ -23,8 +24,9 @@ import (
 )
 
 type policyWatcher struct {
-	log    *slog.Logger
-	config *option.DaemonConfig
+	log                     *slog.Logger
+	config                  *option.DaemonConfig
+	clusterMeshPolicyConfig cmtypes.PolicyConfig
 
 	k8sResourceSynced *k8sSynced.Resources
 	k8sAPIGroups      *k8sSynced.APIGroups
@@ -156,7 +158,10 @@ func (p *policyWatcher) watchResources(ctx context.Context) {
 				var err error
 				switch event.Kind {
 				case resource.Upsert:
-					err = p.addK8sNetworkPolicyV1(event.Object, k8sAPIGroupNetworkingV1Core, knpDone)
+					err = p.addK8sNetworkPolicyV1(
+						event.Object, k8sAPIGroupNetworkingV1Core, knpDone,
+						cmtypes.LocalClusterNameForPolicies(p.clusterMeshPolicyConfig, p.config.ClusterName),
+					)
 				case resource.Delete:
 					err = p.deleteK8sNetworkPolicyV1(event.Object, k8sAPIGroupNetworkingV1Core, knpDone)
 				}
diff --git a/pkg/policy/repository.go b/pkg/policy/repository.go
index 55b6975a5e..51eed3feea 100644
--- a/pkg/policy/repository.go
+++ b/pkg/policy/repository.go
@@ -171,10 +171,11 @@ func (p *Repository) addListLocked(rules api.Rules) (ruleSlice, uint64) {
 func (p *Repository) insert(r *rule) {
 	p.rules[r.key] = r
 	p.metricsManager.AddRule(r.Rule)
-	if _, ok := p.rulesByNamespace[r.key.resource.Namespace()]; !ok {
-		p.rulesByNamespace[r.key.resource.Namespace()] = sets.New[ruleKey]()
+	namespace := r.key.resource.Namespace()
+	if _, ok := p.rulesByNamespace[namespace]; !ok {
+		p.rulesByNamespace[namespace] = sets.New[ruleKey]()
 	}
-	p.rulesByNamespace[r.key.resource.Namespace()].Insert(r.key)
+	p.rulesByNamespace[namespace].Insert(r.key)
 	rid := r.key.resource
 	if len(rid) > 0 {
 		if p.rulesByResource[rid] == nil {
@@ -193,9 +194,10 @@ func (p *Repository) del(key ruleKey) {
 	}
 	p.metricsManager.DelRule(r.Rule)
 	delete(p.rules, key)
-	p.rulesByNamespace[key.resource.Namespace()].Delete(key)
-	if len(p.rulesByNamespace[key.resource.Namespace()]) == 0 {
-		delete(p.rulesByNamespace, key.resource.Namespace())
+	namespace := r.key.resource.Namespace()
+	p.rulesByNamespace[namespace].Delete(key)
+	if len(p.rulesByNamespace[namespace]) == 0 {
+		delete(p.rulesByNamespace, namespace)
 	}
 
 	rid := key.resource
diff --git a/pkg/pprof/cell.go b/pkg/pprof/cell.go
index 408c12e74e..b21d1f456e 100644
--- a/pkg/pprof/cell.go
+++ b/pkg/pprof/cell.go
@@ -5,14 +5,16 @@ package pprof
 
 import (
 	"errors"
+	"log/slog"
 	"net"
 	"net/http"
 	"net/http/pprof"
 	"strconv"
 
 	"github.com/cilium/hive/cell"
-	"github.com/sirupsen/logrus"
 	"github.com/spf13/pflag"
+
+	"github.com/cilium/cilium/pkg/logging/logfields"
 )
 
 const (
@@ -58,7 +60,7 @@ func (def Config) Flags(flags *pflag.FlagSet) {
 	flags.Uint16(PprofPort, def.PprofPort, "Port that pprof listens on")
 }
 
-func newServer(lc cell.Lifecycle, log logrus.FieldLogger, cfg Config) Server {
+func newServer(lc cell.Lifecycle, log *slog.Logger, cfg Config) Server {
 	if !cfg.Pprof {
 		return nil
 	}
@@ -74,7 +76,7 @@ func newServer(lc cell.Lifecycle, log logrus.FieldLogger, cfg Config) Server {
 }
 
 type server struct {
-	logger logrus.FieldLogger
+	logger *slog.Logger
 
 	address string
 	port    uint16
@@ -90,10 +92,10 @@ func (s *server) Start(ctx cell.HookContext) error {
 	}
 	s.listener = listener
 
-	s.logger = s.logger.WithFields(logrus.Fields{
-		"ip":   s.listener.Addr().(*net.TCPAddr).IP,
-		"port": s.listener.Addr().(*net.TCPAddr).Port,
-	})
+	s.logger = s.logger.With(
+		logfields.IPAddr, s.listener.Addr().(*net.TCPAddr).IP,
+		logfields.Port, s.listener.Addr().(*net.TCPAddr).Port,
+	)
 
 	mux := http.NewServeMux()
 	mux.HandleFunc("/debug/pprof/", pprof.Index)
@@ -107,7 +109,7 @@ func (s *server) Start(ctx cell.HookContext) error {
 	}
 	go func() {
 		if err := s.httpSrv.Serve(s.listener); !errors.Is(err, http.ErrServerClosed) {
-			s.logger.WithError(err).Error("server stopped unexpectedly")
+			s.logger.Error("server stopped unexpectedly", logfields.Error, err)
 		}
 	}()
 	s.logger.Info("Started pprof server")
diff --git a/pkg/pprof/pprof.go b/pkg/pprof/pprof.go
index edddc7fa81..a50338d34b 100644
--- a/pkg/pprof/pprof.go
+++ b/pkg/pprof/pprof.go
@@ -6,25 +6,23 @@ package pprof
 
 import (
 	"errors"
+	"log/slog"
 	"net"
 	"net/http"
 	_ "net/http/pprof"
 	"strconv"
 
-	"github.com/cilium/cilium/pkg/logging"
 	"github.com/cilium/cilium/pkg/logging/logfields"
 )
 
-var log = logging.DefaultLogger.WithField(logfields.LogSubsys, "pprof")
-
 // Enable runs an HTTP server to serve the pprof API
 //
 // Deprecated: use pprof.Cell() instead.
-func Enable(host string, port int) {
+func Enable(logger *slog.Logger, host string, port int) {
 	var apiAddress = net.JoinHostPort(host, strconv.Itoa(port))
 	go func() {
 		if err := http.ListenAndServe(apiAddress, nil); !errors.Is(err, http.ErrServerClosed) {
-			log.WithError(err).Warn("Unable to serve pprof API")
+			logger.Warn("Unable to serve pprof API", logfields.Error, err)
 		}
 	}()
 }
diff --git a/pkg/proxy/endpoint/endpoint.go b/pkg/proxy/endpoint/endpoint.go
index 91d3ba4d30..ec84668e2c 100644
--- a/pkg/proxy/endpoint/endpoint.go
+++ b/pkg/proxy/endpoint/endpoint.go
@@ -37,4 +37,8 @@ type EndpointUpdater interface {
 	// desired policy, if any.
 	// Must be called with Endpoint's read lock taken.
 	GetPolicyVersionHandle() *versioned.VersionHandle
+
+	// GetListenerProxyPort returns the proxy port for the given listener reference.
+	// Returns zero if the proxy port does not exist (yet).
+	GetListenerProxyPort(listener string) uint16
 }
diff --git a/pkg/proxy/endpoint/test/updater_mock.go b/pkg/proxy/endpoint/test/updater_mock.go
index 6d2d2339b2..df4aea5a7f 100644
--- a/pkg/proxy/endpoint/test/updater_mock.go
+++ b/pkg/proxy/endpoint/test/updater_mock.go
@@ -48,3 +48,7 @@ func (m *ProxyUpdaterMock) OnDNSPolicyUpdateLocked(rules restore.DNSRules) {}
 func (m *ProxyUpdaterMock) GetPolicyVersionHandle() *versioned.VersionHandle {
 	return m.VersionHandle
 }
+
+func (m *ProxyUpdaterMock) GetListenerProxyPort(listener string) uint16 {
+	return 0
+}
diff --git a/pkg/proxy/proxy.go b/pkg/proxy/proxy.go
index 53d2135cec..cedc4d49a8 100644
--- a/pkg/proxy/proxy.go
+++ b/pkg/proxy/proxy.go
@@ -116,6 +116,11 @@ func (p *Proxy) GetOpenLocalPorts() map[uint16]struct{} {
 	return p.proxyPorts.GetOpenLocalPorts()
 }
 
+func (p *Proxy) GetListenerProxyPort(listener string) uint16 {
+	proxyPort, _, _ := p.proxyPorts.GetProxyPort(listener)
+	return proxyPort
+}
+
 // CreateOrUpdateRedirect creates or updates a L4 redirect with corresponding
 // proxy configuration. This will allocate a proxy port as required and launch
 // a proxy instance. If the redirect is already in place, only the rules will be
diff --git a/pkg/signal/cell.go b/pkg/signal/cell.go
index a060257225..6900283994 100644
--- a/pkg/signal/cell.go
+++ b/pkg/signal/cell.go
@@ -4,6 +4,8 @@
 package signal
 
 import (
+	"log/slog"
+
 	"github.com/cilium/hive/cell"
 
 	"github.com/cilium/cilium/pkg/maps/signalmap"
@@ -17,8 +19,8 @@ var Cell = cell.Module(
 	cell.Provide(provideSignalManager),
 )
 
-func provideSignalManager(lifecycle cell.Lifecycle, signalMap signalmap.Map) SignalManager {
-	sm := newSignalManager(signalMap)
+func provideSignalManager(lifecycle cell.Lifecycle, logger *slog.Logger, signalMap signalmap.Map) SignalManager {
+	sm := newSignalManager(signalMap, logger)
 
 	lifecycle.Append(cell.Hook{
 		OnStart: func(startCtx cell.HookContext) error {
diff --git a/pkg/signal/signal.go b/pkg/signal/signal.go
index 63079393d3..887f7a711b 100644
--- a/pkg/signal/signal.go
+++ b/pkg/signal/signal.go
@@ -9,22 +9,19 @@ import (
 	"errors"
 	"fmt"
 	"io"
+	"log/slog"
 	"os"
 	"sync/atomic"
 
 	"github.com/cilium/ebpf/perf"
-	"github.com/sirupsen/logrus"
 
 	"github.com/cilium/cilium/pkg/byteorder"
 	"github.com/cilium/cilium/pkg/lock"
-	"github.com/cilium/cilium/pkg/logging"
 	"github.com/cilium/cilium/pkg/logging/logfields"
 	"github.com/cilium/cilium/pkg/maps/signalmap"
 	"github.com/cilium/cilium/pkg/metrics"
 )
 
-var log = logging.DefaultLogger.WithField(logfields.LogSubsys, "signal")
-
 type SignalType uint32
 
 const (
@@ -69,6 +66,7 @@ type SignalManager interface {
 }
 
 type signalManager struct {
+	logger    *slog.Logger
 	signalmap signalmap.Map
 	handlers  [SignalTypeMax]SignalHandler
 	events    signalmap.PerfReader
@@ -81,8 +79,9 @@ type signalManager struct {
 	activeSignals atomic.Uint64
 }
 
-func newSignalManager(signalMap signalmap.Map) *signalManager {
+func newSignalManager(signalMap signalmap.Map, logger *slog.Logger) *signalManager {
 	return &signalManager{
+		logger:    logger,
 		signalmap: signalMap,
 		done:      make(chan struct{}),
 	}
@@ -136,12 +135,15 @@ func (sm *signalManager) signalReceive(msg *perf.Record) {
 	var which SignalType
 	reader := bytes.NewReader(msg.RawSample)
 	if err := binary.Read(reader, byteorder.Native, &which); err != nil {
-		log.WithError(err).Warning("cannot parse signal type from BPF datapath")
+		sm.logger.Warn("cannot parse signal type from BPF datapath", logfields.Error, err)
 		return
 	}
 
 	if which >= SignalTypeMax {
-		log.WithField(logfields.Signal, which).Warning("invalid signal type")
+		sm.logger.Warn(
+			"invalid signal type",
+			logfields.Signal, which,
+		)
 		return
 	}
 
@@ -162,7 +164,11 @@ func (sm *signalManager) signalReceive(msg *perf.Record) {
 		if errors.Is(err, ErrFullChannel) {
 			status = "channel overflow"
 		} else {
-			log.WithError(err).WithField(logfields.Signal, name).Warning("cannot parse signal data from BPF datapath")
+			sm.logger.Warn(
+				"cannot parse signal data from BPF datapath",
+				logfields.Error, err,
+				logfields.Signal, name,
+			)
 			status = "parse error"
 		}
 	}
@@ -261,7 +267,7 @@ func (sm *signalManager) start() error {
 	}
 
 	go func() {
-		log.Info("Datapath signal listener running")
+		sm.logger.Info("Datapath signal listener running")
 		for {
 			record, err := sm.events.Read()
 			if err != nil {
@@ -269,9 +275,11 @@ func (sm *signalManager) start() error {
 					break
 				}
 				signalCollectMetrics("", "", "error")
-				log.WithError(err).WithFields(logrus.Fields{
-					logfields.BPFMapName: signalmap.MapName,
-				}).Error("failed to read event")
+				sm.logger.Error(
+					"failed to read event",
+					logfields.Error, err,
+					logfields.BPFMapName, signalmap.MapName,
+				)
 				continue
 			}
 
@@ -281,7 +289,7 @@ func (sm *signalManager) start() error {
 			}
 			sm.signalReceive(&record)
 		}
-		log.Info("Datapath signal listener exiting")
+		sm.logger.Info("Datapath signal listener exiting")
 
 		// Close registered signal channels
 		for i, handler := range sm.handlers {
@@ -291,7 +299,7 @@ func (sm *signalManager) start() error {
 			}
 		}
 		close(sm.done)
-		log.Info("Datapath signal listener done")
+		sm.logger.Info("Datapath signal listener done")
 	}()
 
 	return nil
diff --git a/pkg/signal/signal_test.go b/pkg/signal/signal_test.go
index 87cd38737c..1d88e9fb46 100644
--- a/pkg/signal/signal_test.go
+++ b/pkg/signal/signal_test.go
@@ -11,6 +11,7 @@ import (
 	"time"
 
 	"github.com/cilium/ebpf/perf"
+	"github.com/cilium/hive/hivetest"
 	"github.com/stretchr/testify/require"
 
 	"github.com/cilium/cilium/pkg/byteorder"
@@ -160,6 +161,7 @@ func (d SignalData) String() string {
 
 func TestLifeCycle(t *testing.T) {
 	logging.SetLogLevelToDebug()
+	logger := hivetest.Logger(t)
 
 	buf1 := new(bytes.Buffer)
 	binary.Write(buf1, byteorder.Native, SignalNatFillUp)
@@ -171,7 +173,7 @@ func TestLifeCycle(t *testing.T) {
 
 	messages := [][]byte{buf1.Bytes(), buf2.Bytes()}
 
-	sm := newSignalManager(fakesignalmap.NewFakeSignalMap(messages, time.Second))
+	sm := newSignalManager(fakesignalmap.NewFakeSignalMap(messages, time.Second), logger)
 	require.True(t, sm.isMuted())
 
 	wakeup := make(chan SignalData, 1024)
diff --git a/test/config/config.go b/test/config/config.go
index 1f274e8d28..ae690d993d 100644
--- a/test/config/config.go
+++ b/test/config/config.go
@@ -44,9 +44,8 @@ type CiliumTestConfigType struct {
 
 	// Multinode enables the running of tests that involve more than one
 	// node. If false, some tests will silently skip multinode checks.
-	Multinode      bool
-	RunQuarantined bool
-	Help           bool
+	Multinode bool
+	Help      bool
 }
 
 // CiliumTestConfig holds the global configuration of commandline flags
@@ -93,8 +92,6 @@ func (c *CiliumTestConfigType) ParseFlags() {
 		"Registry credentials to be used to download images")
 	flagset.BoolVar(&c.Multinode, "cilium.multinode", true,
 		"Enable tests across multiple nodes. If disabled, such tests may silently pass")
-	flagset.BoolVar(&c.RunQuarantined, "cilium.runQuarantined", false,
-		"Run tests that are under quarantine.")
 	flagset.BoolVar(&c.Help, "cilium.help", false, "Display this help message.")
 	flagset.StringVar(&c.InstallHelmOverrides, "cilium.install-helm-overrides", "",
 		"Comma separated list of cilium install helm --set overrides. "+
diff --git a/test/helpers/cilium.go b/test/helpers/cilium.go
index 96116f951c..055693cf76 100644
--- a/test/helpers/cilium.go
+++ b/test/helpers/cilium.go
@@ -140,31 +140,6 @@ func (s *SSHMeta) WaitEndpointsDeleted() bool {
 
 }
 
-// WaitDockerPluginReady waits up until timeout reached for Cilium docker plugin to be ready
-func (s *SSHMeta) WaitDockerPluginReady() bool {
-	logger := s.logger.WithFields(logrus.Fields{"functionName": "WaitDockerPluginReady"})
-
-	body := func() bool {
-		// check that docker plugin socket exists
-		cmd := `stat /run/docker/plugins/cilium.sock`
-		res := s.ExecWithSudo(cmd)
-		if !res.WasSuccessful() {
-			return false
-		}
-		// check that connect works
-		cmd = `nc -U -z /run/docker/plugins/cilium.sock`
-		res = s.ExecWithSudo(cmd)
-		return res.WasSuccessful()
-	}
-	err := WithTimeout(body, "Docker plugin is not ready after timeout", &TimeoutConfig{Timeout: HelperTimeout})
-	if err != nil {
-		logger.WithError(err).Warn("Docker plugin is not ready after timeout")
-		s.ExecWithSudo("ls -l /run/docker/plugins/cilium.sock") // This function is only for debugginag.
-		return false
-	}
-	return true
-}
-
 func (s *SSHMeta) MonitorDebug(on bool, epID string) bool {
 	logger := s.logger.WithFields(logrus.Fields{"functionName": "MonitorDebug"})
 	dbg := "Disabled"
@@ -433,16 +408,6 @@ func (s *SSHMeta) PolicyImportAndWait(path string, timeout time.Duration) (int,
 	return revision, err
 }
 
-// PolicyImport imports a new policy into Cilium.
-func (s *SSHMeta) PolicyImport(path string) error {
-	res := s.ExecCilium(fmt.Sprintf("policy import %s", path))
-	if !res.WasSuccessful() {
-		s.logger.Errorf("could not import policy: %s", res.CombineOutput())
-		return fmt.Errorf("could not import policy %s", path)
-	}
-	return nil
-}
-
 // PolicyRenderAndImport receives an string with a policy, renders it in the
 // test root directory and imports the policy to cilium. It returns the new
 // policy id.  Returns an error if the file cannot be created or if the policy
@@ -734,18 +699,6 @@ func (s *SSHMeta) RestartCilium() error {
 	return nil
 }
 
-// AddIPToLoopbackDevice adds the specified IP (assumed to be in form <ip>/<mask>)
-// to the loopback device on s.
-func (s *SSHMeta) AddIPToLoopbackDevice(ip string) *CmdRes {
-	return s.ExecWithSudo(fmt.Sprintf("ip addr add dev lo %s", ip))
-}
-
-// RemoveIPFromLoopbackDevice removes the specified IP (assumed to be in form <ip>/<mask>)
-// from the loopback device on s.
-func (s *SSHMeta) RemoveIPFromLoopbackDevice(ip string) *CmdRes {
-	return s.ExecWithSudo(fmt.Sprintf("ip addr del dev lo %s", ip))
-}
-
 // FlushGlobalConntrackTable flushes the global connection tracking table.
 func (s *SSHMeta) FlushGlobalConntrackTable() *CmdRes {
 	return s.ExecCilium("bpf ct flush global")
diff --git a/test/helpers/cmd.go b/test/helpers/cmd.go
index 8fcaa039ee..bebfd16e00 100644
--- a/test/helpers/cmd.go
+++ b/test/helpers/cmd.go
@@ -9,7 +9,6 @@ import (
 	"errors"
 	"fmt"
 	"reflect"
-	"regexp"
 	"strconv"
 	"strings"
 	"sync"
@@ -218,16 +217,6 @@ func (res *CmdRes) ExpectFail(optionalDescription ...any) bool {
 		CMDSuccess(), optionalDescription...)
 }
 
-// ExpectFailWithError asserts whether res failed to execute with the
-// error output containing the given data.  It accepts an optional
-// parameter that can be used to annotate failure messages.
-func (res *CmdRes) ExpectFailWithError(data string, optionalDescription ...any) bool {
-	return gomega.ExpectWithOffset(1, res).ShouldNot(
-		CMDSuccess(), optionalDescription...) &&
-		gomega.ExpectWithOffset(1, res.Stderr()).To(
-			gomega.ContainSubstring(data), optionalDescription...)
-}
-
 // ExpectSuccess asserts whether res executed successfully. It accepts an optional
 // parameter that can be used to annotate failure messages.
 func (res *CmdRes) ExpectSuccess(optionalDescription ...any) bool {
@@ -267,22 +256,6 @@ func (res *CmdRes) ExpectContainsFilterLine(filter, expected string, optionalDes
 		gomega.ContainElement(expected), optionalDescription...)
 }
 
-// ExpectDoesNotContain asserts that a string is not contained in the stdout of
-// the executed command. It accepts an optional parameter that can be used to
-// annotate failure messages.
-func (res *CmdRes) ExpectDoesNotContain(data string, optionalDescription ...any) bool {
-	return gomega.ExpectWithOffset(1, res.Stdout()).ToNot(
-		gomega.ContainSubstring(data), optionalDescription...)
-}
-
-// ExpectDoesNotMatchRegexp asserts that the stdout of the executed command
-// doesn't match the regexp. It accepts an optional parameter that can be used
-// to annotate failure messages.
-func (res *CmdRes) ExpectDoesNotMatchRegexp(regexp string, optionalDescription ...any) bool {
-	return gomega.ExpectWithOffset(1, res.Stdout()).ToNot(
-		gomega.MatchRegexp(regexp), optionalDescription...)
-}
-
 // ExpectDoesNotContainFilterLine applies the provided JSONPath filter to each
 // line of stdout of the executed command and asserts that the expected string
 // does not matches any of the lines.
@@ -420,14 +393,6 @@ func (res *CmdRes) OutputPrettyPrint() string {
 		format(res.Stderr()))
 }
 
-// ExpectEqual asserts whether cmdRes.Output().String() and expected are equal.
-// It accepts an optional parameter that can be used to annotate failure
-// messages.
-func (res *CmdRes) ExpectEqual(expected string, optionalDescription ...any) bool {
-	return gomega.ExpectWithOffset(1, res.Stdout()).Should(
-		gomega.Equal(expected), optionalDescription...)
-}
-
 // Reset resets res's stdout buffer to be empty.
 func (res *CmdRes) Reset() {
 	res.stdout.Reset()
@@ -470,20 +435,6 @@ func (res *CmdRes) WaitUntilMatchTimeout(substr string, timeout time.Duration) e
 		&TimeoutConfig{Timeout: timeout})
 }
 
-// WaitUntilMatchRegexp waits until the `CmdRes.stdout` matches the given regexp.
-// If the timeout is reached it will return an error.
-func (res *CmdRes) WaitUntilMatchRegexp(expr string, timeout time.Duration) error {
-	r := regexp.MustCompile(expr)
-	body := func() bool {
-		return r.Match(res.GetStdOut().Bytes())
-	}
-
-	return WithTimeout(
-		body,
-		fmt.Sprintf("The output doesn't match regexp %q after timeout", expr),
-		&TimeoutConfig{Timeout: timeout})
-}
-
 // WaitUntilMatchFilterLineTimeout applies the JSONPath 'filter' to each line of
 // `CmdRes.stdout` and waits until a line matches the 'expected' output.
 // If the 'timeout' is reached it will return an error.
diff --git a/test/helpers/cons.go b/test/helpers/cons.go
index 2a25720850..8a899e19d5 100644
--- a/test/helpers/cons.go
+++ b/test/helpers/cons.go
@@ -9,8 +9,6 @@ import (
 	"os"
 	"time"
 
-	k8sConst "github.com/cilium/cilium/pkg/k8s/apis/cilium.io"
-	"github.com/cilium/cilium/pkg/versioncheck"
 	ginkgoext "github.com/cilium/cilium/test/ginkgo-ext"
 	"github.com/cilium/cilium/test/helpers/logutils"
 )
@@ -61,9 +59,6 @@ const (
 	// CiliumAgentLabel is the label used for Cilium
 	CiliumAgentLabel = "k8s-app=cilium"
 
-	// CiliumOperatorLabel is the label used in the Cilium Operator deployment
-	CiliumOperatorLabel = "io.cilium/app=operator"
-
 	// HubbleRelayLabel is the label used for the Hubble Relay deployment
 	HubbleRelayLabel = "k8s-app=hubble-relay"
 
@@ -80,11 +75,6 @@ const (
 	// is imported.
 	PolicyEnforcementAlways = "always"
 
-	// PolicyEnforcementNever represents the PolicyEnforcement mode
-	// for Cilium in which traffic is always allowed even if there is a policy
-	// selecting endpoints.
-	PolicyEnforcementNever = "never"
-
 	// CiliumDockerNetwork is the name of the Docker network which Cilium manages.
 	CiliumDockerNetwork = "cilium-net"
 
@@ -124,20 +114,8 @@ const (
 	// functions instead of using basic strings.
 
 	OptionConntrackAccounting = "ConntrackAccounting"
-	OptionDebug               = "Debug"
-	OptionDropNotify          = "DropNotification"
-	OptionTraceNotify         = "TraceNotification"
-	OptionIngressPolicy       = "IngressPolicy"
-	OptionEgressPolicy        = "EgressPolicy"
-	OptionIngress             = "ingress"
-	OptionEgress              = "egress"
-	OptionNone                = "none"
-	OptionDisabled            = "Disabled"
 	OptionEnabled             = "Enabled"
 
-	StateTerminating = "Terminating"
-	StateRunning     = "Running"
-
 	PingCount   = 5
 	PingTimeout = 5
 
@@ -164,15 +142,10 @@ const (
 	CiliumBugtool          = "cilium-bugtool"
 	CiliumBugtoolArgs      = "--exclude-object-files"
 	CiliumDockerDaemonName = "cilium-docker"
-	AgentDaemon            = "cilium-agent"
 
-	KubectlCreate = ResourceLifeCycleAction("create")
 	KubectlDelete = ResourceLifeCycleAction("delete")
 	KubectlApply  = ResourceLifeCycleAction("apply")
 
-	KubectlPolicyNameLabel      = k8sConst.PolicyLabelName
-	KubectlPolicyNameSpaceLabel = k8sConst.PolicyLabelNamespace
-
 	MonitorLogFileName = "monitor.log"
 
 	// CiliumTestLog is the filename where the cilium logs that happens during
@@ -227,11 +200,8 @@ const (
 	podCIDRUnavailable         = " PodCIDR not available"                                                // cf. https://github.com/cilium/cilium/issues/29680
 	unableGetNode              = "Unable to get node resource"                                           // cf. https://github.com/cilium/cilium/issues/29710
 	objectHasBeenModified      = "the object has been modified; please apply your changes"               // cf. https://github.com/cilium/cilium/issues/29712
-	legacyBGPFeature           = "You are using the legacy BGP feature"                                  // Expected when testing the legacy BGP feature.
 	etcdTimeout                = "etcd client timeout exceeded"                                          // cf. https://github.com/cilium/cilium/issues/29714
 	endpointRestoreFailed      = "Unable to restore endpoint, ignoring"                                  // cf. https://github.com/cilium/cilium/issues/29716
-	unableRestoreRouterIP      = "Unable to restore router IP from filesystem"                           // cf. https://github.com/cilium/cilium/issues/29715
-	routerIPReallocated        = "Router IP could not be re-allocated"                                   // cf. https://github.com/cilium/cilium/issues/29715
 	cantFindIdentityInCache    = "unable to release identity: unable to find key in local cache"         // cf. https://github.com/cilium/cilium/issues/29732
 	keyAllocFailedFoundMaster  = "Found master key after proceeding with new allocation"                 // cf. https://github.com/cilium/cilium/issues/29738
 	cantRecreateMasterKey      = "unable to re-create missing master key"                                // cf. https://github.com/cilium/cilium/issues/29738
@@ -264,20 +234,6 @@ const (
 	ReservedIdentityHost = 1
 )
 
-var (
-	IsCiliumV1_8  = versioncheck.MustCompile(">=1.7.90 <1.9.0")
-	IsCiliumV1_9  = versioncheck.MustCompile(">=1.8.90 <1.10.0")
-	IsCiliumV1_10 = versioncheck.MustCompile(">=1.9.90 <1.11.0")
-	IsCiliumV1_11 = versioncheck.MustCompile(">=1.10.90 <1.12.0")
-	IsCiliumV1_12 = versioncheck.MustCompile(">=1.11.90 <1.13.0")
-	IsCiliumV1_13 = versioncheck.MustCompile(">=1.12.90 <1.14.0")
-	IsCiliumV1_14 = versioncheck.MustCompile(">=1.13.90 <1.15.0")
-	IsCiliumV1_15 = versioncheck.MustCompile(">=1.14.90 <1.16.0")
-	IsCiliumV1_16 = versioncheck.MustCompile(">=1.15.90 <1.17.0")
-	IsCiliumV1_17 = versioncheck.MustCompile(">=1.16.90 <1.18.0")
-	IsCiliumV1_18 = versioncheck.MustCompile(">=1.17.90 <1.19.0")
-)
-
 // badLogMessages is a map which key is a part of a log message which indicates
 // a failure if the message does not contain any part from value list.
 var badLogMessages = map[string][]string{
@@ -301,8 +257,7 @@ var badLogMessages = map[string][]string{
 		removeInexistentID, failedToListCRDs, retrieveResLock, failedToRelLockEmptyName,
 		failedToUpdateLock, failedToReleaseLock, errorCreatingInitialLeader},
 	logutils.WarningLogs: {cantEnableJIT, delMissingService, podCIDRUnavailable,
-		unableGetNode, objectHasBeenModified, legacyBGPFeature,
-		etcdTimeout, endpointRestoreFailed, unableRestoreRouterIP, routerIPReallocated,
+		unableGetNode, objectHasBeenModified, etcdTimeout, endpointRestoreFailed,
 		cantFindIdentityInCache, keyAllocFailedFoundMaster, cantRecreateMasterKey,
 		cantUpdateCRDIdentity, cantDeleteFromPolicyMap, failedToListCRDs, mutationDetector},
 }
@@ -349,11 +304,6 @@ func K8s1VMName() string {
 	return fmt.Sprintf("k8s1-%s", GetCurrentK8SEnv())
 }
 
-// K8s2VMName is the name of the Kubernetes worker node when running K8s tests.
-func K8s2VMName() string {
-	return fmt.Sprintf("k8s2-%s", GetCurrentK8SEnv())
-}
-
 // GetBadLogMessages returns a deep copy of badLogMessages to allow removing
 // messages for specific tests.
 func GetBadLogMessages() map[string][]string {
diff --git a/test/helpers/docker.go b/test/helpers/docker.go
index 380f3dc030..497f8744a6 100644
--- a/test/helpers/docker.go
+++ b/test/helpers/docker.go
@@ -21,19 +21,6 @@ func (s *SSHMeta) ContainerExec(name string, cmd string, optionalArgs ...string)
 	return s.Exec(dockerCmd)
 }
 
-// ContainerRun is a wrapper to a one execution docker run container. It runs
-// an instance of the specific Docker image with the provided network, name and
-// options.
-func (s *SSHMeta) ContainerRun(name, image, net, options string, cmdParams ...string) *CmdRes {
-	cmdOnStart := ""
-	if len(cmdParams) > 0 {
-		cmdOnStart = strings.Join(cmdParams, " ")
-	}
-	cmd := fmt.Sprintf(
-		"docker run --name %s --net %s %s %s %s", name, net, options, image, cmdOnStart)
-	return s.ExecWithSudo(cmd)
-}
-
 // ContainerCreate is a wrapper for `docker run`. It runs an instance of the
 // specified Docker image with the provided network, name, options and container
 // startup commands.
@@ -94,14 +81,6 @@ func (s *SSHMeta) containerInspectNet(name string, network string) (map[string]s
 	return result, nil
 }
 
-// ContainerInspectOtherNet returns a map of Docker networking information
-// fields and their associated values for the container of the provided name,
-// on the specified docker network. An error is returned if the networking
-// information could not be retrieved.
-func (s *SSHMeta) ContainerInspectOtherNet(name string, network string) (map[string]string, error) {
-	return s.containerInspectNet(name, network)
-}
-
 // ContainerInspectNet returns a map of Docker networking information fields and
 // their associated values for the container of the provided name. An error
 // is returned if the networking information could not be retrieved.
@@ -138,12 +117,6 @@ func (s *SSHMeta) NetworkCreate(name string, subnet string) *CmdRes {
 		"--driver cilium --ipam-driver cilium")
 }
 
-// NetworkDelete deletes the Docker network of the provided name. It is a wrapper
-// around `docker network rm`.
-func (s *SSHMeta) NetworkDelete(name string) *CmdRes {
-	return s.ExecWithSudo(fmt.Sprintf("docker network rm  %s", name))
-}
-
 // NetworkGet returns all of the Docker network configuration for the provided
 // network. It is a wrapper around `docker network inspect`.
 func (s *SSHMeta) NetworkGet(name string) *CmdRes {
diff --git a/test/helpers/kubectl.go b/test/helpers/kubectl.go
index 97a0a484d6..03243db5d8 100644
--- a/test/helpers/kubectl.go
+++ b/test/helpers/kubectl.go
@@ -74,8 +74,7 @@ const (
 	// CIIntegrationMicrok8s is the value to set CNI_INTEGRATION when running with minikube.
 	CIIntegrationMinikube = "minikube"
 
-	LogGathererSelector = "k8s-app=cilium-test-logs"
-	CiliumSelector      = "k8s-app=cilium"
+	CiliumSelector = "k8s-app=cilium"
 
 	IPv4NativeRoutingCIDR = "10.0.0.0/8"
 	IPv6NativeRoutingCIDR = "fd02::/112"
@@ -125,6 +124,8 @@ var (
 		"extraEnv[0].value":      "'true'",
 		"extraEnv[1].name":       "CILIUM_FEATURE_METRICS_WITH_DEFAULTS",
 		"extraEnv[1].value":      "'true'",
+		"extraEnv[2].name":       "CILIUM_INVALID_METRIC_VALUE_DETECTOR",
+		"extraEnv[2].value":      "'true'",
 
 		// We need CNP node status to know when a policy is being enforced
 		"ipv4NativeRoutingCIDR": IPv4NativeRoutingCIDR,
@@ -803,12 +804,6 @@ func (kub *Kubectl) CreateSecret(secretType, name, namespace, args string) *CmdR
 	return kub.ExecShort(fmt.Sprintf("kubectl create secret %s %s -n %s %s", secretType, name, namespace, args))
 }
 
-// CopyFileToPod copies a file to a pod's file-system.
-func (kub *Kubectl) CopyFileToPod(namespace string, pod string, fromFile, toFile string) *CmdRes {
-	kub.Logger().Debug(fmt.Sprintf("copyiong file %s to pod %s/%s:%s", fromFile, namespace, pod, toFile))
-	return kub.Exec(fmt.Sprintf("%s cp %s %s/%s:%s", KubectlCmd, fromFile, namespace, pod, toFile))
-}
-
 // ExecKafkaPodCmd executes shell command with arguments arg in the specified pod residing in the specified
 // namespace. It returns the stdout of the command that was executed.
 // The kafka producer and consumer scripts do not return error if command
@@ -836,14 +831,6 @@ func (kub *Kubectl) ExecPodCmd(namespace string, pod string, cmd string, options
 	return kub.Exec(command, options...)
 }
 
-// ExecPodContainerCmd executes command cmd in the specified container residing
-// in the specified namespace and pod. It returns a pointer to CmdRes with all
-// the output
-func (kub *Kubectl) ExecPodContainerCmd(namespace, pod, container, cmd string, options ...ExecOptions) *CmdRes {
-	command := fmt.Sprintf("%s exec -n %s %s -c %s -- %s", KubectlCmd, namespace, pod, container, cmd)
-	return kub.Exec(command, options...)
-}
-
 // ExecPodCmdContext synchronously executes command cmd in the specified pod residing in the
 // specified namespace. It returns a pointer to CmdRes with all the output.
 func (kub *Kubectl) ExecPodCmdContext(ctx context.Context, namespace string, pod string, cmd string, options ...ExecOptions) *CmdRes {
@@ -901,25 +888,6 @@ func (kub *Kubectl) GetCNP(namespace string, cnp string) *cnpv2.CiliumNetworkPol
 	return &result
 }
 
-func (kub *Kubectl) WaitForCRDCount(filter string, count int, timeout time.Duration) error {
-	// Set regexp flag m for multi-line matching, then add the
-	// matches for beginning and end of a line, so that we count
-	// at most one match per line (like "grep <filter> | wc -l")
-	regex := regexp.MustCompile("(?m:^.*(?:" + filter + ").*$)")
-	body := func() bool {
-		res := kub.ExecShort(fmt.Sprintf("%s get crds", KubectlCmd))
-		if !res.WasSuccessful() {
-			log.Error(res.GetErr("kubectl get crds failed"))
-			return false
-		}
-		return len(regex.FindAllString(res.Stdout(), -1)) == count
-	}
-	return WithTimeout(
-		body,
-		fmt.Sprintf("timed out waiting for %d CRDs matching filter \"%s\" to be ready", count, filter),
-		&TimeoutConfig{Timeout: timeout})
-}
-
 // GetPods gets all of the pods in the given namespace that match the provided
 // filter.
 func (kub *Kubectl) GetPods(namespace string, filter string) *CmdRes {
@@ -966,30 +934,6 @@ func (kub *Kubectl) GetPodOnNodeLabeledWithOffset(label string, podFilter string
 	return podName, podIP
 }
 
-// GetSvcIP returns the cluster IP for the given service. If the service
-// does not contain a cluster IP, the function keeps retrying until it has or
-// the context timesout.
-func (kub *Kubectl) GetSvcIP(ctx context.Context, namespace, name string) (string, error) {
-	for {
-		select {
-		case <-ctx.Done():
-			return "", ctx.Err()
-		default:
-		}
-		jsonFilter := `{.spec.clusterIP}`
-		res := kub.ExecContext(ctx, fmt.Sprintf("%s -n %s get svc %s -o jsonpath='%s'",
-			KubectlCmd, namespace, name, jsonFilter))
-		if !res.WasSuccessful() {
-			return "", fmt.Errorf("cannot retrieve pods: %s", res.CombineOutput())
-		}
-		clusterIP := res.CombineOutput().String()
-		if clusterIP != "" {
-			return clusterIP, nil
-		}
-		time.Sleep(time.Second)
-	}
-}
-
 // GetPodsIPs returns a map with pod name as a key and pod IP name as value. It
 // only gets pods in the given namespace that match the provided filter. It
 // returns an error if pods cannot be retrieved correctly
@@ -1210,18 +1154,6 @@ func (kub *Kubectl) GetServiceHostPort(namespace string, service string) (string
 	return data.Spec.ClusterIP, int(data.Spec.Ports[0].Port), nil
 }
 
-// GetServiceClusterIPs returns the list of cluster IPs associated with the service. The support
-// for this is only present in later version of Kubernetes(>= 1.20).
-func (kub *Kubectl) GetServiceClusterIPs(namespace string, service string) ([]string, error) {
-	var data v1.Service
-	err := kub.Get(namespace, "service "+service).Unmarshal(&data)
-	if err != nil {
-		return nil, err
-	}
-
-	return data.Spec.ClusterIPs, nil
-}
-
 // GetLoadBalancerIP waits until a loadbalancer IP addr has been assigned for
 // the given service, and then returns the IP addr.
 func (kub *Kubectl) GetLoadBalancerIP(namespace string, service string, timeout time.Duration) (string, error) {
@@ -1262,21 +1194,6 @@ func (kub *Kubectl) Logs(namespace string, pod string) *CmdRes {
 		fmt.Sprintf("%s -n %s logs %s", KubectlCmd, namespace, pod))
 }
 
-// LogsPreviousWithLabel returns a CmdRes with command output from the
-// execution of `kubectl logs --previous=true -l <label string> -n <namespace>`.
-func (kub *Kubectl) LogsPreviousWithLabel(namespace string, labelStr string) *CmdRes {
-	return kub.Exec(
-		fmt.Sprintf("%s -n %s -l %s logs --previous", KubectlCmd, namespace, labelStr))
-}
-
-// LogsStream returns a CmdRes with command output from the
-// execution of `kubectl logs -f <pod> -n <namespace>`.
-func (kub *Kubectl) LogsStream(namespace string, pod string, ctx context.Context) *CmdRes {
-	logCmd := fmt.Sprintf("%s -n %s logs -f %s", KubectlCmd, namespace, pod)
-
-	return kub.ExecInBackground(ctx, logCmd, ExecOptions{})
-}
-
 // MonitorStart runs cilium-dbg monitor in the background and returns the command
 // result, CmdRes, along with a cancel function. The cancel function is used to
 // stop the monitor.
@@ -1389,12 +1306,6 @@ func (kub *Kubectl) DeleteAllInNamespace(name string) error {
 	return nil
 }
 
-// NamespaceLabel sets a label in a Kubernetes namespace
-func (kub *Kubectl) NamespaceLabel(namespace string, label string) *CmdRes {
-	ginkgoext.By("Setting label %s in namespace %s", label, namespace)
-	return kub.ExecShort(fmt.Sprintf("%s label --overwrite namespace %s %s", KubectlCmd, namespace, label))
-}
-
 // WaitforPods waits up until timeout seconds have elapsed for all pods in the
 // specified namespace that match the provided JSONPath filter to have their
 // containterStatuses equal to "ready". Returns true if all pods achieve
@@ -1667,13 +1578,6 @@ func (kub *Kubectl) Create(filePath string) *CmdRes {
 		fmt.Sprintf("%s create -f  %s", KubectlCmd, filePath))
 }
 
-// CreateResource is a wrapper around `kubernetes create <resource>
-// <resourceName>.
-func (kub *Kubectl) CreateResource(resource, resourceName string) *CmdRes {
-	kub.Logger().Debug(fmt.Sprintf("creating resource %s with name %s", resource, resourceName))
-	return kub.ExecShort(fmt.Sprintf("kubectl create %s %s", resource, resourceName))
-}
-
 // DeleteResource is a wrapper around `kubernetes delete <resource>
 // resourceName>.
 func (kub *Kubectl) DeleteResource(resource, resourceName string) *CmdRes {
@@ -2183,19 +2087,6 @@ func (kub *Kubectl) ScaleUpDNS() *CmdRes {
 	return res
 }
 
-// SetCiliumOperatorReplicas sets the number of replicas for the cilium-operator.
-func (kub *Kubectl) SetCiliumOperatorReplicas(nReplicas int) *CmdRes {
-	res := kub.ExecShort(fmt.Sprintf("%s get deploy -n %s -l %s -o jsonpath='{.items[*].metadata.name}'", KubectlCmd, CiliumNamespace, operatorLabel))
-	if !res.WasSuccessful() {
-		return res
-	}
-
-	// kubectl -n kube-system patch deploy cilium-operator --patch '{"spec": { "replicas":1}}'
-	name := res.Stdout()
-	spec := fmt.Sprintf("{\"spec\": { \"replicas\":%d}}", nReplicas)
-	return kub.ExecShort(fmt.Sprintf("%s patch deploy -n %s %s --patch '%s'", KubectlCmd, CiliumNamespace, name, spec))
-}
-
 // redeployDNS deletes the kube-dns pods and does not wait for the deletion
 // to complete.
 func (kub *Kubectl) redeployDNS() *CmdRes {
@@ -2386,72 +2277,6 @@ func (kub *Kubectl) WaitTerminatingPodsInNsWithFilter(ns, filter string, timeout
 	return nil
 }
 
-// DeployPatchStdIn deploys the original kubernetes descriptor with the given patch.
-func (kub *Kubectl) DeployPatchStdIn(original, patch string) error {
-	// debugYaml only dumps the full created yaml file to the test output if
-	// the cilium manifest can not be created correctly.
-	debugYaml := func(original, patch string) {
-		_ = kub.ExecShort(fmt.Sprintf(
-			`%s patch --filename='%s' --patch %s --local --dry-run -o yaml`,
-			KubectlCmd, original, patch))
-	}
-
-	// validation 1st
-	res := kub.ExecShort(fmt.Sprintf(
-		`%s patch --filename='%s' --patch %s --local --dry-run`,
-		KubectlCmd, original, patch))
-	if !res.WasSuccessful() {
-		debugYaml(original, patch)
-		return res.GetErr("Cilium patch validation failed")
-	}
-
-	res = kub.Apply(ApplyOptions{
-		FilePath: "-",
-		Force:    true,
-		Piped: fmt.Sprintf(
-			`%s patch --filename='%s' --patch %s --local -o yaml`,
-			KubectlCmd, original, patch),
-	})
-	if !res.WasSuccessful() {
-		debugYaml(original, patch)
-		return res.GetErr("Cilium manifest patch installation failed")
-	}
-	return nil
-}
-
-// DeployPatch deploys the original kubernetes descriptor with the given patch.
-func (kub *Kubectl) DeployPatch(original, patchFileName string) error {
-	// debugYaml only dumps the full created yaml file to the test output if
-	// the cilium manifest can not be created correctly.
-	debugYaml := func(original, patch string) {
-		_ = kub.ExecShort(fmt.Sprintf(
-			`%s patch --filename='%s' --patch "$(cat '%s')" --local -o yaml`,
-			KubectlCmd, original, patch))
-	}
-
-	// validation 1st
-	res := kub.ExecShort(fmt.Sprintf(
-		`%s patch --filename='%s' --patch "$(cat '%s')" --local --dry-run`,
-		KubectlCmd, original, patchFileName))
-	if !res.WasSuccessful() {
-		debugYaml(original, patchFileName)
-		return res.GetErr("Cilium patch validation failed")
-	}
-
-	res = kub.Apply(ApplyOptions{
-		FilePath: "-",
-		Force:    true,
-		Piped: fmt.Sprintf(
-			`%s patch --filename='%s' --patch "$(cat '%s')" --local -o yaml`,
-			KubectlCmd, original, patchFileName),
-	})
-	if !res.WasSuccessful() {
-		debugYaml(original, patchFileName)
-		return res.GetErr("Cilium manifest patch installation failed")
-	}
-	return nil
-}
-
 // Patch patches the given object with the given patch (string).
 func (kub *Kubectl) Patch(namespace, objType, objName, patch string) *CmdRes {
 	ginkgoext.By("Patching %s %s in namespace %s", objType, objName, namespace)
@@ -2459,13 +2284,6 @@ func (kub *Kubectl) Patch(namespace, objType, objName, patch string) *CmdRes {
 		KubectlCmd, namespace, objType, objName, patch))
 }
 
-// JsonPatch patches the given object with the given patch in JSON format.
-func (kub *Kubectl) JsonPatch(namespace, objType, objName, patch string) *CmdRes {
-	ginkgoext.By("Patching %s %s in namespace %s", objType, objName, namespace)
-	return kub.ExecShort(fmt.Sprintf("%s -n %s patch %s %s --type=json --patch %q",
-		KubectlCmd, namespace, objType, objName, patch))
-}
-
 func addIfNotOverwritten(options map[string]string, field, value string) map[string]string {
 	if _, ok := options[field]; !ok {
 		options[field] = value
@@ -2592,10 +2410,6 @@ func (kub *Kubectl) overwriteHelmOptions(options map[string]string) error {
 		options["ciliumEndpointSlice.enabled"] = "true"
 	}
 
-	if !SupportIPv6Connectivity() {
-		options["ipv6.enabled"] = "false"
-	}
-
 	maps.Copy(options, cliOverrideOptions)
 
 	return nil
@@ -2634,20 +2448,6 @@ func (kub *Kubectl) GetPrivateIface(label string) (string, error) {
 	return kub.getIfaceByIPAddr(label, ipAddr)
 }
 
-// GetPublicIface returns an interface name of a netdev which has ExternalIP
-// addr.
-// Assumes that all nodes have identical interfaces.
-func (kub *Kubectl) GetPublicIface(label string) (string, error) {
-	ipAddr, err := kub.GetNodeIPByLabel(label, true)
-	if err != nil {
-		return "", err
-	} else if ipAddr == "" {
-		return "", fmt.Errorf("%s does not have ExternalIP", label)
-	}
-
-	return kub.getIfaceByIPAddr(label, ipAddr)
-}
-
 func (kub *Kubectl) waitToDelete(name, label string) error {
 	var (
 		pods []string
@@ -2696,14 +2496,6 @@ func (kub *Kubectl) GetDefaultIface(ipv6 bool) (string, error) {
 	return strings.Trim(iface, "\n"), nil
 }
 
-func (kub *Kubectl) DeleteCiliumDS() error {
-	// Do not assert on success in AfterEach intentionally to avoid
-	// incomplete teardown.
-	ginkgoext.By("DeleteCiliumDS(namespace=%q)", CiliumNamespace)
-	_ = kub.DeleteResource("ds", fmt.Sprintf("-n %s cilium", CiliumNamespace))
-	return kub.waitToDelete("Cilium", CiliumAgentLabel)
-}
-
 func (kub *Kubectl) DeleteHubbleRelay(ns string) error {
 	ginkgoext.By("DeleteHubbleRelay(namespace=%q)", ns)
 	_ = kub.DeleteResource("deployment", fmt.Sprintf("-n %s hubble-relay", ns))
@@ -2738,28 +2530,6 @@ func (kub *Kubectl) CiliumInstall(filename string, options map[string]string) er
 	return nil
 }
 
-// RunHelm runs the helm command with the given options.
-func (kub *Kubectl) RunHelm(action, repo, helmName, version, namespace string, options map[string]string) (*CmdRes, error) {
-	err := kub.overwriteHelmOptions(options)
-	if err != nil {
-		return nil, err
-	}
-	optionsString := ""
-
-	for k, v := range options {
-		if v == "true" || v == "false" {
-			optionsString += fmt.Sprintf(" --set %s=%s ", k, v)
-		} else {
-			optionsString += fmt.Sprintf(" --set '%s=%s' ", k, v)
-		}
-	}
-
-	return kub.ExecMiddle(fmt.Sprintf("helm %s %s %s "+
-		"--version=%s "+
-		"--namespace=%s "+
-		"%s", action, helmName, repo, version, namespace, optionsString)), nil
-}
-
 // GetCiliumPods returns a list of all Cilium pods in the specified namespace,
 // and an error if the Cilium pods were not able to be retrieved.
 func (kub *Kubectl) GetCiliumPods() ([]string, error) {
@@ -2778,16 +2548,6 @@ func (kub *Kubectl) CiliumEndpointsList(ctx context.Context, pod string) *CmdRes
 	return kub.CiliumExecContext(ctx, pod, "cilium-dbg endpoint list -o json")
 }
 
-// CiliumEndpointIPv6 returns the IPv6 address of each endpoint which matches
-// the given endpoint selector.
-func (kub *Kubectl) CiliumEndpointIPv6(pod string, endpoint string) map[string]string {
-	filter := `{range [*]}{@.status.external-identifiers.pod-name}{"="}{@.status.networking.addressing[*].ipv6}{"\n"}{end}`
-	ctx, cancel := context.WithTimeout(context.Background(), ShortCommandTimeout)
-	defer cancel()
-	return kub.CiliumExecContext(ctx, pod, fmt.Sprintf(
-		"cilium-dbg endpoint get %s -o jsonpath='%s'", endpoint, filter)).KVOutput()
-}
-
 // CiliumEndpointWaitReady waits until all endpoints managed by all Cilium pod
 // are ready. Returns an error if the Cilium pods cannot be retrieved via
 // Kubernetes, or endpoints are not ready after a specified timeout
@@ -2905,24 +2665,6 @@ func (kub *Kubectl) CiliumEndpointWaitReady() error {
 	return NewSSHMetaError(err.Error(), callback)
 }
 
-// WaitForCEPIdentity waits for a particular CEP to have an identity present.
-func (kub *Kubectl) WaitForCEPIdentity(ns, podName string) error {
-	body := func(ctx context.Context) (bool, error) {
-		ep, err := kub.GetCiliumEndpoint(ns, podName)
-		if err != nil || ep == nil {
-			return false, nil
-		}
-		if ep.Identity == nil {
-			return false, nil
-		}
-		return ep.Identity.ID != 0, nil
-	}
-
-	ctx, cancel := context.WithTimeout(context.Background(), HelperTimeout)
-	defer cancel()
-	return WithContext(ctx, body, 1*time.Second)
-}
-
 // CiliumExecContext runs cmd in the specified Cilium pod with the given context.
 func (kub *Kubectl) CiliumExecContext(ctx context.Context, pod string, cmd string) *CmdRes {
 	limitTimes := 5
@@ -2987,43 +2729,6 @@ func (kub *Kubectl) CiliumExecMustSucceedOnAll(ctx context.Context, cmd string,
 	}
 }
 
-// ExecUntilMatch executes the specified command repeatedly for the
-// specified pod until the given substring is present in stdout.
-// If the timeout is reached it will return an error.
-func (kub *Kubectl) ExecUntilMatch(namespace, pod, cmd, substr string) (*CmdRes, error) {
-	ctx, cancel := context.WithTimeout(context.Background(), ShortCommandTimeout)
-	defer cancel()
-	var res *CmdRes
-	for {
-		select {
-		case <-ctx.Done():
-			return res, fmt.Errorf("timeout waiting for %q to be present in stdout", substr)
-		default:
-			res = kub.ExecPodCmd(namespace, pod, cmd)
-			if strings.Contains(res.Stdout(), substr) {
-				return res, nil
-			}
-		}
-	}
-}
-
-// CiliumExecUntilMatch executes the specified command repeatedly for the
-// specified Cilium pod until the given substring is present in stdout.
-// If the timeout is reached it will return an error.
-func (kub *Kubectl) CiliumExecUntilMatch(pod, cmd, substr string) error {
-	body := func() bool {
-		ctx, cancel := context.WithTimeout(context.Background(), ShortCommandTimeout)
-		defer cancel()
-		res := kub.CiliumExecContext(ctx, pod, cmd)
-		return strings.Contains(res.Stdout(), substr)
-	}
-
-	return WithTimeout(
-		body,
-		fmt.Sprintf("%s is not in the output after timeout", substr),
-		&TimeoutConfig{Timeout: HelperTimeout})
-}
-
 // CiliumNodesWait waits until all nodes in the Kubernetes cluster are annotated
 // with Cilium annotations. Its runtime is bounded by a maximum of `HelperTimeout`.
 // When a node is annotated with said annotations, it indicates
@@ -3078,32 +2783,6 @@ func (kub *Kubectl) LoadedPolicyInFirstAgent() (string, error) {
 	return "", fmt.Errorf("no running cilium pods")
 }
 
-// WaitPolicyDeleted waits for policy policyName to be deleted from the
-// cilium-agent running in pod. Returns an error if policyName was unable to
-// be deleted after some amount of time.
-func (kub *Kubectl) WaitPolicyDeleted(pod string, policyName string) error {
-	body := func() bool {
-		ctx, cancel := context.WithTimeout(context.Background(), ShortCommandTimeout)
-		defer cancel()
-		res := kub.CiliumExecContext(ctx, pod, fmt.Sprintf("cilium-dbg policy get %s", policyName))
-
-		// `cilium-dbg policy get <policy name>` fails if the policy is not loaded,
-		// which is the condition we want.
-		return !res.WasSuccessful()
-	}
-
-	return WithTimeout(body, fmt.Sprintf("Policy %s was not deleted in time", policyName), &TimeoutConfig{Timeout: HelperTimeout})
-}
-
-// CiliumIsPolicyLoaded returns true if the policy is loaded in the given
-// cilium Pod. it returns false in case that the policy is not in place
-func (kub *Kubectl) CiliumIsPolicyLoaded(pod string, policyCmd string) bool {
-	ctx, cancel := context.WithTimeout(context.Background(), ShortCommandTimeout)
-	defer cancel()
-	res := kub.CiliumExecContext(ctx, pod, fmt.Sprintf("cilium-dbg policy get %s", policyCmd))
-	return res.WasSuccessful()
-}
-
 // CiliumPolicyRevision returns the policy revision in the specified Cilium pod.
 // Returns an error if the policy revision cannot be retrieved.
 func (kub *Kubectl) CiliumPolicyRevision(pod string) (int, error) {
@@ -3223,35 +2902,6 @@ func (kub *Kubectl) CiliumClusterwidePolicyAction(filepath string, action Resour
 	return "", kub.waitNextPolicyRevisions(podRevisions, timeout)
 }
 
-// OutsideNodeReport collects command output on the outside node.
-func (kub *Kubectl) OutsideNodeReport(outsideNode string, commands ...string) {
-	if config.CiliumTestConfig.SkipLogGathering {
-		ginkgoext.GinkgoPrint("Skipped gathering logs (-cilium.skipLogs=true)\n")
-		return
-	}
-
-	if kub == nil {
-		ginkgoext.GinkgoPrint("Skipped gathering logs due to kubectl not being initialized")
-		return
-	}
-
-	// Log gathering should take at most 10 minutes.
-	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Minute)
-	defer cancel()
-
-	results := make([]*CmdRes, 0, len(commands))
-	ginkgoext.GinkgoPrint("Fetching command output on outside node %s", outsideNode)
-	for _, cmd := range commands {
-		res := kub.ExecInHostNetNS(ctx, outsideNode, cmd)
-		results = append(results, res)
-	}
-
-	for _, res := range results {
-		res.WaitUntilFinish()
-		ginkgoext.GinkgoPrint(res.GetDebugMessage())
-	}
-}
-
 // CiliumReport report the cilium pod to the log and appends the logs for the
 // given commands.
 func (kub *Kubectl) CiliumReport(commands ...string) {
@@ -3550,23 +3200,6 @@ func (kub *Kubectl) ExecInHostNetNS(ctx context.Context, node, cmd string) *CmdR
 	return kub.ExecInFirstPod(ctx, LogGathererNamespace, selector, cmd)
 }
 
-// ExecInHostNetNSInBackground runs given command in a pod running in a host network namespace
-// but in background.
-func (kub *Kubectl) ExecInHostNetNSInBackground(ctx context.Context, node, cmd string) (*CmdRes, func(), error) {
-	selector := fmt.Sprintf("%s --field-selector spec.nodeName=%s",
-		logGathererSelector(true), node)
-	names, err := kub.GetPodNamesContext(ctx, LogGathererNamespace, selector)
-	if err != nil {
-		return nil, nil, err
-	}
-	pod := names[0]
-
-	bgCmd := fmt.Sprintf("%s exec -n %s %s -- %s", KubectlCmd, LogGathererNamespace, pod, cmd)
-	ctx, cancel := context.WithCancel(context.Background())
-
-	return kub.ExecInBackground(ctx, bgCmd, ExecOptions{}), cancel, nil
-}
-
 // ExecInHostNetNSByLabel runs given command in a pod running in a host network namespace.
 // The pod's node is identified by the given label.
 func (kub *Kubectl) ExecInHostNetNSByLabel(ctx context.Context, label, cmd string) (string, error) {
@@ -4187,11 +3820,6 @@ func (kub *Kubectl) reportMapHost(ctx context.Context, path string, reportCmds m
 	wg.Wait()
 }
 
-// HelmAddCiliumRepo installs the repository that contain Cilium helm charts.
-func (kub *Kubectl) HelmAddCiliumRepo() *CmdRes {
-	return kub.ExecMiddle("helm repo add cilium https://helm.cilium.io")
-}
-
 // HelmTemplate renders given helm template. TODO: use go helm library for that
 // We use --validate with `helm template` to properly populate the built-in objects like
 // .Capabilities.KubeVersion with the values from associated cluster.
@@ -4262,44 +3890,6 @@ func (kub *Kubectl) WaitForIPCacheEntry(node, ipAddr string) error {
 		&TimeoutConfig{Timeout: HelperTimeout})
 }
 
-func (kub *Kubectl) WaitForEgressPolicyEntries(node string, expectedCount int) error {
-	ciliumPod, err := kub.GetCiliumPodOnNode(node)
-	if err != nil {
-		return err
-	}
-
-	body := func() bool {
-		ctx, cancel := context.WithTimeout(context.Background(), ShortCommandTimeout)
-		defer cancel()
-		cmd := "cilium-dbg bpf egress list | tail -n +2 | wc -l"
-		out := kub.CiliumExecContext(ctx, ciliumPod, cmd)
-		if !out.WasSuccessful() {
-			kub.Logger().
-				WithFields(logrus.Fields{"cmd": cmd}).
-				WithError(out.GetError()).
-				Warning("Failed to list bpf egress policy map")
-
-			return false
-		}
-
-		count, err := strconv.Atoi(strings.TrimSpace(out.Stdout()))
-		if err != nil {
-			kub.Logger().
-				WithFields(logrus.Fields{"cmd": cmd}).
-				WithError(err).
-				Warning("Failed to parse command output")
-
-			return false
-		}
-
-		return count == expectedCount
-	}
-
-	return WithTimeout(body,
-		fmt.Sprintf("could not ensure egress policy entries count is equal to %d", expectedCount),
-		&TimeoutConfig{Timeout: HelperTimeout})
-}
-
 // RepeatCommandInBackground runs command on repeat in goroutine until quit channel
 // is closed and closes run channel when command is first run
 func (kub *Kubectl) RepeatCommandInBackground(cmd string) (quit, run chan struct{}) {
@@ -4716,26 +4306,6 @@ func (kub *Kubectl) WaitForServiceFrontend(nodeName, ipAddr string) error {
 		&TimeoutConfig{Timeout: HelperTimeout})
 }
 
-// WaitForServiceBackend waits until the service backend with the given ipAddr
-// appears in "cilium-dbg bpf lb list --backends" on the given node.
-func (kub *Kubectl) WaitForServiceBackend(node, ipAddr string) error {
-	ciliumPod, err := kub.GetCiliumPodOnNode(node)
-	if err != nil {
-		return err
-	}
-
-	body := func() bool {
-		ctx, cancel := context.WithTimeout(context.Background(), ShortCommandTimeout)
-		defer cancel()
-		cmd := fmt.Sprintf(`cilium-dbg bpf lb list --backends | grep -q %s`, ipAddr)
-		return kub.CiliumExecContext(ctx, ciliumPod, cmd).WasSuccessful()
-	}
-
-	return WithTimeout(body,
-		fmt.Sprintf("backend entry for %s was not found in time", ipAddr),
-		&TimeoutConfig{Timeout: HelperTimeout})
-}
-
 func (kub *Kubectl) AddVXLAN(nodeName, remote, dev, addr string, vxlanId int) *CmdRes {
 	cmd := fmt.Sprintf("ip link add vxlan%d type vxlan id %d remote %s dstport 4789 dev %s",
 		vxlanId, vxlanId, remote, dev)
@@ -4776,16 +4346,12 @@ func compareServicePortToFrontEnd(sP *v1.ServicePort, fA *models.FrontendAddress
 }
 
 func serviceAddressKey(ip, port, proto, scope string) string {
-	newOutputStyle := HasNewServiceOutput(GetRunningCiliumVersion())
 	k := net.JoinHostPort(ip, port)
-	if newOutputStyle {
-		p := strings.ToLower(proto)
-		if p == "" || p == "none" || p == "any" {
-			proto = "ANY"
-		}
-		return fmt.Sprintf("%s/%s%s", k, proto, scope)
+	p := strings.ToLower(proto)
+	if p == "" || p == "none" || p == "any" {
+		proto = "ANY"
 	}
-	return fmt.Sprintf("%s%s", k, scope)
+	return fmt.Sprintf("%s/%s%s", k, proto, scope)
 }
 
 func (kub *Kubectl) CollectFeatures() {
diff --git a/test/helpers/ssh_command.go b/test/helpers/ssh_command.go
index 958d35d911..d3f5fb4c6f 100644
--- a/test/helpers/ssh_command.go
+++ b/test/helpers/ssh_command.go
@@ -174,20 +174,6 @@ func runCommand(session *ssh.Session, cmd *SSHCommand) (bool, error) {
 	return true, nil
 }
 
-// RunCommand runs a SSHCommand using SSHClient client. The returned error is
-// nil if the command runs, has no problems copying stdin, stdout, and stderr,
-// and exits with a zero exit status.
-func (client *SSHClient) RunCommand(cmd *SSHCommand) error {
-	session, err := client.newSession()
-	if err != nil {
-		return err
-	}
-	defer session.Close()
-
-	_, err = runCommand(session, cmd)
-	return err
-}
-
 // RunCommandInBackground runs an SSH command in a similar way to
 // RunCommandContext, but with a context which allows the command to be
 // cancelled at any time. When cancel is called the error of the command is
diff --git a/test/helpers/utils.go b/test/helpers/utils.go
index fc3cb4d618..620104c6bd 100644
--- a/test/helpers/utils.go
+++ b/test/helpers/utils.go
@@ -13,60 +13,22 @@ import (
 	"os"
 	"path/filepath"
 	"slices"
-	"strconv"
 	"strings"
 	"time"
 
-	"github.com/blang/semver/v4"
 	"github.com/onsi/ginkgo"
 	. "github.com/onsi/gomega"
 	"golang.org/x/sys/unix"
 
-	"github.com/cilium/cilium/pkg/versioncheck"
 	"github.com/cilium/cilium/test/config"
 	ginkgoext "github.com/cilium/cilium/test/ginkgo-ext"
 )
 
-var runningCiliumVersion string
-
-// GetRunningCiliumVersion gets the currently running cilium version.
-func GetRunningCiliumVersion() string {
-	return runningCiliumVersion
-}
-
-// HasNewServiceOutput checks to see if the current running cilium
-// version uses the old style service output (e.g. "0.0.0.0:53") vs
-// the new style (e.g. "0.0.0.0:53/TCP").
-func HasNewServiceOutput(ver string) bool {
-	cst, err := versioncheck.Version(ver)
-	// If the version is not parseable it is probably
-	// someone's custom build  or not set.
-	// Either way, it is probably using the new output
-	// format.
-	if err != nil {
-		return true
-	}
-	return versioncheck.MustCompile(">=1.17.0")(cst)
-}
-
 // Sleep sleeps for the specified duration in seconds
 func Sleep(delay time.Duration) {
 	time.Sleep(delay * time.Second)
 }
 
-// CountValues returns the count of the occurrences of key in data, as well as
-// the length of data.
-func CountValues(key string, data []string) (int, int) {
-	var result int
-
-	for _, x := range data {
-		if x == key {
-			result++
-		}
-	}
-	return result, len(data)
-}
-
 // MakeUID returns a randomly generated string.
 func MakeUID() string {
 	return fmt.Sprintf("%08x", rand.Uint32())
@@ -384,80 +346,6 @@ func WriteOrAppendToFile(filename string, data []byte, perm os.FileMode) error {
 	return err
 }
 
-// DNSDeployment returns the manifest to install dns engine on the server.
-func DNSDeployment(base string) string {
-	var DNSEngine = "coredns"
-	k8sVersion := GetCurrentK8SEnv()
-	switch k8sVersion {
-	case "1.8", "1.9", "1.10":
-		DNSEngine = "kubedns"
-	}
-
-	if integration := GetCurrentIntegration(); integration != "" {
-		fullPath := filepath.Join("provision", "manifest", k8sVersion, integration, DNSEngine+"_deployment.yaml")
-		_, err := os.Stat(fullPath)
-		if err == nil {
-			return filepath.Join(base, fullPath)
-		}
-	}
-
-	fullPath := filepath.Join("provision", "manifest", k8sVersion, DNSEngine+"_deployment.yaml")
-	_, err := os.Stat(fullPath)
-	if err == nil {
-		return filepath.Join(base, fullPath)
-	}
-	return filepath.Join(base, "provision", "manifest", DNSEngine+"_deployment.yaml")
-}
-
-// getK8sSupportedConstraints returns the Kubernetes versions supported by
-// a specific Cilium version.
-func getK8sSupportedConstraints(ciliumVersion string) (semver.Range, error) {
-	cst, err := versioncheck.Version(ciliumVersion)
-	if err != nil {
-		return nil, err
-	}
-	switch {
-	case IsCiliumV1_18(cst):
-		return versioncheck.MustCompile(">=1.16.0 <1.34.0"), nil
-	case IsCiliumV1_17(cst):
-		return versioncheck.MustCompile(">=1.16.0 <1.33.0"), nil
-	case IsCiliumV1_16(cst):
-		return versioncheck.MustCompile(">=1.16.0 <1.31.0"), nil
-	case IsCiliumV1_15(cst):
-		return versioncheck.MustCompile(">=1.16.0 <1.30.0"), nil
-	case IsCiliumV1_14(cst):
-		return versioncheck.MustCompile(">=1.16.0 <1.28.0"), nil
-	case IsCiliumV1_13(cst):
-		return versioncheck.MustCompile(">=1.16.0 <1.27.0"), nil
-	case IsCiliumV1_12(cst):
-		return versioncheck.MustCompile(">=1.16.0 <1.25.0"), nil
-	case IsCiliumV1_11(cst):
-		return versioncheck.MustCompile(">=1.16.0 <1.24.0"), nil
-	case IsCiliumV1_10(cst):
-		return versioncheck.MustCompile(">=1.16.0 <1.22.0"), nil
-	case IsCiliumV1_9(cst):
-		return versioncheck.MustCompile(">=1.12.0 <1.20.0"), nil
-	case IsCiliumV1_8(cst):
-		return versioncheck.MustCompile(">=1.10.0 <1.19.0"), nil
-	default:
-		return nil, fmt.Errorf("unrecognized version '%s'", ciliumVersion)
-	}
-}
-
-// CanRunK8sVersion returns true if the givel ciliumVersion can run in the given
-// Kubernetes version. If any version is unparsable, an error is returned.
-func CanRunK8sVersion(ciliumVersion, k8sVersionStr string) (bool, error) {
-	k8sVersion, err := versioncheck.Version(k8sVersionStr)
-	if err != nil {
-		return false, err
-	}
-	constraint, err := getK8sSupportedConstraints(ciliumVersion)
-	if err != nil {
-		return false, err
-	}
-	return constraint(k8sVersion), nil
-}
-
 // failIfContainsBadLogMsg makes a test case to fail if any message from
 // given log messages contains an entry from the blacklist (map key) AND
 // does not contain ignore messages (map value).
@@ -552,16 +440,6 @@ func DoesNotRunOnAKS() bool {
 	return !RunsOnAKS()
 }
 
-// RunsOnEKS returns true if the tests are running on EKS.
-func RunsOnEKS() bool {
-	return GetCurrentIntegration() == CIIntegrationEKS
-}
-
-// DoesNotRunOnEKS is the complement function of DoesNotRunOnEKS.
-func DoesNotRunOnEKS() bool {
-	return !RunsOnEKS()
-}
-
 // RunsWithKubeProxyReplacement returns true if the kernel supports our
 // kube-proxy replacement. Note that kube-proxy may still be running
 // alongside Cilium.
@@ -575,18 +453,6 @@ func DoesNotRunWithKubeProxyReplacement() bool {
 	return !RunsWithKubeProxyReplacement()
 }
 
-// DoesNotHaveHosts returns a function which returns true if a CI job
-// has less VMs than the given count.
-func DoesNotHaveHosts(count int) func() bool {
-	return func() bool {
-		if c, err := strconv.Atoi(os.Getenv("K8S_NODES")); err != nil {
-			return true
-		} else {
-			return c < count
-		}
-	}
-}
-
 // RunsWithHostFirewall returns true is Cilium runs with the host firewall enabled.
 func RunsWithHostFirewall() bool {
 	return os.Getenv("HOST_FIREWALL") != "0" && os.Getenv("HOST_FIREWALL") != ""
@@ -597,11 +463,6 @@ func RunsWithKubeProxy() bool {
 	return os.Getenv("KUBEPROXY") != "0"
 }
 
-// RunsWithoutKubeProxy is the complement function of RunsWithKubeProxy.
-func RunsWithoutKubeProxy() bool {
-	return !RunsWithKubeProxy()
-}
-
 // ExistNodeWithoutCilium returns true if there is a node in a cluster which does
 // not run Cilium.
 func ExistNodeWithoutCilium() bool {
@@ -678,11 +539,6 @@ func IsNodeWithoutCilium(node string) bool {
 	return slices.Contains(GetNodesWithoutCilium(), node)
 }
 
-// SkipQuarantined returns whether test under quarantine should be skipped
-func SkipQuarantined() bool {
-	return !config.CiliumTestConfig.RunQuarantined
-}
-
 // SkipRaceDetectorEnabled returns whether tests failing with race detector
 // enabled should be skipped.
 func SkipRaceDetectorEnabled() bool {
@@ -690,99 +546,33 @@ func SkipRaceDetectorEnabled() bool {
 	return race == "1" || race == "true"
 }
 
-// SkipK8sVersions returns true if the current K8s versions matched the
-// constraints passed in argument.
-func SkipK8sVersions(k8sVersions string) bool {
-	k8sVersion, err := versioncheck.Version(GetCurrentK8SEnv())
-	if err != nil {
-		return false
-	}
-	constraint := versioncheck.MustCompile(k8sVersions)
-	return constraint(k8sVersion)
-}
-
 // DualStackSupported returns whether the current environment has DualStack IPv6
 // enabled or not for the cluster.
 func DualStackSupported() bool {
-	supportedVersions := versioncheck.MustCompile(">=1.18.0")
-	kubeProxyOnlySupportedVersions := versioncheck.MustCompile(">=1.20.0")
-
-	k8sVersion, err := versioncheck.Version(GetCurrentK8SEnv())
-	if err != nil {
-		// If we cannot conclude the k8s version we assume that dual stack is not
-		// supported.
-		return false
-	}
-
-	// When running with kube-proxy only, some IPv6 family services are not
-	// provisioned in ip6tables on k8s < 1.20. Therefore, skip any DualStack
-	// tests on those versions/configurations.
-	if DoesNotRunWithKubeProxyReplacement() && !kubeProxyOnlySupportedVersions(k8sVersion) {
-		return false
-	}
-
 	// AKS does not support dual stack yet
 	if IsIntegration(CIIntegrationAKS) {
 		return false
 	}
 
 	// We only have DualStack enabled in KIND.
-	return (GetCurrentIntegration() == "" || IsIntegration(CIIntegrationKind)) &&
-		supportedVersions(k8sVersion)
+	return GetCurrentIntegration() == "" || IsIntegration(CIIntegrationKind)
 }
 
 // DualStackSupportBeta returns true if the environment has a Kubernetes version that
 // has support for k8s DualStack beta API types.
 func DualStackSupportBeta() bool {
-	// DualStack support was promoted to beta with API types finalized in k8s 1.21
-	// The API types for dualstack services are same in k8s 1.20 and 1.21 so we include
-	// K8s version 1.20 as well.
-	// https://github.com/kubernetes/kubernetes/pull/98969
-	supportedVersions := versioncheck.MustCompile(">=1.20.0")
-	k8sVersion, err := versioncheck.Version(GetCurrentK8SEnv())
-	if err != nil {
-		return false
-	}
-
 	// AKS does not support dual stack yet
 	if IsIntegration(CIIntegrationAKS) {
 		return false
 	}
 
-	return (GetCurrentIntegration() == "" || IsIntegration(CIIntegrationKind)) &&
-		supportedVersions(k8sVersion)
+	return GetCurrentIntegration() == "" || IsIntegration(CIIntegrationKind)
 }
 
 // CiliumEndpointSliceFeatureEnabled returns true only if the environment has a kubernetes version
 // greater than or equal to 1.21.
 func CiliumEndpointSliceFeatureEnabled() bool {
-	k8sVersionGreaterEqual121 := versioncheck.MustCompile(">=1.21.0")
-	k8sVersion, err := versioncheck.Version(GetCurrentK8SEnv())
-	if err != nil {
-		return false
-	}
-	return k8sVersionGreaterEqual121(k8sVersion) && (GetCurrentIntegration() == "" ||
-		IsIntegration(CIIntegrationKind))
-}
-
-// SupportIPv6Connectivity returns true if the CI environment supports IPv6
-// connectivity across pods.
-func SupportIPv6Connectivity() bool {
-	supportedVersions := versioncheck.MustCompile(">=1.20.0")
-	k8sVersion, err := versioncheck.Version(GetCurrentK8SEnv())
-	if err != nil {
-		return false
-	}
-
-	if supportedVersions(k8sVersion) {
-		return true
-	}
-
-	if IsIntegration(CIIntegrationKind) {
-		return false
-	}
-
-	return true
+	return GetCurrentIntegration() == "" || IsIntegration(CIIntegrationKind)
 }
 
 // SupportIPv6ToOutside returns true if the CI environment supports IPv6
diff --git a/test/helpers/wrappers.go b/test/helpers/wrappers.go
index 62460f3450..7f784aff10 100644
--- a/test/helpers/wrappers.go
+++ b/test/helpers/wrappers.go
@@ -8,7 +8,6 @@ import (
 	"fmt"
 	"strconv"
 	"strings"
-	"time"
 )
 
 // PerfTest represents a type of test to run when running `netperf`.
@@ -105,20 +104,6 @@ func CurlWithRetries(endpoint string, retries int, fail bool, optionalValues ...
 		retries, endpoint)
 }
 
-// CurlTimeout does the same as CurlFail() except you can define the timeout.
-// See note about optionalValues on CurlFail().
-func CurlTimeout(endpoint string, timeout time.Duration, optionalValues ...any) string {
-	statsInfo := `time-> DNS: '%{time_namelookup}(%{remote_ip})', Connect: '%{time_connect}',` +
-		`Transfer '%{time_starttransfer}', total '%{time_total}'`
-
-	if len(optionalValues) > 0 {
-		endpoint = fmt.Sprintf(endpoint, optionalValues...)
-	}
-	return fmt.Sprintf(
-		`curl -k --path-as-is -s -D /dev/stderr --fail --connect-timeout %d --max-time %d %s -w "%s"`,
-		timeout, timeout, endpoint, statsInfo)
-}
-
 // SuperNetperf returns the string representing the super_netperf command to use when
 // testing connectivity between endpoints.
 func SuperNetperf(sessions int, endpoint string, perfTest PerfTest, options string) string {
diff --git a/tools/mount/main.go b/tools/mount/main.go
index ca1addce8b..30a23efd12 100644
--- a/tools/mount/main.go
+++ b/tools/mount/main.go
@@ -8,6 +8,7 @@ import (
 	"os"
 
 	"github.com/cilium/cilium/pkg/cgroups"
+	"github.com/cilium/cilium/pkg/logging"
 )
 
 func main() {
@@ -20,5 +21,5 @@ func main() {
 	// This program is executed by an init container so we purposely don't
 	// exit with any error codes. In case of errors, the function will log warnings,
 	// but we don't block cilium agent pod from running.
-	cgroups.CheckOrMountCgrpFS(cgroupMountPoint)
+	cgroups.CheckOrMountCgrpFS(logging.DefaultSlogLogger, cgroupMountPoint)
 }
