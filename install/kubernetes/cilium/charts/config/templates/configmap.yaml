{{- /*  Default values with backwards compatibility */ -}}
{{- $defaultEnableCnpStatusUpdates := "true" -}}
{{- $defaultBpfMapDynamicSizeRatio := 0.0 -}}
{{- $defaultBpfMasquerade := "false" -}}
{{- $defaultBpfClockProbe := "false" -}}
{{- $defaultBpfTProxy := "false" -}}
{{- $defaultIPAM := "hostscope" -}}
{{- $defaultSessionAffinity := "false" -}}
{{- $defaultOperatorApiServeAddr := "localhost:9234" -}}
{{- $defaultBpfCtTcpMax := 524288 -}}
{{- $defaultBpfCtAnyMax := 262144 -}}
{{- $enableIdentityMark := "true" -}}
{{- $fragmentTracking := "true" -}}
{{- $crdWaitTimeout := "5m" -}}

{{- /* Default values when 1.8 was initially deployed */ -}}
{{- if semverCompare ">=1.8" (default "1.8" .Values.upgradeCompatibility) -}}
{{- $defaultEnableCnpStatusUpdates = "false" -}}
{{- $defaultBpfMapDynamicSizeRatio = 0.0025 -}}
{{- $defaultBpfMasquerade = "true" -}}
{{- $defaultBpfClockProbe = "true" -}}
{{- $defaultIPAM = "cluster-pool" -}}
{{- $defaultSessionAffinity = "true" -}}
{{- if .Values.global.ipv4.enabled }}
{{- $defaultOperatorApiServeAddr = "127.0.0.1:9234" -}}
{{- else -}}
{{- $defaultOperatorApiServeAddr = "[::1]:9234" -}}
{{- end }}
{{- $defaultBpfCtTcpMax = 0 -}}
{{- $defaultBpfCtAnyMax = 0 -}}
{{- end -}}

{{- /* Default values when 1.9 was initially deployed */ -}}
{{- if semverCompare ">=1.9" (default "1.9" .Values.upgradeCompatibility) -}}
{{- $defaultBpfTProxy = "true" -}}
{{- end -}}

{{- $ipam := (coalesce .Values.ipam $defaultIPAM) -}}
{{- $bpfCtTcpMax := (coalesce .Values.global.bpf.ctTcpMax $defaultBpfCtTcpMax) -}}
{{- $bpfCtAnyMax := (coalesce .Values.global.bpf.ctAnyMax $defaultBpfCtAnyMax) -}}

apiVersion: v1
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: {{ .Release.Namespace }}
data:
{{- if .Values.global.etcd.enabled }}
  # The kvstore configuration is used to enable use of a kvstore for state
  # storage. This can either be provided with an external kvstore or with the
  # help of cilium-etcd-operator which operates an etcd cluster automatically.
  kvstore: etcd
{{- if .Values.global.etcd.k8sService }}
  kvstore-opt: '{"etcd.config": "/var/lib/etcd-config/etcd.config", "etcd.operator": "true"}'
{{- else }}
  kvstore-opt: '{"etcd.config": "/var/lib/etcd-config/etcd.config"}'
{{- end }}

  # This etcd-config contains the etcd endpoints of your cluster. If you use
  # TLS please make sure you follow the tutorial in https://cilium.link/etcd-config
  etcd-config: |-
    ---
    endpoints:
{{- if .Values.global.etcd.managed }}
      - https://cilium-etcd-client.{{ .Release.Namespace }}.svc:2379
{{- else }}
{{- range .Values.global.etcd.endpoints }}
      - {{ . }}
{{- end }}
{{- end }}
{{- if or .Values.global.etcd.ssl .Values.global.etcd.managed }}
    trusted-ca-file: '/var/lib/etcd-secrets/etcd-client-ca.crt'
    key-file: '/var/lib/etcd-secrets/etcd-client.key'
    cert-file: '/var/lib/etcd-secrets/etcd-client.crt'
{{- end }}
{{- end }}

{{- if hasKey .Values "disableEnvoyVersionCheck" }}
  disable-envoy-version-check: {{ .Values.disableEnvoyVersionCheck | quote }}
{{- end }}

  # Identity allocation mode selects how identities are shared between cilium
  # nodes by setting how they are stored. The options are "crd" or "kvstore".
  # - "crd" stores identities in kubernetes as CRDs (custom resource definition).
  #   These can be queried with:
  #     kubectl get ciliumid
  # - "kvstore" stores identities in a kvstore, etcd or consul, that is
  #   configured below. Cilium versions before 1.6 supported only the kvstore
  #   backend. Upgrades from these older cilium versions should continue using
  #   the kvstore by commenting out the identity-allocation-mode below, or
  #   setting it to "kvstore".
{{- if .Values.global.etcd.enabled }}
  identity-allocation-mode: kvstore
{{- else }}
  identity-allocation-mode: {{ .Values.global.identityAllocationMode }}
{{- end }}
{{- if .Values.global.identityHeartbeatTimeout }}
  identity-heartbeat-timeout: "{{ .Values.global.identityHeartbeatTimeout }}"
{{- end }}
{{- if .Values.global.identityGCInterval }}
  identity-gc-interval: "{{ .Values.global.identityGCInterval }}"
{{- end }}
{{- if .Values.global.endpointGCInterval }}
  cilium-endpoint-gc-interval: "{{ .Values.global.endpointGCInterval }}"
{{- end }}

{{- if .Values.identityChangeGracePeriod }}
  # identity-change-grace-period is the grace period that needs to pass
  # before an endpoint that has changed its identity will start using
  # that new identity. During the grace period, the new identity has
  # already been allocated and other nodes in the cluster have a chance
  # to whitelist the new upcoming identity of the endpoint.
  identity-change-grace-period: {{ default "5s" .Values.identityChangeGracePeriod | quote }}
{{- end }}

{{- if hasKey .Values "labels" }}
  # To include or exclude matched resources from cilium identity evaluation
  labels: {{ .Values.labels | quote }}
{{- end }}

  # If you want to run cilium in debug mode change this value to true
  debug: {{ .Values.global.debug.enabled | quote }}

{{- if .Values.global.debug.verbose }}
  debug-verbose: "{{ .Values.global.debug.verbose }}"
{{- end }}

{{- if ne (int .Values.global.agent.healthPort) 9876 }}
  # Set the TCP port for the agent health status API. This is not the port used
  # for cilium-health.
  agent-health-port: "{{ .Values.global.agent.healthPort }}"
{{- end }}

{{- if .Values.global.prometheus.enabled }}
  # If you want metrics enabled in all of your Cilium agents, set the port for
  # which the Cilium agents will have their metrics exposed.
  # This option deprecates the "prometheus-serve-addr" in the
  # "cilium-metrics-config" ConfigMap
  # NOTE that this will open the port on ALL nodes where Cilium pods are
  # scheduled.
  prometheus-serve-addr: ":{{ .Values.global.prometheus.port }}"
  # Port to expose Envoy metrics (e.g. "9095"). Envoy metrics listener will be disabled if this
  # field is not set.
  proxy-prometheus-port: "{{ .Values.global.proxy.prometheus.port }}"
{{- end }}

{{- if .Values.global.operatorPrometheus.enabled }}
  # If you want metrics enabled in cilium-operator, set the port for
  # which the Cilium Operator will have their metrics exposed.
  # NOTE that this will open the port on the nodes where Cilium operator pod
  # is scheduled.
  operator-prometheus-serve-addr: ":{{ .Values.global.operatorPrometheus.port }}"
  enable-metrics: "true"
{{- end }}

  # Enable IPv4 addressing. If enabled, all endpoints are allocated an IPv4
  # address.
{{- if .Values.global.ipv4 }}
  enable-ipv4: {{ .Values.global.ipv4.enabled | quote }}
{{- end }}

  # Enable IPv6 addressing. If enabled, all endpoints are allocated an IPv6
  # address.
{{- if .Values.global.ipv6 }}
  enable-ipv6: {{ .Values.global.ipv6.enabled | quote }}
{{- end }}

{{- if .Values.global.cleanState }}
  # If a serious issue occurs during Cilium startup, this
  # invasive option may be set to true to remove all persistent
  # state. Endpoints will not be restored using knowledge from a
  # prior Cilium run, so they may receive new IP addresses upon
  # restart. This also triggers clean-cilium-bpf-state.
  clean-cilium-state: "true"
{{- end }}

{{- if .Values.global.cleanBpfState }}
  # If you want to clean cilium BPF state, set this to true;
  # Removes all BPF maps from the filesystem. Upon restart,
  # endpoints are restored with the same IP addresses, however
  # any ongoing connections may be disrupted briefly.
  # Loadbalancing decisions will be reset, so any ongoing
  # connections via a service may be loadbalanced to a different
  # backend after restart.
  clean-cilium-bpf-state: "true"
{{- end }}

{{- if .Values.global.cni.customConf }}
  # Users who wish to specify their own custom CNI configuration file must set
  # custom-cni-conf to "true", otherwise Cilium may overwrite the configuration.
  custom-cni-conf: "{{ .Values.global.cni.customConf }}"
{{- end }}

{{- if hasKey .Values "bpfClockProbe" }}
  enable-bpf-clock-probe: {{ .Values.bpfClockProbe | quote }}
{{- else if eq $defaultBpfClockProbe "true" }}
  enable-bpf-clock-probe: {{ $defaultBpfClockProbe | quote }}
{{- end }}

{{- if hasKey .Values "bpfTProxy" }}
  enable-bpf-tproxy: {{ .Values.bpfTProxy | quote }}
{{- else if eq $defaultBpfTProxy "true" }}
  enable-bpf-tproxy: {{ $defaultBpfTProxy | quote }}
{{- end }}

  # If you want cilium monitor to aggregate tracing for packets, set this level
  # to "low", "medium", or "maximum". The higher the level, the less packets
  # that will be seen in monitor output.
  monitor-aggregation: {{ .Values.global.bpf.monitorAggregation }}

  # The monitor aggregation interval governs the typical time between monitor
  # notification events for each allowed connection.
  #
  # Only effective when monitor aggregation is set to "medium" or higher.
  monitor-aggregation-interval: {{ .Values.global.bpf.monitorInterval }}

  # The monitor aggregation flags determine which TCP flags which, upon the
  # first observation, cause monitor notifications to be generated.
  #
  # Only effective when monitor aggregation is set to "medium" or higher.
  monitor-aggregation-flags: {{ .Values.global.bpf.monitorFlags }}

{{- if or $bpfCtTcpMax $bpfCtAnyMax }}
  # bpf-ct-global-*-max specifies the maximum number of connections
  # supported across all endpoints, split by protocol: tcp or other. One pair
  # of maps uses these values for IPv4 connections, and another pair of maps
  # use these values for IPv6 connections.
  #
  # If these values are modified, then during the next Cilium startup the
  # tracking of ongoing connections may be disrupted. This may lead to brief
  # policy drops or a change in loadbalancing decisions for a connection.
  #
  # For users upgrading from Cilium 1.2 or earlier, to minimize disruption
  # during the upgrade process, set bpf-ct-global-tcp-max to 1000000.
{{- if $bpfCtTcpMax }}
  bpf-ct-global-tcp-max: {{ $bpfCtTcpMax | quote }}
{{- end }}
{{- if $bpfCtAnyMax }}
  bpf-ct-global-any-max: {{ $bpfCtAnyMax | quote }}
{{- end }}
{{- end }}

{{- if .Values.global.bpf.natMax }}
  # bpf-nat-global-max specified the maximum number of entries in the
  # BPF NAT table.
  bpf-nat-global-max: "{{ .Values.global.bpf.natMax }}"
{{- end }}

{{- if .Values.global.bpf.neighMax }}
  # bpf-neigh-global-max specified the maximum number of entries in the
  # BPF neighbor table.
  bpf-neigh-global-max: "{{ .Values.global.bpf.neighMax }}"
{{- end }}

{{- if .Values.global.bpf.policyMapMax }}
  # bpf-policy-map-max specifies the maximum number of entries in endpoint
  # policy map (per endpoint)
  bpf-policy-map-max: "{{ .Values.global.bpf.policyMapMax }}"
{{- end }}

{{- if .Values.global.bpf.lbMapMax }}
  # bpf-lb-map-max specifies the maximum number of entries in bpf lb service,
  # backend and affinity maps.
  bpf-lb-map-max: "{{ .Values.global.bpf.lbMapMax }}"
{{- end }}


{{- if hasKey .Values "bpfMapDynamicSizeRatio" }}
  bpf-map-dynamic-size-ratio: {{ .Values.bpfMapDynamicSizeRatio | quote }}
{{- else if ne $defaultBpfMapDynamicSizeRatio 0.0 }}
  # Specifies the ratio (0.0-1.0) of total system memory to use for dynamic
  # sizing of the TCP CT, non-TCP CT, NAT and policy BPF maps.
  bpf-map-dynamic-size-ratio: {{ $defaultBpfMapDynamicSizeRatio | quote }}
{{- end }}

  # Pre-allocation of map entries allows per-packet latency to be reduced, at
  # the expense of up-front memory allocation for the entries in the maps. The
  # default value below will minimize memory usage in the default installation;
  # users who are sensitive to latency may consider setting this to "true".
  #
  # This option was introduced in Cilium 1.4. Cilium 1.3 and earlier ignore
  # this option and behave as though it is set to "true".
  #
  # If this value is modified, then during the next Cilium startup the restore
  # of existing endpoints and tracking of ongoing connections may be disrupted.
  # This may lead to policy drops or a change in loadbalancing decisions for a
  # connection for some time. Endpoints may need to be recreated to restore
  # connectivity.
  #
  # If this option is set to "false" during an upgrade from 1.3 or earlier to
  # 1.4 or later, then it may cause one-time disruptions during the upgrade.
  preallocate-bpf-maps: "{{ .Values.global.bpf.preallocateMaps }}"

  # Regular expression matching compatible Istio sidecar istio-proxy
  # container image names
  sidecar-istio-proxy-image: "{{ .Values.global.proxy.sidecarImageRegex }}"

  # Encapsulation mode for communication between nodes
  # Possible values:
  #   - disabled
  #   - vxlan (default)
  #   - geneve
  tunnel: {{ .Values.global.tunnel }}

  # Name of the cluster. Only relevant when building a mesh of clusters.
  cluster-name: {{ .Values.global.cluster.name }}

{{- if .Values.global.cluster.id }}
  # Unique ID of the cluster. Must be unique across all conneted clusters and
  # in the range of 1 and 255. Only relevant when building a mesh of clusters.
  cluster-id: "{{ .Values.global.cluster.id }}"
{{- end }}

{{- if .Values.global.gke.enabled }}

  ipam: "kubernetes"
  tunnel: "disabled"
  enable-endpoint-routes: "true"
  blacklist-conflicting-routes: "false"
  enable-local-node-route: "false"
{{- end }}

{{- if .Values.global.eni }}
  enable-endpoint-routes: "true"
  auto-create-cilium-node-resource: "true"
  blacklist-conflicting-routes: "false"
{{- end }}

{{- if .Values.global.azure.enabled }}
  enable-endpoint-routes: "true"
  auto-create-cilium-node-resource: "true"
  blacklist-conflicting-routes: "false"
  enable-local-node-route: "false"
{{- end }}

{{- if .Values.global.flannel.enabled }}
  # Interface to be used when running Cilium on top of a CNI plugin.
  # For flannel, use "cni0"
  flannel-master-device: {{ .Values.global.flannel.masterDevice }}

  # When running Cilium with policy enforcement enabled on top of a CNI plugin
  # the BPF programs will be installed on the network interface specified in
  # 'flannel-master-device' and on all network interfaces belonging to
  # a container. When the Cilium DaemonSet is removed, the BPF programs will
  # be kept in the interfaces unless this option is set to "true".
  flannel-uninstall-on-exit: "{{ .Values.global.flannel.uninstallOnExit}}"

{{- end }}

{{- if .Values.global.l7Proxy }}

  # Enables L7 proxy for L7 policy enforcement and visibility
  enable-l7-proxy: {{ .Values.global.l7Proxy.enabled | quote }}
{{- end }}

  # wait-bpf-mount makes init container wait until bpf filesystem is mounted
  wait-bpf-mount: "{{ .Values.global.bpf.waitForMount }}"

{{- if ne .Values.global.cni.chainingMode "none" }}
  # Enable chaining with another CNI plugin
  #
  # Supported modes:
  #  - none
  #  - aws-cni
  #  - flannel
  #  - portmap (Enables HostPort support for Cilium)
  cni-chaining-mode: {{ .Values.global.cni.chainingMode }}

{{- if hasKey .Values "enableIdentityMark"}}
  enable-identity-mark: {{ .Values.enableIdentityMark | quote }}
{{- else if (ne $enableIdentityMark "true") }}
  enable-identity-mark: "false"
{{- end }}

{{- if ne .Values.global.cni.chainingMode "portmap" }}
  # Disable the PodCIDR route to the cilium_host interface as it is not
  # required. While chaining, it is the responsibility of the underlying plugin
  # to enable routing.
  enable-local-node-route: "false"
{{- end }}
{{- end }}

  masquerade: {{ .Values.global.masquerade | quote }}
{{- if hasKey .Values "bpfMasquerade" }}
  enable-bpf-masquerade: {{ .Values.bpfMasquerade | quote }}
{{- else if eq $defaultBpfMasquerade "true" }}
  enable-bpf-masquerade: {{ $defaultBpfMasquerade | quote }}
{{- end }}
{{- if .Values.global.egressMasqueradeInterfaces }}
  egress-masquerade-interfaces: {{ .Values.global.egressMasqueradeInterfaces }}
{{- end }}
{{- if and .Values.global.ipMasqAgent .Values.global.ipMasqAgent.enabled }}
  enable-ip-masq-agent: "true"
{{- end }}

{{- if .Values.global.encryption.enabled }}
  enable-ipsec: {{ .Values.global.encryption.enabled | quote }}
  ipsec-key-file: {{ .Values.global.encryption.mountPath }}/{{ .Values.global.encryption.keyFile }}
{{- if .Values.global.encryption.interface }}
  encrypt-interface: {{ .Values.global.encryption.interface }}
{{- end }}
{{- if .Values.global.encryption.nodeEncryption }}
  encrypt-node: {{ .Values.global.encryption.nodeEncryption | quote }}
{{- end }}
{{- end }}
{{- if .Values.global.datapathMode }}
{{- if eq .Values.global.datapathMode "ipvlan" }}
  datapath-mode: ipvlan
  ipvlan-master-device: {{ .Values.global.ipvlan.masterDevice }}
{{- end }}
{{- end }}
  enable-xt-socket-fallback: {{ .Values.global.enableXTSocketFallback | quote }}
  install-iptables-rules: {{ .Values.global.installIptablesRules | quote }}
{{- if .Values.global.iptablesLockTimeout }}
  iptables-lock-timeout: {{ .Values.global.iptablesLockTimeout | quote }}
{{- end }}
  auto-direct-node-routes: {{ .Values.global.autoDirectNodeRoutes | quote }}
  enable-bandwidth-manager: {{ .Values.global.bandwidthManager | quote }}
{{- if .Values.global.nativeRoutingCIDR }}
  native-routing-cidr: {{ .Values.global.nativeRoutingCIDR }}
{{- end }}

{{- if hasKey .Values "fragmentTracking" }}
  enable-ipv4-fragment-tracking: {{ .Values.fragmentTracking | quote }}
{{- else if (ne $fragmentTracking "true") }}
  enable-ipv4-fragment-tracking: "false"
{{- end }}

{{- if .Values.global.hostFirewall }}
  enable-host-firewall: {{ .Values.global.hostFirewall | quote }}
{{- end}}

{{- if .Values.global.devices }}
  # List of devices used to attach bpf_host.o (implements BPF NodePort,
  # host-firewall and BPF masquerading)
  devices: {{ join " " .Values.global.devices | quote }}
{{- end }}

{{- if .Values.global.kubeProxyReplacement }}
  kube-proxy-replacement:  {{ .Values.global.kubeProxyReplacement | quote }}
{{- end }}
{{- if .Values.global.hostServices }}
{{- if .Values.global.hostServices.enabled }}
  enable-host-reachable-services: {{ .Values.global.hostServices.enabled | quote }}
{{- end }}
{{- if ne .Values.global.hostServices.protocols "tcp,udp" }}
  host-reachable-services-protos: {{ .Values.global.hostServices.protocols }}
{{- end }}
{{- end }}
{{- if .Values.global.hostPort }}
{{- if eq .Values.global.kubeProxyReplacement "partial" }}
  enable-host-port: {{ .Values.global.hostPort.enabled | quote }}
{{- end }}
{{- end }}
{{- if .Values.global.externalIPs }}
{{- if eq .Values.global.kubeProxyReplacement "partial" }}
  enable-external-ips: {{ .Values.global.externalIPs.enabled | quote }}
{{- end }}
{{- end }}
{{- if .Values.global.nodePort }}
{{- if eq .Values.global.kubeProxyReplacement "partial" }}
  enable-node-port: {{ .Values.global.nodePort.enabled | quote }}
{{- end }}
{{- if .Values.global.nodePort.range }}
  node-port-range: {{ .Values.global.nodePort.range | quote }}
{{- end }}
{{- if .Values.global.nodePort.directRoutingDevice }}
  direct-routing-device: {{ .Values.global.nodePort.directRoutingDevice | quote }}
{{- end }}
{{- if .Values.global.nodePort.mode }}
  node-port-mode: {{ .Values.global.nodePort.mode | quote }}
{{- end }}
{{- if .Values.global.nodePort.enableHealthCheck }}
  enable-health-check-nodeport: {{ .Values.global.nodePort.enableHealthCheck | quote}}
{{- end }}
{{- if .Values.global.nodePort.acceleration }}
  node-port-acceleration: {{ .Values.global.nodePort.acceleration | quote }}
{{- end }}
  node-port-bind-protection: {{ .Values.global.nodePort.bindProtection | quote }}
  enable-auto-protect-node-port-range: {{ .Values.global.nodePort.autoProtectPortRange | quote }}
{{- end }}
{{- if hasKey .Values "sessionAffinity" }}
  enable-session-affinity: {{ .Values.sessionAffinity | quote }}
{{- else if eq $defaultSessionAffinity "true" }}
  enable-session-affinity: {{ $defaultSessionAffinity | quote }}
{{- end }}
{{- if hasKey .Values "svcSourceRangeCheck" }}
  enable-svc-source-range-check: {{ .Values.svcSourceRangeCheck | quote }}
{{- end }}

{{- if and .Values.global.pprof .Values.global.pprof.enabled }}
  pprof: {{ .Values.global.pprof.enabled | quote }}
{{- end }}
{{- if .Values.global.logSystemLoad }}
  log-system-load: {{ .Values.global.logSystemLoad | quote }}
{{- end }}
{{- if .Values.global.logOptions }}
  log-opt: {{ toYaml .Values.global.logOptions | nindent 4 }}
{{- end }}
{{- if and .Values.global.sockops .Values.global.sockops.enabled }}
  sockops-enable: {{ .Values.global.sockops.enabled | quote }}
{{- end }}
{{- if and .Values.global.k8s .Values.global.k8s.requireIPv4PodCIDR }}
  k8s-require-ipv4-pod-cidr: {{ .Values.global.k8s.requireIPv4PodCIDR | quote }}
{{- else }}
{{- if eq $ipam "cluster-pool" }}
  k8s-require-ipv4-pod-cidr: {{ .Values.global.ipv4.enabled | quote}}
{{- end }}
{{- end }}
{{- if eq $ipam "cluster-pool" }}
  k8s-require-ipv6-pod-cidr: {{ .Values.global.ipv6.enabled | quote}}
{{- end }}
{{- if and .Values.global.endpointRoutes .Values.global.endpointRoutes.enabled }}
  enable-endpoint-routes: {{ .Values.global.endpointRoutes.enabled | quote }}
{{- end }}
{{- if .Values.global.cni.configMap }}
  read-cni-conf: {{ .Values.global.cni.confFileMountPath }}/{{ .Values.global.cni.configMapKey }}
  write-cni-conf-when-ready: {{ .Values.global.cni.hostConfDirMountPath }}/05-cilium.conflist
{{- else if .Values.global.cni.readCniConf }}
  read-cni-conf: {{ .Values.global.cni.readCniConf }}
{{- end }}
{{- if .Values.global.kubeConfigPath }}
  k8s-kubeconfig-path: {{ .Values.global.kubeConfigPath | quote }}
{{- end }}
{{- if and ( .Values.global.endpointHealthChecking.enabled ) (or (eq .Values.global.cni.chainingMode "portmap") (eq .Values.global.cni.chainingMode "none")) }}
  enable-endpoint-health-checking: "true"
{{- else}}
  # Disable health checking, when chaining mode is not set to portmap or none
  enable-endpoint-health-checking: "false"
{{- end }}
{{- if hasKey .Values "healthChecking" }}
  enable-health-checking: {{ .Values.healthChecking | quote }}
{{- end }}
{{- if or .Values.global.wellKnownIdentities.enabled .Values.global.etcd.managed }}
  enable-well-known-identities: "true"
{{- else }}
  enable-well-known-identities: "false"
{{- end }}
  enable-remote-node-identity: {{ .Values.global.remoteNodeIdentity | quote }}

{{- if hasKey .Values "synchronizeK8sNodes" }}
  synchronize-k8s-nodes: {{ .Values.synchronizeK8sNodes | quote }}
{{- end }}
{{- if .Values.policyAuditMode }}
  policy-audit-mode: {{ .Values.policyAuditMode | quote }}
{{- end }}

{{- if ne $defaultOperatorApiServeAddr "localhost:9234" }}
  operator-api-serve-addr: {{ $defaultOperatorApiServeAddr | quote }}
{{- end }}

{{- if .Values.global.hubble.enabled }}
  # Enable Hubble gRPC service.
  enable-hubble: {{ .Values.global.hubble.enabled  | quote }}
  # UNIX domain socket for Hubble server to listen to.
  hubble-socket-path:  {{ .Values.global.hubble.socketPath | quote }}
{{- if .Values.global.hubble.eventQueueSize }}
  # Buffer size of the channel for Hubble to receive monitor events. If this field is not set,
  # the buffer size is set to the default monitor queue size.
  hubble-event-queue-size: {{ .Values.global.hubble.eventQueueSize | quote }}
{{- end }}
{{- if .Values.global.hubble.flowBufferSize }}
  # Size of the buffer to store recent flows.
  hubble-flow-buffer-size: {{ .Values.global.hubble.flowBufferSize | quote }}
{{- end }}
{{- if .Values.global.hubble.metrics.enabled }}
  # Address to expose Hubble metrics (e.g. ":7070"). Metrics server will be disabled if this
  # field is not set.
  hubble-metrics-server: ":{{ .Values.global.hubble.metrics.port }}"
  # A space separated list of metrics to enable. See [0] for available metrics.
  #
  # https://github.com/cilium/hubble/blob/master/Documentation/metrics.md
  hubble-metrics: {{- range .Values.global.hubble.metrics.enabled }}
    {{.}}
{{- end }}
{{- end }}
{{- if and (.Values.global.hubble.listenAddress) }}
  # An additional address for Hubble server to listen to (e.g. ":4244").
  hubble-listen-address: {{ .Values.global.hubble.listenAddress | quote }}
{{- if .Values.global.hubble.tls.enabled }}
  hubble-disable-tls: "false"
  hubble-tls-cert-file: /var/lib/cilium/tls/hubble/server.crt
  hubble-tls-key-file: /var/lib/cilium/tls/hubble/server.key
  hubble-tls-client-ca-files: /var/lib/cilium/tls/hubble/client-ca.crt
{{- else }}
  hubble-disable-tls: "true"
{{- end}}
{{- end }}
{{- end }}
{{- if .Values.disableIptablesFeederRules }}
  # A space separated list of iptables chains to disable when installing feeder rules.
  disable-iptables-feeder-rules: {{ .Values.disableIptablesFeederRules | join " " | quote }}
{{- end }}
{{- if ne $ipam "hostscope" }}
  ipam: {{ $ipam | quote }}
{{- end }}
{{- if eq $ipam "cluster-pool" }}
{{- if .Values.global.ipv4.enabled }}
  cluster-pool-ipv4-cidr: {{ .Values.global.ipam.operator.clusterPoolIPv4PodCIDR | quote }}
  cluster-pool-ipv4-mask-size: {{ .Values.global.ipam.operator.clusterPoolIPv4MaskSize | quote  }}
{{- end }}
{{- if .Values.global.ipv6.enabled }}
  cluster-pool-ipv6-cidr: {{ .Values.global.ipam.operator.clusterPoolIPv6PodCIDR | quote }}
  cluster-pool-ipv6-mask-size: {{ .Values.global.ipam.operator.clusterPoolIPv6MaskSize | quote }}
{{- end }}
{{- end }}

{{- if hasKey .Values "enableCnpStatusUpdates" }}
  disable-cnp-status-updates: {{ (not .Values.enableCnpStatusUpdates) | quote }}
{{- else if (eq $defaultEnableCnpStatusUpdates "false") }}
  disable-cnp-status-updates: "true"
{{- end }}

{{- if hasKey .Values "blacklistConflictingRoutes" }}
  # Configure blacklisting of local routes not owned by Cilium.
  blacklist-conflicting-routes: {{ .Values.blacklistConflictingRoutes | quote }}
{{- end }}

{{- if hasKey .Values "crdWaitTimeout" }}
  crd-wait-timeout: {{ .Values.crdWaitTimeout | quote }}
{{- else if ( ne $crdWaitTimeout "5m" ) }}
  crd-wait-timeout: {{ $crdWaitTimeout | quote }}
{{- end }}
