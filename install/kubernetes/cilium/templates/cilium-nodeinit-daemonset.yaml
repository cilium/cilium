{{- if .Values.nodeinit.enabled }}

{{- /* Workaround so that we can set the minimal k8s version that we support */ -}}
{{- $k8sVersion := .Capabilities.KubeVersion.Version -}}
{{- $k8sMajor := .Capabilities.KubeVersion.Major -}}
{{- $k8sMinor := .Capabilities.KubeVersion.Minor -}}

{{- if .Values.Capabilities -}}
{{- if .Values.Capabilities.KubeVersion -}}
{{- if .Values.Capabilities.KubeVersion.Version -}}
{{- $k8sVersion = .Values.Capabilities.KubeVersion.Version -}}
{{- if .Values.Capabilities.KubeVersion.Major -}}
{{- $k8sMajor = toString (.Values.Capabilities.KubeVersion.Major) -}}
{{- if .Values.Capabilities.KubeVersion.Minor -}}
{{- $k8sMinor = toString (.Values.Capabilities.KubeVersion.Minor) -}}
{{- end -}}
{{- end -}}
{{- end -}}
{{- end -}}
{{- end -}}

kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: cilium-node-init
  namespace: {{ .Release.Namespace }}
  labels:
    app: cilium-node-init
spec:
  selector:
    matchLabels:
      app: cilium-node-init
  template:
    metadata:
      annotations:
{{- with .Values.nodeinit.podAnnotations }}
        {{- toYaml . | nindent 8 }}
{{- end }}
      labels:
        app: cilium-node-init
{{- with .Values.nodeinit.podLabels }}
        {{- toYaml . | nindent 8 }}
{{- end }}
    spec:
{{- with .Values.tolerations }}
      tolerations:
      {{- toYaml . | trim | nindent 6 }}
{{- end }}
      hostPID: true
      hostNetwork: true
{{- if and (or (and (eq .Release.Namespace "kube-system") (gt $k8sMinor "10")) (ge $k8sMinor "17") (gt $k8sMajor "1")) .Values.enableCriticalPriorityClass }}
      priorityClassName: system-node-critical
{{- end }}
{{- if .Values.imagePullSecrets }}
      imagePullSecrets:
        {{ toYaml .Values.imagePullSecrets | indent 6 }}
{{- end }}
      containers:
        - name: node-init
          image: {{ if .Values.nodeinit.image.override }}{{ .Values.nodeinit.image.override }}{{ else }}{{ .Values.nodeinit.image.repository }}:{{ .Values.nodeinit.image.tag }}{{ end }}
          imagePullPolicy: {{ .Values.nodeinit.image.pullPolicy }}
          terminationMessagePolicy: FallbackToLogsOnError
          securityContext:
            privileged: true
          lifecycle:
{{- if .Values.eni.enabled }}
            postStart:
              exec:
                command:
                  - nsenter
                  - --target=1
                  - --mount
                  - --
                  - "/bin/bash"
                  - "-c"
                  - |
                    #!/bin/bash

                    set -o errexit
                    set -o pipefail
                    set -o nounset

                    # When running in AWS ENI mode, it's likely that 'aws-node' has
                    # had a chance to install SNAT iptables rules. These can result
                    # in dropped traffic, so we should attempt to remove them.
                    # We do it using a 'postStart' hook since this may need to run
                    # for nodes which might have already been init'ed but may still
                    # have dangling rules. This is safe because there are no
                    # dependencies on anything that is part of the startup script
                    # itself, and can be safely run multiple times per node (e.g. in
                    # case of a restart).
                    if [[ "$(iptables-save | grep -c AWS-SNAT-CHAIN)" != "0" ]];
                    then
                      echo 'Deleting iptables rules created by the AWS CNI VPC plugin'
                      iptables-save | grep -v AWS-SNAT-CHAIN | iptables-restore
                    fi
                    echo 'Done!'
{{- end }}
{{- if .Values.nodeinit.revertReconfigureKubelet }}
            preStop:
              exec:
                command:
                  - "nsenter"
                  - "-t"
                  - "1"
                  - "-m"
                  - "--"
                  - "/bin/bash"
                  - "-c"
                  - |
                    #!/bin/bash

                    set -o errexit
                    set -o pipefail
                    set -o nounset

                    if stat /tmp/node-deinit.cilium.io > /dev/null 2>&1; then
                      exit 0
                    fi

                    echo "Waiting on pods to stop..."
                    if [ ! -f /etc/crictl.yaml ] || grep -q 'docker' /etc/crictl.yaml; then
                      # Works for COS, ubuntu
                      while docker ps | grep -v "node-init" | grep -q "POD_cilium"; do sleep 1; done
                    else
                      # COS-beta (with containerd). Some versions of COS have crictl in /home/kubernetes/bin.
                      while PATH="${PATH}:/home/kubernetes/bin" crictl ps | grep -v "node-init" | grep -q "POD_cilium"; do sleep 1; done
                    fi

                    if ip link show cilium_host; then
                      echo "Deleting cilium_host interface..."
                      ip link del cilium_host
                    fi

{{- if not (eq .Values.nodeinit.bootstrapFile "") }}
                    rm -f {{ .Values.nodeinit.bootstrapFile | quote }}
{{- end }}

                    rm -f /tmp/node-init.cilium.io
                    touch /tmp/node-deinit.cilium.io

{{- if .Values.nodeinit.reconfigureKubelet }}
                    # Check if we're running on a GKE containerd flavor as indicated by the presence
                    # of the '--container-runtime-endpoint' flag in '/etc/default/kubelet'.
                    GKE_KUBERNETES_BIN_DIR="/home/kubernetes/bin"
                    KUBELET_DEFAULTS_FILE="/etc/default/kubelet"
                    if [[ -f "${GKE_KUBERNETES_BIN_DIR}/gke" ]] && [[ $(grep -cF -- '--container-runtime-endpoint' "${KUBELET_DEFAULTS_FILE}") == "1" ]]; then
                      CONTAINERD_CONFIG="/etc/containerd/config.toml"
                      echo "Reverting changes to the containerd configuration"
                      sed -Ei "s/^\#(\s+conf_template)/\1/g" "${CONTAINERD_CONFIG}"
                      echo "Removing the kubelet wrapper"
                      [[ -f "${GKE_KUBERNETES_BIN_DIR}/the-kubelet" ]] && mv "${GKE_KUBERNETES_BIN_DIR}/the-kubelet" "${GKE_KUBERNETES_BIN_DIR}/kubelet"
                    else
                      echo "Changing kubelet configuration to --network-plugin=kubenet"
                      sed -i "s:--network-plugin=cni\ --cni-bin-dir={{ .Values.cni.binPath }}:--network-plugin=kubenet:g" "${KUBELET_DEFAULTS_FILE}"
                    fi
                    echo "Restarting the kubelet"
                    systemctl restart kubelet
{{- end }}

{{- if (and .Values.gke.enabled (or .Values.enableIPv4Masquerade .Values.gke.disableDefaultSnat))}}
                    # If the IP-MASQ chain exists, add back default jump rule from the GKE instance configure script
                    if iptables -w -t nat -L IP-MASQ > /dev/null; then
                      iptables -w -t nat -A POSTROUTING -m comment --comment "ip-masq: ensure nat POSTROUTING directs all non-LOCAL destination traffic to our custom IP-MASQ chain" -m addrtype ! --dst-type LOCAL -j IP-MASQ
                    fi
{{- end }}

                    echo "Node de-initialization complete"
{{- end }}
          env:
          # STARTUP_SCRIPT is the script run on node bootstrap. Node
          # bootstrapping can be customized in this script. This script is invoked
          # using nsenter, so it runs in the host's network and mount namespace using
          # the host's userland tools!
          - name: STARTUP_SCRIPT
            value: |
              #!/bin/bash

              set -o errexit
              set -o pipefail
              set -o nounset

              echo "Link information:"
              ip link

              echo "Routing table:"
              ip route

              echo "Addressing:"
              ip -4 a
              ip -6 a

{{- if .Values.nodeinit.removeCbrBridge }}
              if ip link show cbr0; then
                echo "Detected cbr0 bridge. Deleting interface..."
                ip link del cbr0
              fi
{{- end }}

{{- if .Values.nodeinit.reconfigureKubelet }}
              # Check if we're running on a GKE containerd flavor.
              GKE_KUBERNETES_BIN_DIR="/home/kubernetes/bin"
              if [[ -f "${GKE_KUBERNETES_BIN_DIR}/gke" ]] && command -v containerd &>/dev/null; then
                echo "GKE *_containerd flavor detected..."

                # (GKE *_containerd) Upon node restarts, GKE's containerd images seem to reset
                # the /etc directory and our changes to the kubelet and Cilium's CNI
                # configuration are removed. This leaves room for containerd and its CNI to
                # take over pods previously managed by Cilium, causing Cilium to lose
                # ownership over these pods. We rely on the empirical observation that
                # /home/kubernetes/bin/kubelet is not changed across node reboots, and replace
                # it with a wrapper script that performs some initialization steps when
                # required and then hands over control to the real kubelet.

                # Only create the kubelet wrapper if we haven't previously done so.
                if [[ ! -f "${GKE_KUBERNETES_BIN_DIR}/the-kubelet" ]];
                then
                  echo "Installing the kubelet wrapper..."

                  # Rename the real kubelet.
                  mv "${GKE_KUBERNETES_BIN_DIR}/kubelet" "${GKE_KUBERNETES_BIN_DIR}/the-kubelet"

                  # Initialize the kubelet wrapper which lives in the place of the real kubelet.
                  touch "${GKE_KUBERNETES_BIN_DIR}/kubelet"
                  chmod a+x "${GKE_KUBERNETES_BIN_DIR}/kubelet"

                  # Populate the kubelet wrapper. It will perform the initialization steps we
                  # need and then become the kubelet.
                  cat <<'EOF' | tee "${GKE_KUBERNETES_BIN_DIR}/kubelet"
              #!/bin/bash
              set -euo pipefail
              CNI_CONF_DIR="/etc/cni/net.d"
              CONTAINERD_CONFIG="/etc/containerd/config.toml"
              # Only stop and start containerd if the Cilium CNI configuration does not exist,
              # or if the 'conf_template' property is present in the containerd config file,
              # in order to avoid unnecessarily restarting containerd.
              if [[ -z "$(find "${CNI_CONF_DIR}" -type f -name '*cilium*')" || \
                    "$(grep -cE '^\s+conf_template' "${CONTAINERD_CONFIG}")" != "0" ]];
              then
                # Stop containerd as it starts by creating a CNI configuration from a template
                # causing pods to start with IPs assigned by GKE's CNI.
                # 'disable --now' is used instead of stop as this script runs concurrently
                # with containerd on node startup, and hence containerd might not have been
                # started yet, in which case 'disable' prevents it from starting.
                echo "Disabling and stopping containerd"
                systemctl disable --now containerd
                # Remove any pre-existing files in the CNI configuration directory. We skip
                # any possibly existing Cilium configuration file for the obvious reasons.
                echo "Removing undesired CNI configuration files"
                find "${CNI_CONF_DIR}" -type f -not -name '*cilium*' -exec rm {} \;
                # As mentioned above, the containerd configuration needs a little tweak in
                # order not to create the default CNI configuration, so we update its config.
                echo "Fixing containerd configuration"
                sed -Ei 's/^(\s+conf_template)/\#\1/g' "${CONTAINERD_CONFIG}"
                # Start containerd. It won't create it's CNI configuration file anymore.
                echo "Enabling and starting containerd"
                systemctl enable --now containerd
              fi
              # Become the real kubelet, and pass it some additionally required flags (and
              # place these last so they have precedence).
              exec /home/kubernetes/bin/the-kubelet "${@}" --network-plugin=cni --cni-bin-dir={{ .Values.cni.binPath }}
              EOF
                else
                  echo "Kubelet wrapper already exists, skipping..."
                fi
              else
                # (Generic) Alter the kubelet configuration to run in CNI mode
                echo "Changing kubelet configuration to --network-plugin=cni --cni-bin-dir={{ .Values.cni.binPath }}"
                mkdir -p {{ .Values.cni.binPath }}
                sed -i "s:--network-plugin=kubenet:--network-plugin=cni\ --cni-bin-dir={{ .Values.cni.binPath }}:g" /etc/default/kubelet
              fi
              echo "Restarting the kubelet..."
              systemctl restart kubelet
{{- end }}

{{- if (and .Values.gke.enabled (or .Values.enableIPv4Masquerade .Values.gke.disableDefaultSnat))}}
              # If Cilium is configured to manage masquerading of traffic leaving the node,
              # we need to disable the IP-MASQ chain because even if ip-masq-agent
              # is not installed, the node init script installs some default rules into
              # the IP-MASQ chain.
              # If we remove the jump to that ip-masq chain, then we ensure the ip masquerade
              # configuration is solely managed by Cilium.
              # Also, if Cilium is installed, it may be expected that it would be solely responsible
              # for the networking configuration on that node. So provide the same functionality
              # as the --disable-snat-flag for existing GKE clusters.
              iptables -w -t nat -D POSTROUTING -m comment --comment "ip-masq: ensure nat POSTROUTING directs all non-LOCAL destination traffic to our custom IP-MASQ chain" -m addrtype ! --dst-type LOCAL -j IP-MASQ || true
{{- end }}

{{- if not (eq .Values.nodeinit.bootstrapFile "") }}
              mkdir -p {{ .Values.nodeinit.bootstrapFile | dir | quote }}
              date > {{ .Values.nodeinit.bootstrapFile | quote }}
{{- end }}

{{- if .Values.nodeinit.restartPods }}
              echo "Restarting kubenet managed pods"
              if [ ! -f /etc/crictl.yaml ] || grep -q 'docker' /etc/crictl.yaml; then
                # Works for COS, ubuntu
                # Note the first line is the containerID with a trailing \r
                for f in `find /var/lib/cni/networks/ -type f ! -name lock ! -name last_reserved_ip.0`; do docker rm -f "$(sed 's/\r//;1q' $f)" || true; done
              elif [ -n "$(docker ps --format '{{ "{{" }}.Image{{ "}}" }}' | grep ^[0-9]*\.dkr\.ecr\.[a-z]*-[a-z]*-[0-9]*\.amazonaws\.com/amazon-k8s-cni)" ]; then
                timeout=1
                for i in $(seq 1 7); do
                  echo "Checking introspection API"
                  curl localhost:61679 && retry=false || retry=true
                  if [ $retry == false ]; then break ; fi
                  sleep "$timeout"
                  timeout=$(($timeout * 2))
                done

                for pod in $(curl "localhost:61679/v1/pods" 2> /dev/null | jq -r '. | keys[]'); do
                  container_id=$(echo "$pod" | awk -F_ ' { print $3 } ' | cut -c1-12)
                  echo "Restarting ${container_id}"
                  docker kill "${container_id}" || true
                done
              else
                # COS-beta (with containerd). Some versions of COS have crictl in /home/kubernetes/bin.
                for f in `find /var/lib/cni/networks/ -type f ! -name lock ! -name last_reserved_ip.0`; do PATH="${PATH}:/home/kubernetes/bin" crictl stopp "$(sed 's/\r//;1q' $f)" || true; done
              fi
{{- end }}

              # AKS: If azure-vnet is installed on the node, and (still) configured in bridge mode,
              # configure it as 'transparent' to be consistent with Cilium's CNI chaining config.
              # If the azure-vnet CNI config is not removed, kubelet will execute CNI CHECK commands
              # against it every 5 seconds and write 'bridge' to its state file, causing inconsistent
              # behaviour when Pods are removed.
              if [ -f /etc/cni/net.d/10-azure.conflist ]; then
                echo "Ensuring azure-vnet is configured in 'transparent' mode..."
                sed -i 's/"mode":\s*"bridge"/"mode":"transparent"/g' /etc/cni/net.d/10-azure.conflist
              fi

{{- if .Values.azure.enabled }}
              # The azure0 interface being present means the node was booted with azure-vnet configured
              # in bridge mode. This means there might be ebtables rules and neight entries interfering
              # with pod connectivity if we deploy with Azure IPAM.
              if ip l show dev azure0 >/dev/null 2>&1; then

                # In Azure IPAM mode, also remove the azure-vnet state file, otherwise ebtables rules get
                # restored by the azure-vnet CNI plugin on every CNI CHECK, which can cause connectivity
                # issues in Cilium-managed Pods. Since azure-vnet is no longer called on scheduling events,
                # this file can be removed.
                rm -f /var/run/azure-vnet.json

                # This breaks connectivity for existing workload Pods when Cilium is scheduled, but we need
                # to flush these to prevent Cilium-managed Pod IPs conflicting with Pod IPs previously allocated
                # by azure-vnet. These ebtables DNAT rules contain fixed MACs that are no longer bound on the node,
                # causing packets for these Pods to be redirected back out to the gateway, where they are dropped.
                echo 'Flushing ebtables pre/postrouting rules in nat table.. (disconnecting non-Cilium Pods!)'
                ebtables -t nat -F PREROUTING || true
                ebtables -t nat -F POSTROUTING || true

                # ip-masq-agent periodically injects PERM neigh entries towards the gateway
                # for all other k8s nodes in the cluster. These are safe to flush, as ARP can
                # resolve these nodes as usual. PERM entries will be automatically restored later.
                echo 'Deleting all permanent neighbour entries on azure0...'
                ip neigh show dev azure0 nud permanent | cut -d' ' -f1 | xargs -r -n1 ip neigh del dev azure0 to || true
              fi
{{- end }}

{{- if .Values.nodeinit.revertReconfigureKubelet }}
              rm -f /tmp/node-deinit.cilium.io
{{- end }}
              echo "Node initialization complete"
{{- end }}
