{{- if .Values.nodeinit.enabled }}

{{- /* Workaround so that we can set the minimal k8s version that we support */ -}}
{{- $k8sVersion := .Capabilities.KubeVersion.Version -}}
{{- $k8sMajor := .Capabilities.KubeVersion.Major -}}
{{- $k8sMinor := .Capabilities.KubeVersion.Minor -}}

{{- if .Values.Capabilities -}}
{{- if .Values.Capabilities.KubeVersion -}}
{{- if .Values.Capabilities.KubeVersion.Version -}}
{{- $k8sVersion = .Values.Capabilities.KubeVersion.Version -}}
{{- if .Values.Capabilities.KubeVersion.Major -}}
{{- $k8sMajor = toString (.Values.Capabilities.KubeVersion.Major) -}}
{{- if .Values.Capabilities.KubeVersion.Minor -}}
{{- $k8sMinor = toString (.Values.Capabilities.KubeVersion.Minor) -}}
{{- end -}}
{{- end -}}
{{- end -}}
{{- end -}}
{{- end -}}

kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: cilium-node-init
  namespace: {{ .Release.Namespace }}
  labels:
    app: cilium-node-init
spec:
  selector:
    matchLabels:
      app: cilium-node-init
  template:
    metadata:
      annotations:
{{- with .Values.nodeinit.podAnnotations }}
        {{- toYaml . | nindent 8 }}
{{- end }}
      labels:
        app: cilium-node-init
{{- with .Values.nodeinit.podLabels }}
        {{- toYaml . | nindent 8 }}
{{- end }}
    spec:
{{- with .Values.tolerations }}
      tolerations:
      {{- toYaml . | trim | nindent 6 }}
{{- end }}
      hostPID: true
      hostNetwork: true
{{- if and (or (and (eq .Release.Namespace "kube-system") (gt $k8sMinor "10")) (ge $k8sMinor "17") (gt $k8sMajor "1")) .Values.enableCriticalPriorityClass }}
      priorityClassName: system-node-critical
{{- end }}
{{- if .Values.imagePullSecrets }}
      imagePullSecrets:
        {{ toYaml .Values.imagePullSecrets | indent 6 }}
{{- end }}
      containers:
        - name: node-init
          image: {{ .Values.nodeinit.image.repository }}:{{ .Values.nodeinit.image.tag }}
          imagePullPolicy: {{ .Values.nodeinit.image.pullPolicy }}
          securityContext:
            privileged: true
{{- if .Values.nodeinit.revertReconfigureKubelet }}
          lifecycle:
            preStop:
              exec:
                command:
                  - "nsenter"
                  - "-t"
                  - "1"
                  - "-m"
                  - "--"
                  - "/bin/sh"
                  - "-c"
                  - |
                    #!/bin/bash

                    set -o errexit
                    set -o pipefail
                    set -o nounset

                    if stat /tmp/node-deinit.cilium.io > /dev/null 2>&1; then
                      exit 0
                    fi

                    echo "Waiting on pods to stop..."
                    if [ ! -f /etc/crictl.yaml ] || grep -q 'docker' /etc/crictl.yaml; then
                      # Works for COS, ubuntu
                      while docker ps | grep -v "node-init" | grep -q "POD_cilium"; do sleep 1; done
                    else
                      # COS-beta (with containerd). Some versions of COS have crictl in /home/kubernetes/bin.
                      while PATH="${PATH}:/home/kubernetes/bin" crictl ps | grep -v "node-init" | grep -q "POD_cilium"; do sleep 1; done
                    fi

                    if ip link show cilium_host; then
                      echo "Deleting cilium_host interface..."
                      ip link del cilium_host
                    fi

{{- if not (eq .Values.nodeinit.bootstrapFile "") }}
                    rm -f {{ .Values.nodeinit.bootstrapFile }}
{{- end }}

                    rm -f /tmp/node-init.cilium.io
                    touch /tmp/node-deinit.cilium.io

{{- if .Values.nodeinit.reconfigureKubelet }}
                    echo "Changing kubelet configuration to --network-plugin=kubenet"
                    sed -i "s:--network-plugin=cni\ --cni-bin-dir={{ .Values.cni.binPath }}:--network-plugin=kubenet:g" /etc/default/kubelet
                    echo "Restarting kubelet..."
                    systemctl restart kubelet
{{- end }}

{{- if (and .Values.gke.enabled (or .Values.masquerade .Values.gke.disableDefaultSnat))}}
                    # If the IP-MASQ chain exists, add back default jump rule from the GKE instance configure script
                    if iptables -w -t nat -L IP-MASQ > /dev/null; then
                      iptables -w -t nat -A POSTROUTING -m comment --comment "ip-masq: ensure nat POSTROUTING directs all non-LOCAL destination traffic to our custom IP-MASQ chain" -m addrtype ! --dst-type LOCAL -j IP-MASQ
                    fi
{{- end }}

                    echo "Node de-initialization complete"
{{- end }}
          env:
          - name: CHECKPOINT_PATH
            value: /tmp/node-init.cilium.io
          # STARTUP_SCRIPT is the script run on node bootstrap. Node
          # bootstrapping can be customized in this script. This script is invoked
          # using nsenter, so it runs in the host's network and mount namespace using
          # the host's userland tools!
          - name: STARTUP_SCRIPT
            value: |
              #!/bin/bash

              set -o errexit
              set -o pipefail
              set -o nounset

              echo "Link information:"
              ip link

              echo "Routing table:"
              ip route

              echo "Addressing:"
              ip -4 a
              ip -6 a

{{- if .Values.nodeinit.removeCbrBridge }}
              if ip link show cbr0; then
                echo "Detected cbr0 bridge. Deleting interface..."
                ip link del cbr0
              fi
{{- end }}

{{- if .Values.nodeinit.reconfigureKubelet }}
              # GKE: Alter the kubelet configuration to run in CNI mode
              echo "Changing kubelet configuration to --network-plugin=cni --cni-bin-dir={{ .Values.cni.binPath }}"
              mkdir -p {{ .Values.cni.binPath }}
              sed -i "s:--network-plugin=kubenet:--network-plugin=cni\ --cni-bin-dir={{ .Values.cni.binPath }}:g" /etc/default/kubelet
              echo "Restarting kubelet..."
              systemctl restart kubelet
{{- end }}

{{- if (and .Values.gke.enabled (or .Values.masquerade .Values.gke.disableDefaultSnat))}}
              # If Cilium is configured to manage masquerading of traffic leaving the node,
              # we need to disable the IP-MASQ chain because even if ip-masq-agent
              # is not installed, the node init script installs some default rules into
              # the IP-MASQ chain.
              # If we remove the jump to that ip-masq chain, then we ensure the ip masquerade
              # configuration is solely managed by Cilium.
              # Also, if Cilium is installed, it may be expected that it would be solely responsible
              # for the networking configuration on that node. So provide the same functionality
              # as the --disable-snat-flag for existing GKE clusters.
              iptables -w -t nat -D POSTROUTING -m comment --comment "ip-masq: ensure nat POSTROUTING directs all non-LOCAL destination traffic to our custom IP-MASQ chain" -m addrtype ! --dst-type LOCAL -j IP-MASQ || true
{{- end }}

{{- if not (eq .Values.nodeinit.bootstrapFile "") }}
              date > {{ .Values.nodeinit.bootstrapFile }}
{{- end }}

{{- if .Values.nodeinit.restartPods }}
              echo "Restarting kubenet managed pods"
              if [ ! -f /etc/crictl.yaml ] || grep -q 'docker' /etc/crictl.yaml; then
                # Works for COS, ubuntu
                # Note the first line is the containerID with a trailing \r
                for f in `find /var/lib/cni/networks/ -type f ! -name lock ! -name last_reserved_ip.0`; do docker rm -f "$(sed 's/\r//;1q' $f)" || true; done
              elif [ -n "$(docker ps --format '{{ "{{" }}.Image{{ "}}" }}' | grep ^[0-9]*\.dkr\.ecr\.[a-z]*-[a-z]*-[0-9]*\.amazonaws\.com/amazon-k8s-cni)" ]; then
                timeout=1
                for i in $(seq 1 7); do
                  echo "Checking introspection API"
                  curl localhost:61679 && retry=false || retry=true
                  if [ $retry == false ]; then break ; fi
                  sleep "$timeout"
                  timeout=$(($timeout * 2))
                done

                for pod in $(curl "localhost:61679/v1/pods" 2> /dev/null | jq -r '. | keys[]'); do
                  container_id=$(echo "$pod" | awk -F_ ' { print $3 } ' | cut -c1-12)
                  echo "Restarting ${container_id}"
                  docker kill "${container_id}" || true
                done
              else
                # COS-beta (with containerd). Some versions of COS have crictl in /home/kubernetes/bin.
                for f in `find /var/lib/cni/networks/ -type f ! -name lock ! -name last_reserved_ip.0`; do PATH="${PATH}:/home/kubernetes/bin" crictl stopp "$(sed 's/\r//;1q' $f)" || true; done
              fi
{{- end }}

              # AKS: If azure-vnet is installed on the node, and (still) configured in bridge mode,
              # configure it as 'transparent' to be consistent with Cilium's CNI chaining config.
              # If the azure-vnet CNI config is not removed, kubelet will execute CNI CHECK commands
              # against it every 5 seconds and write 'bridge' to its state file, causing inconsistent
              # behaviour when Pods are removed.
              if [ -f /etc/cni/net.d/10-azure.conflist ]; then
                echo "Ensuring azure-vnet is configured in 'transparent' mode..."
                sed -i 's/"mode":\s*"bridge"/"mode":"transparent"/g' /etc/cni/net.d/10-azure.conflist
              fi

{{- if .Values.azure.enabled }}
              # The azure0 interface being present means the node was booted with azure-vnet configured
              # in bridge mode. This means there might be ebtables rules and neight entries interfering
              # with pod connectivity if we deploy with Azure IPAM.
              if ip l show dev azure0 >/dev/null 2>&1; then

                # In Azure IPAM mode, also remove the azure-vnet state file, otherwise ebtables rules get
                # restored by the azure-vnet CNI plugin on every CNI CHECK, which can cause connectivity
                # issues in Cilium-managed Pods. Since azure-vnet is no longer called on scheduling events,
                # this file can be removed.
                rm -f /var/run/azure-vnet.json

                # This breaks connectivity for existing workload Pods when Cilium is scheduled, but we need
                # to flush these to prevent Cilium-managed Pod IPs conflicting with Pod IPs previously allocated
                # by azure-vnet. These ebtables DNAT rules contain fixed MACs that are no longer bound on the node,
                # causing packets for these Pods to be redirected back out to the gateway, where they are dropped.
                echo 'Flushing ebtables pre/postrouting rules in nat table.. (disconnecting non-Cilium Pods!)'
                ebtables -t nat -F PREROUTING || true
                ebtables -t nat -F POSTROUTING || true

                # ip-masq-agent periodically injects PERM neigh entries towards the gateway
                # for all other k8s nodes in the cluster. These are safe to flush, as ARP can
                # resolve these nodes as usual. PERM entries will be automatically restored later.
                echo 'Deleting all permanent neighbour entries on azure0...'
                ip neigh show dev azure0 nud permanent | cut -d' ' -f1 | xargs -r -n1 ip neigh del dev azure0 to || true
              fi
{{- end }}

{{- if .Values.nodeinit.revertReconfigureKubelet }}
              rm -f /tmp/node-deinit.cilium.io
{{- end }}
              echo "Node initialization complete"
{{- end }}
