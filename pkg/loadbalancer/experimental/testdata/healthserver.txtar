#! --enable-experimental-lb --enable-health-check-nodeport --lb-test-fault-probability=0.0

# Start and wait for initialization.
hive start
db/initialized

# Replace the port numbers
cp service.yaml service2.yaml
cp service.yaml service_no_health.yaml
replace '$PORT' $PORT1 service.yaml
replace '$PORT' $PORT2 service2.yaml
replace '$PORT' 0 service_no_health.yaml
replace '$PORT1' $PORT1 services.table
replace '$PORT1' $PORT1 health_with_service.table
replace '$PORT' $PORT1 frontends.table
replace '$PORT' $PORT1 backends.table
replace '$PORT' $PORT1 lbmaps.expected

# HealthServer's jobs should be reported in module health
db/cmp --grep=^loadbalancer health health_no_service.table

# Add a node address, service and endpoints.
db/insert node-addresses addrv4.yaml
db/cmp node-addresses nodeaddrs.table
k8s add service.yaml endpointslice.yaml
db/cmp --grep=^loadbalancer health health_with_service.table
db/cmp services services.table
db/cmp frontends frontends.table
db/cmp backends backends.table 

# Check the BPF maps
lb/maps-dump lbmaps.actual
* cmp lbmaps.expected lbmaps.actual

# Validate health server response
* http/get http://127.0.0.88:$PORT1 healthserver.actual
cmp healthserver.expected healthserver.actual

# Test changing the health check port
k8s update service2.yaml

# Check that the frontend for the health server is updated
replace $PORT1 $PORT2 frontends.table
db/cmp frontends frontends.table
replace $PORT1 $PORT2 backends.table
db/cmp backends backends.table

# Check that $PORT2 now responds and old port does not.
* http/get http://127.0.0.88:$PORT2 healthserver.actual
cmp healthserver.expected healthserver.actual
!* http/get http://127.0.0.88:$PORT1 healthserver.actual

# Setting the traffic policy to Cluster makes the service
# unqualified for health server, removing it.
cp service2.yaml service2_tpcluster.yaml
replace 'externalTrafficPolicy: Local' 'externalTrafficPolicy: Cluster' service2_tpcluster.yaml
replace 'internalTrafficPolicy: Local' 'internalTrafficPolicy: Cluster' service2_tpcluster.yaml
k8s update service2_tpcluster.yaml
db/cmp frontends frontends_tpcluster.table

# The health checker server should be down now.
!* http/get http://127.0.0.88:$PORT2 healthserver.actual

# Restore health checking for next test.
k8s update service2.yaml
* http/get http://127.0.0.88:$PORT2 healthserver.actual
cmp healthserver.expected healthserver.actual

# Test removing the health check port
k8s update service_no_health.yaml

# Both ports should now stop responding
# "!*" means expect failure and retry if needed
!* http/get http://127.0.0.88:$PORT2 healthserver.actual
!* http/get http://127.0.0.88:$PORT1 healthserver.actual

db/cmp frontends frontends_nohealthcheck.table
db/cmp backends backends_nohealthcheck.table

#####

-- addrv4.yaml --
addr: 1.1.1.1
nodeport: true
primary: true
devicename: test

-- health_no_service.table --
Module                                   Component                   Level   Message                    Error   
loadbalancer-experimental.healthserver   job-control-loop            OK      0 health servers running   
loadbalancer-experimental                job-node-addr-reconciler    OK      Running
loadbalancer-experimental.reconciler     job-reconcile               OK      OK, 0 object(s)            
loadbalancer-experimental.reconciler     job-refresh                 OK      Next refresh in 30m0s      
loadbalancer-experimental.reflector      job-reflector               OK      Running                    

-- health_with_service.table --
Module                                   Component                   Level   Message                    Error   
loadbalancer-experimental.healthserver   job-control-loop            OK      1 health servers running   
loadbalancer-experimental.healthserver   job-listener-$PORT1          OK      Running
loadbalancer-experimental                job-node-addr-reconciler    OK      Running
loadbalancer-experimental.reconciler     job-reconcile               OK      OK, 3 object(s)            
loadbalancer-experimental.reconciler     job-refresh                 OK      Next refresh in 30m0s      
loadbalancer-experimental.reflector      job-reflector               OK      Running                    

-- nodeaddrs.table --
Address NodePort Primary DeviceName
1.1.1.1 true     true    test

-- services.table --
Name                   Source   NatPolicy   ExtTrafficPolicy   IntTrafficPolicy   HealthCheckNodePort   
test/echo              k8s                  Local              Local              $PORT1
test/echo-healthserver local                Local              Local              0

-- frontends.table --
Address               Type         ServiceName            PortName   Status  Backends
10.96.50.104:80/TCP   ClusterIP    test/echo              http       Done    10.244.1.1:80/TCP (active), 10.244.1.2:80/TCP (active)
172.16.1.1:80/TCP     LoadBalancer test/echo              http       Done    10.244.1.1:80/TCP (active), 10.244.1.2:80/TCP (active)
172.16.1.1:$PORT/TCP  LoadBalancer test/echo-healthserver            Done    127.0.0.88:$PORT/TCP/i (active)

-- frontends_nohealthcheck.table --
Address               Type         ServiceName            PortName   Status  Backends
10.96.50.104:80/TCP   ClusterIP    test/echo              http       Done    10.244.1.1:80/TCP (active), 10.244.1.2:80/TCP (active)
172.16.1.1:80/TCP     LoadBalancer test/echo              http       Done    10.244.1.1:80/TCP (active), 10.244.1.2:80/TCP (active)

-- frontends_tpcluster.table --
Address               Type         ServiceName            PortName   Status  Backends
10.96.50.104:80/TCP   ClusterIP    test/echo              http       Done    10.244.1.1:80/TCP (active), 10.244.1.2:80/TCP (active), 10.244.1.3:80/TCP (active), 10.244.1.4:80/TCP (active)
172.16.1.1:80/TCP     LoadBalancer test/echo              http       Done    10.244.1.1:80/TCP (active), 10.244.1.2:80/TCP (active), 10.244.1.3:80/TCP (active), 10.244.1.4:80/TCP (active)

-- frontends_empty.table --
Address               Type        ServiceName   PortName   Status  Backends

-- backends.table --
Address                State    Instances              NodeName     ZoneID
10.244.1.1:80/TCP      active   test/echo (http)       testnode     0
10.244.1.2:80/TCP      active   test/echo (http)       testnode     0
10.244.1.3:80/TCP      active   test/echo (http)       othernode    0
10.244.1.4:80/TCP      active   test/echo (http)       othernode    0
127.0.0.88:$PORT/TCP/i active   test/echo-healthserver testnode     0

-- backends_nohealthcheck.table --
Address               State    Instances              NodeName     ZoneID
10.244.1.1:80/TCP     active   test/echo (http)       testnode     0
10.244.1.2:80/TCP     active   test/echo (http)       testnode     0
10.244.1.3:80/TCP     active   test/echo (http)       othernode    0
10.244.1.4:80/TCP     active   test/echo (http)       othernode    0

-- backends_empty.table --
Address             State    Instances            NodeName           ZoneID

-- service.yaml --
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2022-09-13T11:11:26Z"
  name: echo
  namespace: test
  resourceVersion: "741"
  uid: a49fe99c-3564-4754-acc4-780f2331a49b
spec:
  clusterIP: 10.96.50.104
  clusterIPs:
  - 10.96.50.104
  externalTrafficPolicy: Local
  internalTrafficPolicy: Local
  healthCheckNodePort: $PORT
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http
    nodePort: 30781
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    name: echo
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 172.16.1.1

-- endpointslice.yaml --
apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
metadata:
  annotations:
  creationTimestamp: "2022-09-13T11:11:26Z"
  generateName: echo-
  generation: 3
  labels:
    endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io
    kubernetes.io/service-name: echo
  name: echo-kvlm2
  namespace: test
  resourceVersion: "797"
  uid: d1f517f6-ab88-4c76-9bd0-4906a17cdd75
addressType: IPv4
endpoints:
- addresses:
  - 10.244.1.1
  nodeName: testnode
- addresses:
  - 10.244.1.2
  nodeName: testnode
- addresses:
  - 10.244.1.3
  nodeName: othernode
- addresses:
  - 10.244.1.4
  nodeName: othernode
ports:
- name: http
  port: 80
  protocol: TCP

-- healthserver.expected --
200 OK
Content-Length=66
Content-Type=application/json
Date=<omitted>
X-Content-Type-Options=nosniff
X-Load-Balancing-Endpoint-Weight=4
---
{"service":{"namespace":"test","name":"echo"},"localEndpoints":4}
-- lbmaps.expected --
BE: ID=1 ADDR=10.244.1.1:80/TCP STATE=active
BE: ID=2 ADDR=10.244.1.2:80/TCP STATE=active
BE: ID=3 ADDR=127.0.0.88:$PORT/TCP STATE=active
REV: ID=1 ADDR=10.96.50.104:80
REV: ID=2 ADDR=172.16.1.1:80
REV: ID=3 ADDR=172.16.1.1:$PORT
SVC: ID=1 ADDR=10.96.50.104:80/TCP SLOT=0 BEID=16777216 COUNT=2 QCOUNT=0 LBALG=random FLAGS=ClusterIP+Local+InternalLocal+non-routable
SVC: ID=1 ADDR=10.96.50.104:80/TCP SLOT=1 BEID=1 COUNT=0 QCOUNT=0 FLAGS=ClusterIP+Local+InternalLocal+non-routable
SVC: ID=1 ADDR=10.96.50.104:80/TCP SLOT=2 BEID=2 COUNT=0 QCOUNT=0 FLAGS=ClusterIP+Local+InternalLocal+non-routable
SVC: ID=2 ADDR=172.16.1.1:80/TCP SLOT=0 BEID=16777216 COUNT=2 QCOUNT=0 LBALG=random FLAGS=LoadBalancer+Local+InternalLocal
SVC: ID=2 ADDR=172.16.1.1:80/TCP SLOT=1 BEID=1 COUNT=0 QCOUNT=0 FLAGS=LoadBalancer+Local+InternalLocal
SVC: ID=2 ADDR=172.16.1.1:80/TCP SLOT=2 BEID=2 COUNT=0 QCOUNT=0 FLAGS=LoadBalancer+Local+InternalLocal
SVC: ID=3 ADDR=172.16.1.1:$PORT/TCP SLOT=0 BEID=16777216 COUNT=1 QCOUNT=0 LBALG=random FLAGS=LoadBalancer+Local+InternalLocal
SVC: ID=3 ADDR=172.16.1.1:$PORT/TCP SLOT=1 BEID=3 COUNT=0 QCOUNT=0 FLAGS=LoadBalancer+Local+InternalLocal
