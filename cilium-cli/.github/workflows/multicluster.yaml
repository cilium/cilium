name: Multicluster

# Any change in triggers needs to be reflected in the concurrency group.
on:
  pull_request_target: {}
  schedule:
    - cron:  '40 */6 * * *'

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || 'scheduled' }}
  cancel-in-progress: true

env:
  clusterName1: cilium-cli-ci-multicluster-1-${{ github.run_number }}
  clusterName2: cilium-cli-ci-multicluster-2-${{ github.run_number }}
  zone: us-west2-a

jobs:
  installation-and-connectivity:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Set up Go
        uses: actions/setup-go@v2
        with:
          go-version: 1.16.2

      - name: Set up Go for root
        run: |
          sudo ln -sf `which go` `sudo which go` || true
          sudo go version

      - name: Set up gcloud
        uses: google-github-actions/setup-gcloud@master
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          service_account_key: ${{ secrets.GCP_SA_KEY }}
          export_default_credentials: true

      - name: gcloud info
        run: |
          gcloud info

      - name: Create GKE cluster2
        run: |
          gcloud container clusters create ${{ env.clusterName2 }} --image-type COS --num-nodes 2 --machine-type n1-standard-4 --zone ${{ env.zone }} --enable-ip-alias
          gcloud container clusters get-credentials ${{ env.clusterName2 }} --zone ${{ env.zone }}

      - name: Create gcloud-free kubeconfig for cluster 2
        run: |
          .github/get-kubeconfig.sh
          mv kubeconfig kubeconfig-cluster2

      - name: Create GKE cluster1
        run: |
          gcloud container clusters create ${{ env.clusterName1 }} --image-type COS --num-nodes 2 --machine-type n1-standard-4 --zone ${{ env.zone }} --enable-ip-alias
          gcloud container clusters get-credentials ${{ env.clusterName1 }} --zone ${{ env.zone }}

      - name: Allow cross-cluster traffic
        run: |
          TAG1=$(gcloud compute firewall-rules list --filter="name~^gke-${{ env.clusterName1 }}-[0-9a-z]*-all$" --format="value(name)")
          TAG2=$(gcloud compute firewall-rules list --filter="name~^gke-${{ env.clusterName2 }}-[0-9a-z]*-all$" --format="value(name)")
          GCP_FW_RULE=cilium-cli-ci-multicluster-${{ github.run_number }}-rule
          gcloud compute firewall-rules describe $TAG1
          gcloud compute firewall-rules describe $TAG2
          gcloud compute firewall-rules create $GCP_FW_RULE --allow tcp,udp,icmp,sctp,esp,ah --priority=999 --source-ranges=10.0.0.0/9 --target-tags=${TAG1/-all/-node},${TAG2/-all/-node}
          gcloud compute firewall-rules describe $GCP_FW_RULE
          echo "GCP_FW_RULE=$GCP_FW_RULE" >> $GITHUB_ENV

      - name: Set job vars
        id: vars
        run: |
          CLUSTER_CIDR_1=$(gcloud container clusters describe ${{ env.clusterName1 }} --zone ${{ env.zone }} --format="value(clusterIpv4Cidr)")
          echo ::set-output name=cluster_cidr_1::${CLUSTER_CIDR_1}
          CLUSTER_CIDR_2=$(gcloud container clusters describe ${{ env.clusterName2 }} --zone ${{ env.zone }} --format="value(clusterIpv4Cidr)")
          echo ::set-output name=cluster_cidr_2::${CLUSTER_CIDR_2}

          if [ ${{ github.event.issue.pull_request || github.event.pull_request }} ]; then
            PR_API_JSON=$(curl \
              -H "Accept: application/vnd.github.v3+json" \
              -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
              ${{ github.event.issue.pull_request.url || github.event.pull_request.url }})
            SHA=$(echo "$PR_API_JSON" | jq -r ".head.sha")
            OWNER=$(echo "$PR_API_JSON" | jq -r ".number")
            IS_PR=true
          else
            SHA=${{ github.sha }}
            OWNER=${{ github.sha }}
          fi
          echo ::set-output name=sha::${SHA}

      - name: Create gcloud-free kubeconfig for cluster 1, merge kubeconfigs and put them in configmap
        run: |
          .github/get-kubeconfig.sh
          mv kubeconfig kubeconfig-cluster1
          go run .github/tools/kubeconfig-merger/main.go kubeconfig-cluster1 kubeconfig-cluster2 kubeconfig
          kubectl create configmap cilium-cli-kubeconfig -n kube-system --from-file kubeconfig

      - name: Load cilium test script in configmap
        run: |
          kubectl create configmap cilium-cli-test-script -n kube-system --from-file=in-cluster-test-script.sh=.github/in-cluster-test-scripts/multicluster.sh

      - name: Create cilium-cli test job
        run: |
          helm install .github/cilium-cli-test-job-chart --generate-name \
          --set tag=${{ steps.vars.outputs.sha }} \
          --set job_name=cilium-cli \
          --set test_script_cm=cilium-cli-test-script \
          --set cluster_name_1=${{ env.clusterName1 }} \
          --set cluster_cidr_1=${{ steps.vars.outputs.cluster_cidr_1 }} \
          --set cluster_name_2=${{ env.clusterName2 }} \
          --set cluster_cidr_2=${{ steps.vars.outputs.cluster_cidr_2 }}

      - name: Wait for test job
        run: |
          kubectl -n kube-system wait job/cilium-cli --for=condition=complete --timeout=20m

      - name: Post-test information gathering
        if: ${{ always() }}
        run: |
          kubectl logs -n kube-system job/cilium-cli
          kubectl get pods --all-namespaces -o wide
          curl -sLO https://github.com/cilium/cilium-sysdump/releases/latest/download/cilium-sysdump.zip
          python cilium-sysdump.zip --output cilium-sysdump-out
        shell: bash {0}

      - name: Cleanup
        if: ${{ always() }}
        run: |
          gcloud compute firewall-rules delete --quiet $GCP_FW_RULE
          gcloud container clusters delete --quiet ${{ env.clusterName1 }} --zone ${{ env.zone }} --async
          gcloud container clusters delete --quiet ${{ env.clusterName2 }} --zone ${{ env.zone }} --async
        shell: bash {0}

      - name: Upload Artifacts
        if: ${{ always() }}
        uses: actions/upload-artifact@v2
        with:
          name: cilium-sysdump-out.zip
          path: cilium-sysdump-out.zip
          retention-days: 5
