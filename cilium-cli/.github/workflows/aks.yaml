name: AKS

# Any change in triggers needs to be reflected in the concurrency group.
on:
  ### FOR TESTING PURPOSES
  # This workflow runs in the context of `master`, and ignores changes to
  # workflow files in PRs. For testing changes to this workflow from a PR:
  # - Make sure the PR uses a branch from the base repository (requires write
  #   privileges). It will not work with a branch from a fork (missing secrets).
  # - Uncomment the `pull_request` event below, commit separately with a `DO
  #   NOT MERGE` message, and push to the PR. As long as the commit is present,
  #   any push to the PR will trigger this workflow.
  # - Don't forget to remove the `DO NOT MERGE` commit once satisfied. The run
  #   will disappear from the PR checks: please provide a direct link to the
  #   successful workflow run (can be found from Actions tab) in a comment.
  # 
  # pull_request: {}
  ###
  pull_request_target: {}
  # Run every 6 hours
  schedule:
    - cron:  '0 0/6 * * *'

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || 'scheduled' }}
  cancel-in-progress: true

env:
  name: ${{ github.repository_owner }}-${{ github.event.repository.name }}-${{ github.run_id }}
  location: westeurope
  cost_reduction: --node-vm-size Standard_B2s --node-osdisk-size 30

jobs:
  installation-and-connectivity:
    if: ${{ github.repository == 'cilium/cilium-cli' }}
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout
        uses: actions/checkout@ec3a7ce113134d7a93b817d10a8272cb61118579

      - name: Login to Azure
        uses: azure/login@1f63701bf3e6892515f1b7ce2d2bf1708b46beaf
        with:
          creds: ${{ secrets.AZURE_PR_SP_CREDS }}

      - name: Display Azure CLI info
        uses: azure/CLI@7378ce2ca3c38b4b063feb7a4cbe384fef978055
        with:
          azcliversion: 2.0.72
          inlineScript: |
            az account show

      - name: Set up job variables
        id: vars
        run: |
          if [ ${{ github.event.issue.pull_request || github.event.pull_request }} ]; then
            PR_API_JSON=$(curl \
              -H "Accept: application/vnd.github.v3+json" \
              -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
              ${{ github.event.issue.pull_request.url || github.event.pull_request.url }})
            SHA=$(echo "$PR_API_JSON" | jq -r ".head.sha")
            OWNER=$(echo "$PR_API_JSON" | jq -r ".number")
          else
            SHA=${{ github.sha }}
            OWNER=${{ github.sha }}
          fi

          echo ::set-output name=sha::${SHA}
          echo ::set-output name=owner::${OWNER}

      - name: Create AKS cluster
        run: |
          # Create group
          az group create \
            --name ${{ env.name }} \
            --location ${{ env.location }} \
            --tags usage=${{ github.repository_owner }}-${{ github.event.repository.name }} owner=${{ steps.vars.outputs.owner }}

          # Create AKS cluster
          az aks create \
            --resource-group ${{ env.name }} \
            --name ${{ env.name }} \
            --location ${{ env.location }} \
            --network-plugin azure \
            --node-count 1 \
            ${{ env.cost_reduction }} \
            --generate-ssh-keys

          # Get name of initial system node pool
          nodepool_to_delete=$(az aks nodepool list --resource-group ${{ env.name }} --cluster-name ${{ env.name }} --output tsv --query "[0].name")

          # Create system node pool tainted with `CriticalAddonsOnly=true:NoSchedule`
          az aks nodepool add \
            --resource-group ${{ env.name }} \
            --cluster-name ${{ env.name }} \
            --name systempool \
            --mode system \
            --node-count 1 \
            --node-taints "CriticalAddonsOnly=true:NoSchedule" \
            ${{ env.cost_reduction }} \
            --no-wait

          # Create user node pool tainted with `node.cilium.io/agent-not-ready=true:NoExecute`
          az aks nodepool add \
            --resource-group ${{ env.name }} \
            --cluster-name ${{ env.name }} \
            --name userpool \
            --mode user \
            --node-count 2 \
            --node-taints "node.cilium.io/agent-not-ready=true:NoExecute" \
            ${{ env.cost_reduction }} \
            --no-wait

          # Delete the initial system node pool
          az aks nodepool delete \
            --resource-group ${{ env.name }} \
            --cluster-name ${{ env.name }} \
            --name "${nodepool_to_delete}"
          # note: the existing AKS documentation at https://docs.cilium.io/en/latest/gettingstarted/k8s-install-default/
          # recommends using `--no-wait` for the `delete` operation as well,
          # however we had to drop it on `cilium-cli` due to the use of
          # in-cluster scripts to run `cilium install`: since it runs in a pod,
          # we must make sure it does not scheduled on the initial system node
          # pool as the Cilium CLI does not work when `cilium install` is
          # abruptly interrupted, cf. https://github.com/cilium/cilium-cli/issues/589

      - name: Get cluster credentials
        run: |
          az aks get-credentials \
            --resource-group ${{ env.name }} \
            --name ${{ env.name }}

      - name: Create kubeconfig and load it in configmap
        run: |
          .github/get-kubeconfig.sh
          kubectl create configmap cilium-cli-kubeconfig -n kube-system --from-file kubeconfig

      - name: Load cilium install script in configmap
        run: |
          kubectl create configmap cilium-cli-test-script-install -n kube-system --from-file=in-cluster-test-script.sh=.github/in-cluster-test-scripts/aks-install.sh

      - name: Set up Azure-specific CLI variables since `az` is not available in-cluster
        id: az
        run: |
          # Derive subscription ID from credentials
          AZURE_SUBSCRIPTION_ID=$(az account show --query "id" --output tsv)

          # Derive AKS node resource group from created cluster
          AZURE_NODE_RESOURCE_GROUP=$(az aks show --resource-group ${{ env.name }} --name ${{ env.name }} --query "nodeResourceGroup" --output tsv)

          # Create Service Principal with minimal privileges over the AKS node resource group
          AZURE_SERVICE_PRINCIPAL=$(az ad sp create-for-rbac --scopes /subscriptions/${AZURE_SUBSCRIPTION_ID}/resourceGroups/${AZURE_NODE_RESOURCE_GROUP} --role Contributor --output json --only-show-errors)
          TENANT_ID=$(echo ${AZURE_SERVICE_PRINCIPAL} | jq -r '.tenant')
          CLIENT_ID=$(echo ${AZURE_SERVICE_PRINCIPAL} | jq -r '.appId')
          CLIENT_SECRET=$(echo ${AZURE_SERVICE_PRINCIPAL} | jq -r '.password')

          echo ::set-output name=subscription-id::${AZURE_SUBSCRIPTION_ID}
          echo ::set-output name=node-resource-group::${AZURE_NODE_RESOURCE_GROUP}
          echo ::set-output name=tenant-id::${TENANT_ID}
          echo ::set-output name=client-id::${CLIENT_ID}
          echo ::add-mask::${CLIENT_SECRET}
          echo ::set-output name=client-secret::${CLIENT_SECRET}

      - name: Create cilium-cli install job
        run: |
          helm install .github/cilium-cli-test-job-chart \
            --generate-name \
            --set job_name=cilium-cli-install \
            --set test_script_cm=cilium-cli-test-script-install \
            --set tag=${{ steps.vars.outputs.sha }} \
            --set azure.subscription_id=${{ steps.az.outputs.subscription-id }} \
            --set azure.node_resource_group=${{ steps.az.outputs.node-resource-group }} \
            --set azure.tenant_id=${{ steps.az.outputs.tenant-id }} \
            --set azure.client_id=${{ steps.az.outputs.client-id }} \
            --set azure.client_secret=${{ steps.az.outputs.client-secret }}

      - name: Wait for install job
        env:
          timeout: 5m
        run: |
          # Background wait for job to complete or timeout
          kubectl -n kube-system wait job/cilium-cli-install --for=condition=complete --timeout=${{ env.timeout }} &
          complete_pid=$!

          # Background wait for job to fail
          (kubectl -n kube-system wait job/cilium-cli-install --for=condition=failed --timeout=${{ env.timeout }} && exit 1) &
          failed_pid=$!

          # Active wait for whichever background process ends first
          wait -n $complete_pid $failed_pid
          EXIT_CODE=$?

          # Retrieve job logs
          kubectl logs --timestamps -n kube-system job/cilium-cli-install
          exit ${EXIT_CODE}
        shell: bash {0} # Disable default fail-fast behaviour so that all commands run independently

      - name: Load test script in configmap
        run: |
          kubectl create configmap cilium-cli-test-script -n kube-system --from-file=in-cluster-test-script.sh=.github/in-cluster-test-scripts/aks.sh

      - name: Create cilium-cli job
        run: |
          helm install .github/cilium-cli-test-job-chart \
            --generate-name

      - name: Wait for test job
        env:
          timeout: 15m
        run: |
          # Background wait for job to complete or timeout
          kubectl -n kube-system wait job/cilium-cli --for=condition=complete --timeout=${{ env.timeout }} &
          complete_pid=$!

          # Background wait for job to fail
          (kubectl -n kube-system wait job/cilium-cli --for=condition=failed --timeout=${{ env.timeout }} && exit 1) &
          failed_pid=$!

          # Active wait for whichever background process ends first
          wait -n $complete_pid $failed_pid
          EXIT_CODE=$?

          # Retrieve job logs
          kubectl logs --timestamps -n kube-system job/cilium-cli
          exit ${EXIT_CODE}
        shell: bash {0} # Disable default fail-fast behaviour so that all commands run independently

      - name: Post-test information gathering
        if: ${{ !success() }}
        run: |
          echo "=== Install latest stable CLI ==="
          curl -sSL --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}
          sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
          sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/bin
          rm cilium-linux-amd64.tar.gz{,.sha256sum}
          cilium version

          echo "=== Retrieve cluster state ==="
          kubectl get pods --all-namespaces -o wide
          cilium status
          cilium sysdump --output-filename cilium-sysdump-out
        shell: bash {0} # Disable default fail-fast behaviour so that all commands run independently

      - name: Clean up AKS
        if: ${{ always() }}
        run: |
          az group delete --name ${{ env.name }} --yes --no-wait
        shell: bash {0} # Disable default fail-fast behaviour so that all commands run independently

      - name: Upload artifacts
        if: ${{ !success() }}
        uses: actions/upload-artifact@82c141cc518b40d92cc801eee768e7aafc9c2fa2
        with:
          name: cilium-sysdump-out.zip
          path: cilium-sysdump-out.zip
          retention-days: 5

      - name: Send slack notification
        if: ${{ !success() && (github.event_name == 'schedule' || github.event_name == 'push') }}
        uses: 8398a7/action-slack@a74b761b4089b5d730d813fbedcd2ec5d394f3af
        with:
          status: ${{ job.status }}
          fields: repo,message,commit,author,action,eventName,ref,workflow,job,took
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
