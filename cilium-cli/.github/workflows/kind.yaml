name: Kind

# Any change in triggers needs to be reflected in the concurrency group.
on:
  pull_request: {}
  schedule:
    - cron:  '30 */6 * * *'

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || 'scheduled' }}
  cancel-in-progress: true

env:
  KIND_VERSION: v0.14.0
  KIND_CONFIG: .github/kind-config.yaml
  TIMEOUT: 2m
  LOG_TIME: 30m
  cilium_version: v1.13.2
  kubectl_version: v1.23.6

jobs:
  installation-and-connectivity:
    runs-on: ubuntu-22.04
    timeout-minutes: 40
    strategy:
      matrix:
        mode: ["classic", "helm"]
    steps:
      - name: Set mode
        run: |
          echo "CILIUM_CLI_MODE=${{ matrix.mode }}" >> $GITHUB_ENV
      - name: Checkout
        uses: actions/checkout@8e5e7e5ab8b370d6c329ec480221332ada57f0ab # v3.5.2

      - name: Install kubectl
        run: |
          curl -sLO "https://dl.k8s.io/release/${{ env.kubectl_version }}/bin/linux/amd64/kubectl"
          curl -sLO "https://dl.k8s.io/${{ env.kubectl_version }}/bin/linux/amd64/kubectl.sha256"
          echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Set up Go
        uses: actions/setup-go@fac708d6674e30b6ba41289acaab6d4b75aa0753 # v4.0.1
        with:
          # renovate: datasource=golang-version depName=go
          go-version: 1.20.4

      - name: Set up Go for root
        run: |
          sudo ln -sf `which go` `sudo which go` || true
          sudo go version

      - name: Build and install cilium CLI binary
        run: sudo make install

      - name: Create kind cluster
        uses: helm/kind-action@fa81e57adff234b2908110485695db0f181f3c67 # v1.7.0
        with:
          version: ${{ env.KIND_VERSION }}
          config: ${{ env.KIND_CONFIG }}

      - name: Set NODES_WITHOUT_CILIUM
        run: |
          # To add more elements, keep it comma-separated.
          echo "NODES_WITHOUT_CILIUM=chart-testing-worker2" >> $GITHUB_ENV

      - name: Label nodes
        if: ${{ matrix.mode == 'helm' }}
        run: |
          IFS=',' read -ra nodes <<< "$NODES_WITHOUT_CILIUM"
          for node in "${nodes[@]}"; do
            kubectl label nodes "${node}" cilium.io/no-schedule=true
          done

      # This is needed for the tests that run with nodes without Cilium.
      - name: Install static routes
        run: |
          EXTERNAL_FROM_CIDRS=($(kubectl get nodes -o jsonpath='{range .items[*]}{.spec.podCIDR}{"\n"}{end}'))
          EXTERNAL_NODE_IPS=() # Nodes IPs are collected to be passed to the Cilium CLI later on.

          # Loop over each pod CIDR from all nodes.
          for i in "${!EXTERNAL_FROM_CIDRS[@]}"; do
            EXTERNAL_FROM_CIDR=${EXTERNAL_FROM_CIDRS[i]}

            if [[ $EXTERNAL_FROM_CIDR =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+/[0-9]+$ ]]; then
              IP_FAMILY="v4"
            elif [[ $EXTERNAL_FROM_CIDR =~ ^[0-9a-fA-F:]+/[0-9]+$ ]]; then
              IP_FAMILY="v6"
            else
              echo "ERROR: Malformed pod CIDR '${EXTERNAL_FROM_CIDR}'" >&2
              exit 1
            fi

            IFS=',' read -ra NODES_WITHOUT <<< "$NODES_WITHOUT_CILIUM" # Split by comma into an array
            for WITHOUT in "${NODES_WITHOUT[@]}"; do
              # Fetch node with a specific pod CIDR.
              node=$(kubectl get nodes -o jsonpath="{range .items[?(@.spec.podCIDR == '$EXTERNAL_FROM_CIDR')]}{@.metadata.name}{end}")
              if [[ -z "$node" ]]; then
                echo "ERROR: Could not find node with .spec.podCIDR matching ${EXTERNAL_FROM_CIDR}" >&2
                exit 1
              fi
              for NODE_IP in $(kubectl get node "$node" -o jsonpath="{.status.addresses[?(@.type == 'InternalIP')].address}"); do
                # Skip if the node IP's family mismatches with pod CIDR's
                # family. Cannot create a route with the gateway IP family
                # mismatching the subnet.
                if [[ "$IP_FAMILY" == "v4" && ! "$NODE_IP" =~ \. ]]; then
                  continue
                elif [[ "$IP_FAMILY" == "v6" && ! "$NODE_IP" =~ \: ]]; then
                  continue
                fi
                # Install static route on the host, towards the pod CIDR via the node IP.
                docker exec "$WITHOUT" ip route replace "${EXTERNAL_FROM_CIDR}" via "${NODE_IP}"
                EXTERNAL_NODE_IPS+=("${NODE_IP}")
              done
            done
          done

          # Join the elements with a comma delimiter, or leave them unmodified
          # if there's only one element so that it can be passed properly to
          # the CLI.
          if [[ ${#EXTERNAL_NODE_IPS[@]} -eq 1 ]]; then
            EXTERNAL_NODE_IPS_PARAM="${EXTERNAL_NODE_IPS[0]}"
          else
            EXTERNAL_NODE_IPS_PARAM=$(IFS=','; echo "${EXTERNAL_NODE_IPS[*]}")
          fi
          echo "EXTERNAL_NODE_IPS_PARAM=${EXTERNAL_NODE_IPS_PARAM}" >> $GITHUB_ENV

      # Install Cilium with HostPort support and enables Prometheus for extended connectivity test.
      - name: Install Cilium
        run: |
          cilium install \
            --version=${{ env.cilium_version }} \
            --nodes-without-cilium="${NODES_WITHOUT_CILIUM}" \
            --wait=false \
            --helm-set bpf.monitorAggregation=none \
            --helm-set cni.chainingMode=portmap \
            --helm-set loadBalancer.l7.backend=envoy \
            --helm-set tls.secretsBackend=k8s \
            --helm-set prometheus.enabled=true

      - name: Enable Relay
        run: |
          cilium hubble enable --ui
          cilium status --wait

      - name: Relay Port Forward
        run: |
          cilium hubble port-forward&
          sleep 10s
          [[ $(pgrep -f "cilium.*hubble.*port-forward|kubectl.*port-forward.*hubble-relay" | wc -l) == 2 ]]

      - name: Connectivity Test
        run: |
          # Run the connectivity test in non-default namespace (i.e. not cilium-test)
          cilium connectivity test --debug --all-flows --test-namespace test-namespace \
            --external-from-cidrs="${EXTERNAL_NODE_IPS_PARAM}" \
            --collect-sysdump-on-failure --junit-file connectivity-${{ matrix.mode }}.xml

      - name: Upload junit output
        if: ${{ always() }}
        uses: actions/upload-artifact@0b7f8abb1508181956e8e162db84b466c27e18ce # v3.1.2
        with:
          name: connectivity-${{ matrix.mode }}.xml
          path: connectivity-${{ matrix.mode }}.xml
          retention-days: 5

      - name: Uninstall cilium
        run: |
          pkill -f "cilium.*hubble.*port-forward|kubectl.*port-forward.*hubble-relay"
          cilium uninstall --wait

      - name: Install Cilium with IPsec Encryption
        if: ${{ matrix.mode == 'classic' }}
        run: |
          cilium install \
          --version=${{ env.cilium_version}} \
          --encryption=ipsec \
          --nodes-without-cilium="${NODES_WITHOUT_CILIUM}" \
          --helm-set kubeProxyReplacement=disabled

      - name: Install Cilium with IPsec Encryption
        if: ${{ matrix.mode == 'helm' }}
        run: |
          kubectl create -n kube-system secret generic cilium-ipsec-keys \
            --from-literal=keys="3 rfc4106(gcm(aes)) $(echo $(dd if=/dev/urandom count=20 bs=1 2> /dev/null | xxd -p -c 64)) 128"
          cilium install \
          --version=${{ env.cilium_version}} \
          --nodes-without-cilium="${NODES_WITHOUT_CILIUM}" \
          --helm-set encryption.enabled=true \
          --helm-set encryption.type=ipsec \
          --helm-set kubeProxyReplacement=disabled

      - name: Enable Relay
        run: |
          cilium hubble enable
          cilium status --wait

      - name: Relay Port Forward
        run: |
          cilium hubble port-forward&
          sleep 10s
          [[ $(pgrep -f "cilium.*hubble.*port-forward|kubectl.*port-forward.*hubble-relay" | wc -l) == 2 ]]

      - name: Connectivity test
        run: |
          cilium connectivity test --debug --force-deploy --all-flows --test-namespace test-namespace \
            --external-from-cidrs="${EXTERNAL_NODE_IPS_PARAM}" \
            --collect-sysdump-on-failure --junit-file connectivity-ipsec-${{ matrix.mode }}.xml

      - name: Upload junit output
        if: ${{ always() }}
        uses: actions/upload-artifact@0b7f8abb1508181956e8e162db84b466c27e18ce # v3.1.2
        with:
          name: connectivity-ipsec-${{ matrix.mode }}.xml
          path: connectivity-ipsec-${{ matrix.mode }}.xml
          retention-days: 5

      - name: Cleanup
        if: ${{ always() }}
        run: |
          cilium status
          kubectl get pods --all-namespaces -o wide
          cilium sysdump --output-filename cilium-sysdump-out --hubble-flows-count 10000
        shell: bash {0} # Disable default fail-fast behaviour so that all commands run independently

      - name: Unlabel nodes
        if: ${{ matrix.mode == 'helm' }}
        run: |
          IFS=',' read -ra nodes <<< "$NODES_WITHOUT_CILIUM"
          for node in "${nodes[@]}"; do
            kubectl label nodes "${node}" cilium.io/no-schedule-
          done

      - name: Upload sysdump
        if: ${{ !success() }}
        uses: actions/upload-artifact@0b7f8abb1508181956e8e162db84b466c27e18ce # v3.1.2
        with:
          name: cilium-sysdump-out.zip
          path: cilium-sysdump-out.zip
          retention-days: 5

  helm-upgrade-clustermesh:
    runs-on: ubuntu-22.04
    timeout-minutes: 40

    env:
      CILIUM_CLI_MODE: helm
      KIND_CONFIG_1: .github/kind-config-1.yaml
      KIND_CONFIG_2: .github/kind-config-2.yaml
      # helm/kind-action will override the "name:" provided in the kind config with "chart-testing" unless these are
      # specified as inputs. These must also match the suffix here for CLUSTER1 and CLUSTER2.
      CLUSTER_NAME_1: c1
      CLUSTER_NAME_2: c2
      CLUSTER1: kind-c1
      CLUSTER2: kind-c2

    steps:
      - name: Checkout
        uses: actions/checkout@8e5e7e5ab8b370d6c329ec480221332ada57f0ab # v3.5.2

      - name: Install kubectl
        run: |
          curl -sLO "https://dl.k8s.io/release/${{ env.kubectl_version }}/bin/linux/amd64/kubectl"
          curl -sLO "https://dl.k8s.io/${{ env.kubectl_version }}/bin/linux/amd64/kubectl.sha256"
          echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Set up Go
        uses: actions/setup-go@fac708d6674e30b6ba41289acaab6d4b75aa0753 # v4.0.1
        with:
          # renovate: datasource=golang-version depName=go
          go-version: 1.20.4

      - name: Set up Go for root
        run: |
          sudo ln -sf `which go` `sudo which go` || true
          sudo go version

      - name: Build and install cilium CLI binary
        run: sudo make install

      - name: Create kind cluster 1
        uses: helm/kind-action@fa81e57adff234b2908110485695db0f181f3c67 # v1.7.0
        with:
          version: ${{ env.KIND_VERSION }}
          config: ${{ env.KIND_CONFIG_1 }}
          cluster_name: ${{ env.CLUSTER_NAME_1 }}

      - name: Install Cilium on cluster 1
        run: |
          cilium install --context $CLUSTER1 \
            --version=${{ env.cilium_version }} \
            --wait=true \
            --helm-set bpf.monitorAggregation=none \
            --helm-set cni.chainingMode=portmap \
            --helm-set cluster.id=1 \
            --helm-set cluster.name=cluster1

      - name: Create kind cluster 2
        uses: helm/kind-action@fa81e57adff234b2908110485695db0f181f3c67 # v1.7.0
        with:
          version: ${{ env.KIND_VERSION }}
          config: ${{ env.KIND_CONFIG_2 }}
          cluster_name: ${{ env.CLUSTER_NAME_2 }}

      - name: Install Cilium on cluster 2
        run: |
          cilium install --context $CLUSTER2 \
            --version=${{ env.cilium_version }} \
            --wait=true \
            --helm-set bpf.monitorAggregation=none \
            --helm-set cni.chainingMode=portmap \
            --helm-set cluster.id=2 \
            --helm-set cluster.name=cluster2

      - name: Enable ClusterMesh on cluster 1 using helm-based upgrade
        run: |
          cilium upgrade --reuse-values --context $CLUSTER1 \
            --wait=true \
            --helm-set clustermesh.useAPIServer=true \
            --helm-set clustermesh.apiserver.service.type=NodePort

      - name: Copy CA cert from cluster 1 to cluster 2
        run: |
          kubectl --context $CLUSTER2 delete secret -n kube-system cilium-ca && \
          kubectl --context $CLUSTER1 get secrets -n kube-system cilium-ca -oyaml \
            | kubectl --context $CLUSTER2 apply -f -
          # Restart Cilium on cluster 2
          kubectl --context $CLUSTER2 delete pod -l app.kubernetes.io/part-of=cilium -A

      - name: Enable ClusterMesh on cluster 2 using helm-based upgrade
        run: |
          cilium upgrade --reuse-values --context $CLUSTER2 \
            --wait=true \
            --helm-set clustermesh.useAPIServer=true \
            --helm-set clustermesh.apiserver.service.type=NodePort

      - name: Rename the secrets expected by the clustermesh connect command
        run: |
          kubectl get secrets --context $CLUSTER1 \
            -n kube-system clustermesh-apiserver-remote-cert -oyaml \
              | sed 's/name: .*/name: clustermesh-apiserver-client-cert/' \
              | kubectl apply --context $CLUSTER1 -f -
          kubectl get secrets --context $CLUSTER2 \
            -n kube-system clustermesh-apiserver-remote-cert -oyaml \
              | sed 's/name: .*/name: clustermesh-apiserver-client-cert/' \
              | kubectl apply --context $CLUSTER2 -f -

      - name: Connect the two clusters using clustermesh
        run: |
          cilium clustermesh connect --context $CLUSTER1 --destination-context $CLUSTER2
          cilium clustermesh status --context $CLUSTER1 --wait

      - name: Run the multicluster connectivity tests
        run: |
          cilium connectivity test --context $CLUSTER1 --multi-cluster $CLUSTER2 --debug \
          --collect-sysdump-on-failure --junit-file connectivity-clustermesh.xml

      - name: Upload junit output
        if: ${{ always() }}
        uses: actions/upload-artifact@0b7f8abb1508181956e8e162db84b466c27e18ce # v3.1.2
        with:
          name: connectivity-clustermesh.xml
          path: connectivity-clustermesh.xml
          retention-days: 5

      - name: Cleanup
        if: ${{ always() }}
        run: |
          cilium --context $CLUSTER1 status
          kubectl --context $CLUSTER1 get pods --all-namespaces -o wide
          cilium --context $CLUSTER1 sysdump --output-filename cilium-sysdump-out-c1
          cilium --context $CLUSTER2 status
          kubectl --context $CLUSTER2 get pods --all-namespaces -o wide
          cilium --context $CLUSTER2 sysdump --output-filename cilium-sysdump-out-c2
        shell: bash {0} # Disable default fail-fast behaviour so that all commands run independently

      - name: Upload sysdump from cluster 1
        if: ${{ !success() }}
        uses: actions/upload-artifact@0b7f8abb1508181956e8e162db84b466c27e18ce # v3.1.2
        with:
          name: cilium-sysdump-out-c1.zip
          path: cilium-sysdump-out-c1.zip
          retention-days: 5

      - name: Upload sysdump from cluster 2
        if: ${{ !success() }}
        uses: actions/upload-artifact@0b7f8abb1508181956e8e162db84b466c27e18ce # v3.1.2
        with:
          name: cilium-sysdump-out-c2.zip
          path: cilium-sysdump-out-c2.zip
          retention-days: 5
